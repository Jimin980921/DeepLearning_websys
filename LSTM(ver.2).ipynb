{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전력사용하여 예측  \n",
    "###### python version=3.7.6, tensoflow version=1.13.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step1 (전력만, optim=Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmlee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\jmlee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\jmlee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\jmlee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\jmlee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\jmlee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#http://mjgim.me/2017/08/02/LSTM.html\n",
    "\n",
    "#라이브러리 불러오기\n",
    "import tensorflow as tf  #딥러닝에 유용\n",
    "from tensorflow.contrib import rnn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#랜덤한 겂을 다른 컴퓨터에도 동일하게 얻을 수 있음.\n",
    "tf.set_random_seed(777)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 불러오기\n",
    "jan = pd.read_csv(r\"C:/Users/jmlee/Desktop/4학년 2학기/딥러닝 전력 예측/전력 예측/생성data/1월.csv\", header=None)\n",
    "july = pd.read_csv(r\"C:/Users/jmlee/Desktop/4학년 2학기/딥러닝 전력 예측/전력 예측/생성data/7월.csv\", header=None)\n",
    "jan_=jan.loc[:,[1,2,4]][1:]\n",
    "# july_=july.loc[:,[1,2,4]][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All=pd.concat([jan_,july_])\n",
    "#str을 float로 바꾸기\n",
    "jan_.columns=['월','일','전력량']\n",
    "jan_[['월','일','전력량']]=jan_[['월','일','전력량']].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>월</th>\n",
       "      <th>일</th>\n",
       "      <th>전력량</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>238.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>236.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>237.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>236.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>234.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2876</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>340.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2877</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>294.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2878</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>263.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2879</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>261.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2880</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>257.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2880 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      월   일     전력량\n",
       "1     1   1  238.32\n",
       "2     1   1  236.16\n",
       "3     1   1  237.24\n",
       "4     1   1  236.16\n",
       "5     1   1  234.72\n",
       "...  ..  ..     ...\n",
       "2876  1  30  340.20\n",
       "2877  1  30  294.12\n",
       "2878  1  30  263.88\n",
       "2879  1  30  261.00\n",
       "2880  1  30  257.04\n",
       "\n",
       "[2880 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jan_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinMaxScaler(data):\n",
    "    numerator=data-np.min(data,0)\n",
    "    denominator=np.max(data,0)-np.min(data,0)\n",
    "    return numerator/(denominator+1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#하이퍼 파라미터 설정\n",
    "\n",
    "#한개의 시퀀스길이(시계열데이터 입력갯수,몇개의 rnn을 연결할지 결정,input보다는 같거나 많아야함)\n",
    "timesteps = seq_length = 3  \n",
    "#variable개수\n",
    "data_dim = 3  \n",
    "#각셀의 출력크기(많을수록 특정 훈련데이터에 최적화 가능)\n",
    "hidden_dim = 3  \n",
    "#결과분류 총 수\n",
    "output_dim = 1  \n",
    "#학습률\n",
    "learing_rate = 0.001  \n",
    "#에폭횟수\n",
    "iterations = 50_000  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #데이터 조절\n",
    "# jan_[\"전력량\"] /= 1e5\n",
    "\n",
    "#Framework 제작\n",
    "jan_['전력량']=MinMaxScaler(jan_['전력량'])\n",
    "x = jan_[['월','일','전력량']].values\n",
    "y = jan_[\"전력량\"].values  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(y) - seq_length):  #seq_length=timesteps\n",
    "    _x = np.copy(x[i:i + seq_length + 1])\n",
    "#     _x[timesteps-2][data_dim-1] = 0\n",
    "#     _x[timesteps-1][data_dim-1] = 0\n",
    "#     _x[timesteps][data_dim-1] = 0\n",
    "    _y = [y[i + seq_length]] #다음 전력(정답)\n",
    "    dataX.append(_x)\n",
    "    dataY.append(_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습데이터와 테스트데이터 분류\n",
    "\n",
    "train_size = int(len(dataY) * 0.8)\n",
    "test_size = len(dataY) - train_size \n",
    "\n",
    "trainX = np.array(dataX[:train_size])  #2298\n",
    "testX = np.array(dataX[train_size : ])\n",
    "\n",
    "trainY = np.array(dataY[:train_size])  #575\n",
    "testY = np.array(dataY[train_size : ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-b7c77ad44290>:7: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-12-b7c77ad44290>:10: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    }
   ],
   "source": [
    "#LSTM모델 구축\n",
    "#placeholder 초기화 후 텐서에 매핑\n",
    "X = tf.placeholder(tf.float32, [None, seq_length+1, data_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "def lstm_cell(): #cell생성\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(hidden_dim, reuse=tf.AUTO_REUSE)  #출력의 크기를 hidden dim=4\n",
    "    return cell \n",
    "## 5 layers for hidden layer\n",
    "multi_cells = rnn.MultiRNNCell([lstm_cell() for _ in range(5)], state_is_tuple=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-d4faf3dbc2de>:2: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\jmlee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# dynamic rnn 구조(RNN 신경망)연결\n",
    "outputs, _states = tf.nn.dynamic_rnn(multi_cells, X, dtype=tf.float32) #결과값 output차원은 hidden_dim의 크기와 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=None)\n",
    "\n",
    "# sum of the squares= 표준편차(예측값과 실제값의차이) 평균제곱오차\n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSprop 알고리즘->adam\n",
    "optimizer= tf.train.AdamOptimizer(learing_rate)\n",
    "train = optimizer.minimize(loss)\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 50] loss: 367.8433837890625\n",
      "[step: 100] loss: 136.95797729492188\n",
      "[step: 150] loss: 121.83451843261719\n",
      "[step: 200] loss: 121.6681900024414\n",
      "[step: 250] loss: 121.46723937988281\n",
      "[step: 300] loss: 121.10665130615234\n",
      "[step: 350] loss: 120.03763580322266\n",
      "[step: 400] loss: 97.90396881103516\n",
      "[step: 450] loss: 15.181621551513672\n",
      "[step: 500] loss: 13.013257026672363\n",
      "[step: 550] loss: 12.247529029846191\n",
      "[step: 600] loss: 11.639495849609375\n",
      "[step: 650] loss: 11.086577415466309\n",
      "[step: 700] loss: 10.583253860473633\n",
      "[step: 750] loss: 10.132729530334473\n",
      "[step: 800] loss: 9.730953216552734\n",
      "[step: 850] loss: 9.348456382751465\n",
      "[step: 900] loss: 8.994729042053223\n",
      "[step: 950] loss: 8.681501388549805\n",
      "[step: 1000] loss: 8.390107154846191\n",
      "[step: 1050] loss: 8.100534439086914\n",
      "[step: 1100] loss: 7.794903755187988\n",
      "[step: 1150] loss: 7.4552178382873535\n",
      "[step: 1200] loss: 7.060174465179443\n",
      "[step: 1250] loss: 6.506078243255615\n",
      "[step: 1300] loss: 5.93638801574707\n",
      "[step: 1350] loss: 5.304925441741943\n",
      "[step: 1400] loss: 4.582630634307861\n",
      "[step: 1450] loss: 3.767052412033081\n",
      "[step: 1500] loss: 2.8864994049072266\n",
      "[step: 1550] loss: 2.0422165393829346\n",
      "[step: 1600] loss: 1.378228783607483\n",
      "[step: 1650] loss: 0.9619073271751404\n",
      "[step: 1700] loss: 0.7279729247093201\n",
      "[step: 1750] loss: 0.5774685740470886\n",
      "[step: 1800] loss: 0.4674977660179138\n",
      "[step: 1850] loss: 0.38100120425224304\n",
      "[step: 1900] loss: 0.31230413913726807\n",
      "[step: 1950] loss: 0.2644904851913452\n",
      "[step: 2000] loss: 0.22121301293373108\n",
      "[step: 2050] loss: 0.1908688098192215\n",
      "[step: 2100] loss: 0.16701878607273102\n",
      "[step: 2150] loss: 0.14957000315189362\n",
      "[step: 2200] loss: 0.13410861790180206\n",
      "[step: 2250] loss: 0.122325100004673\n",
      "[step: 2300] loss: 0.11275088787078857\n",
      "[step: 2350] loss: 0.1047220230102539\n",
      "[step: 2400] loss: 0.10106293112039566\n",
      "[step: 2450] loss: 0.09258008003234863\n",
      "[step: 2500] loss: 0.08756273984909058\n",
      "[step: 2550] loss: 0.08302122354507446\n",
      "[step: 2600] loss: 0.08229587972164154\n",
      "[step: 2650] loss: 0.07547537237405777\n",
      "[step: 2700] loss: 0.07206512242555618\n",
      "[step: 2750] loss: 0.06892190128564835\n",
      "[step: 2800] loss: 0.0659564957022667\n",
      "[step: 2850] loss: 0.07532254606485367\n",
      "[step: 2900] loss: 0.06110032647848129\n",
      "[step: 2950] loss: 0.05826529487967491\n",
      "[step: 3000] loss: 0.05597081780433655\n",
      "[step: 3050] loss: 0.05390703305602074\n",
      "[step: 3100] loss: 0.05182996019721031\n",
      "[step: 3150] loss: 0.049952372908592224\n",
      "[step: 3200] loss: 0.048134539276361465\n",
      "[step: 3250] loss: 0.04640290513634682\n",
      "[step: 3300] loss: 0.053420912474393845\n",
      "[step: 3350] loss: 0.04343655705451965\n",
      "[step: 3400] loss: 0.04188157245516777\n",
      "[step: 3450] loss: 0.040499478578567505\n",
      "[step: 3500] loss: 0.03917364403605461\n",
      "[step: 3550] loss: 0.03998544439673424\n",
      "[step: 3600] loss: 0.03684905171394348\n",
      "[step: 3650] loss: 0.03573846444487572\n",
      "[step: 3700] loss: 0.034670066088438034\n",
      "[step: 3750] loss: 0.03364010155200958\n",
      "[step: 3800] loss: 0.04008013382554054\n",
      "[step: 3850] loss: 0.03181594982743263\n",
      "[step: 3900] loss: 0.030926277860999107\n",
      "[step: 3950] loss: 0.030077068135142326\n",
      "[step: 4000] loss: 0.029255129396915436\n",
      "[step: 4050] loss: 0.04129599779844284\n",
      "[step: 4100] loss: 0.027899114415049553\n",
      "[step: 4150] loss: 0.02710178680717945\n",
      "[step: 4200] loss: 0.02642141282558441\n",
      "[step: 4250] loss: 0.02576121687889099\n",
      "[step: 4300] loss: 0.025120915845036507\n",
      "[step: 4350] loss: 0.06004633009433746\n",
      "[step: 4400] loss: 0.02421640232205391\n",
      "[step: 4450] loss: 0.023457851260900497\n",
      "[step: 4500] loss: 0.022927673533558846\n",
      "[step: 4550] loss: 0.022414743900299072\n",
      "[step: 4600] loss: 0.0219173151999712\n",
      "[step: 4650] loss: 0.02714700810611248\n",
      "[step: 4700] loss: 0.021872645244002342\n",
      "[step: 4750] loss: 0.020608801394701004\n",
      "[step: 4800] loss: 0.0201950091868639\n",
      "[step: 4850] loss: 0.019794659689068794\n",
      "[step: 4900] loss: 0.019413314759731293\n",
      "[step: 4950] loss: 0.02280258573591709\n",
      "[step: 5000] loss: 0.018774036318063736\n",
      "[step: 5050] loss: 0.0184276532381773\n",
      "[step: 5100] loss: 0.0181081872433424\n",
      "[step: 5150] loss: 0.017798446118831635\n",
      "[step: 5200] loss: 0.04273124784231186\n",
      "[step: 5250] loss: 0.017900584265589714\n",
      "[step: 5300] loss: 0.01699616201221943\n",
      "[step: 5350] loss: 0.016731638461351395\n",
      "[step: 5400] loss: 0.01647826097905636\n",
      "[step: 5450] loss: 0.025091351941227913\n",
      "[step: 5500] loss: 0.016101155430078506\n",
      "[step: 5550] loss: 0.015815606340765953\n",
      "[step: 5600] loss: 0.015597635880112648\n",
      "[step: 5650] loss: 0.015386110171675682\n",
      "[step: 5700] loss: 0.016948610544204712\n",
      "[step: 5750] loss: 0.015044266358017921\n",
      "[step: 5800] loss: 0.01484987698495388\n",
      "[step: 5850] loss: 0.014665858820080757\n",
      "[step: 5900] loss: 0.014485117979347706\n",
      "[step: 5950] loss: 0.014309942722320557\n",
      "[step: 6000] loss: 0.015093352645635605\n",
      "[step: 6050] loss: 0.01403928641229868\n",
      "[step: 6100] loss: 0.013858677819371223\n",
      "[step: 6150] loss: 0.013701642863452435\n",
      "[step: 6200] loss: 0.013546517118811607\n",
      "[step: 6250] loss: 0.01339777559041977\n",
      "[step: 6300] loss: 0.018086936324834824\n",
      "[step: 6350] loss: 0.013153279200196266\n",
      "[step: 6400] loss: 0.013012467883527279\n",
      "[step: 6450] loss: 0.012875705026090145\n",
      "[step: 6500] loss: 0.012740077450871468\n",
      "[step: 6550] loss: 0.012605533935129642\n",
      "[step: 6600] loss: 0.08695507794618607\n",
      "[step: 6650] loss: 0.012791069224476814\n",
      "[step: 6700] loss: 0.012274340726435184\n",
      "[step: 6750] loss: 0.012152379378676414\n",
      "[step: 6800] loss: 0.012032529339194298\n",
      "[step: 6850] loss: 0.011913429945707321\n",
      "[step: 6900] loss: 0.011794843710958958\n",
      "[step: 6950] loss: 0.013044469058513641\n",
      "[step: 7000] loss: 0.011757653206586838\n",
      "[step: 7050] loss: 0.011500272899866104\n",
      "[step: 7100] loss: 0.011392347514629364\n",
      "[step: 7150] loss: 0.011285134591162205\n",
      "[step: 7200] loss: 0.011178168468177319\n",
      "[step: 7250] loss: 0.09865953773260117\n",
      "[step: 7300] loss: 0.011623776517808437\n",
      "[step: 7350] loss: 0.010915328748524189\n",
      "[step: 7400] loss: 0.010816189460456371\n",
      "[step: 7450] loss: 0.010719377547502518\n",
      "[step: 7500] loss: 0.010622692294418812\n",
      "[step: 7550] loss: 0.010545579716563225\n",
      "[step: 7600] loss: 0.013793164864182472\n",
      "[step: 7650] loss: 0.010387120768427849\n",
      "[step: 7700] loss: 0.010288149118423462\n",
      "[step: 7750] loss: 0.010199313051998615\n",
      "[step: 7800] loss: 0.01011074148118496\n",
      "[step: 7850] loss: 0.025853527709841728\n",
      "[step: 7900] loss: 0.009966645389795303\n",
      "[step: 7950] loss: 0.009885776787996292\n",
      "[step: 8000] loss: 0.009801594540476799\n",
      "[step: 8050] loss: 0.009719259105622768\n",
      "[step: 8100] loss: 0.06386909633874893\n",
      "[step: 8150] loss: 0.009753250516951084\n",
      "[step: 8200] loss: 0.009517674334347248\n",
      "[step: 8250] loss: 0.009441934525966644\n",
      "[step: 8300] loss: 0.009366445243358612\n",
      "[step: 8350] loss: 0.00929196085780859\n",
      "[step: 8400] loss: 0.016145499423146248\n",
      "[step: 8450] loss: 0.009179254062473774\n",
      "[step: 8500] loss: 0.009103318676352501\n",
      "[step: 8550] loss: 0.00903294701129198\n",
      "[step: 8600] loss: 0.008964980021119118\n",
      "[step: 8650] loss: 0.013607590459287167\n",
      "[step: 8700] loss: 0.008891827426850796\n",
      "[step: 8750] loss: 0.008790037594735622\n",
      "[step: 8800] loss: 0.008724372833967209\n",
      "[step: 8850] loss: 0.008659344166517258\n",
      "[step: 8900] loss: 0.018362239003181458\n",
      "[step: 8950] loss: 0.008581055328249931\n",
      "[step: 9000] loss: 0.008502989076077938\n",
      "[step: 9050] loss: 0.008442293852567673\n",
      "[step: 9100] loss: 0.008381711319088936\n",
      "[step: 9150] loss: 0.08396393805742264\n",
      "[step: 9200] loss: 0.009048240259289742\n",
      "[step: 9250] loss: 0.008240872994065285\n",
      "[step: 9300] loss: 0.0081814369186759\n",
      "[step: 9350] loss: 0.00812541227787733\n",
      "[step: 9400] loss: 0.0080697201192379\n",
      "[step: 9450] loss: 0.008141814731061459\n",
      "[step: 9500] loss: 0.007998193614184856\n",
      "[step: 9550] loss: 0.007935197092592716\n",
      "[step: 9600] loss: 0.007882742211222649\n",
      "[step: 9650] loss: 0.00783032551407814\n",
      "[step: 9700] loss: 0.016689777374267578\n",
      "[step: 9750] loss: 0.007824885658919811\n",
      "[step: 9800] loss: 0.007706048432737589\n",
      "[step: 9850] loss: 0.007657086942344904\n",
      "[step: 9900] loss: 0.007608016487210989\n",
      "[step: 9950] loss: 0.15722808241844177\n",
      "[step: 10000] loss: 0.008261075243353844\n",
      "[step: 10050] loss: 0.007503576576709747\n",
      "[step: 10100] loss: 0.0074551100842654705\n",
      "[step: 10150] loss: 0.007410197984427214\n",
      "[step: 10200] loss: 0.007365105673670769\n",
      "[step: 10250] loss: 0.007442774251103401\n",
      "[step: 10300] loss: 0.008474184200167656\n",
      "[step: 10350] loss: 0.00727164838463068\n",
      "[step: 10400] loss: 0.007217070087790489\n",
      "[step: 10450] loss: 0.007174839731305838\n",
      "[step: 10500] loss: 0.0071323891170322895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 10550] loss: 0.037981338798999786\n",
      "[step: 10600] loss: 0.007259109988808632\n",
      "[step: 10650] loss: 0.007038936018943787\n",
      "[step: 10700] loss: 0.006999317556619644\n",
      "[step: 10750] loss: 0.006959930062294006\n",
      "[step: 10800] loss: 0.006920244544744492\n",
      "[step: 10850] loss: 0.02353562042117119\n",
      "[step: 10900] loss: 0.0069527216255664825\n",
      "[step: 10950] loss: 0.006835595238953829\n",
      "[step: 11000] loss: 0.006798255257308483\n",
      "[step: 11050] loss: 0.006761427037417889\n",
      "[step: 11100] loss: 0.006726279854774475\n",
      "[step: 11150] loss: 0.01373252086341381\n",
      "[step: 11200] loss: 0.006675929296761751\n",
      "[step: 11250] loss: 0.006639034487307072\n",
      "[step: 11300] loss: 0.00660386448726058\n",
      "[step: 11350] loss: 0.00657700328156352\n",
      "[step: 11400] loss: 0.011486773379147053\n",
      "[step: 11450] loss: 0.006538475397974253\n",
      "[step: 11500] loss: 0.006490206345915794\n",
      "[step: 11550] loss: 0.0064567867666482925\n",
      "[step: 11600] loss: 0.006423441227525473\n",
      "[step: 11650] loss: 0.02287064865231514\n",
      "[step: 11700] loss: 0.006420782767236233\n",
      "[step: 11750] loss: 0.006350500509142876\n",
      "[step: 11800] loss: 0.006318788509815931\n",
      "[step: 11850] loss: 0.006287137046456337\n",
      "[step: 11900] loss: 0.006807657890021801\n",
      "[step: 11950] loss: 0.007545970845967531\n",
      "[step: 12000] loss: 0.006231464445590973\n",
      "[step: 12050] loss: 0.006193098612129688\n",
      "[step: 12100] loss: 0.006163707934319973\n",
      "[step: 12150] loss: 0.006134138908237219\n",
      "[step: 12200] loss: 0.006107969209551811\n",
      "[step: 12250] loss: 0.006573447026312351\n",
      "[step: 12300] loss: 0.00610050605610013\n",
      "[step: 12350] loss: 0.006038960535079241\n",
      "[step: 12400] loss: 0.006010496057569981\n",
      "[step: 12450] loss: 0.005981986876577139\n",
      "[step: 12500] loss: 0.02453749068081379\n",
      "[step: 12550] loss: 0.0060190921649336815\n",
      "[step: 12600] loss: 0.005923897959291935\n",
      "[step: 12650] loss: 0.005897104274481535\n",
      "[step: 12700] loss: 0.005870239809155464\n",
      "[step: 12750] loss: 0.005859916098415852\n",
      "[step: 12800] loss: 0.009202525950968266\n",
      "[step: 12850] loss: 0.005832577124238014\n",
      "[step: 12900] loss: 0.005783680826425552\n",
      "[step: 12950] loss: 0.005757682491093874\n",
      "[step: 13000] loss: 0.00630948506295681\n",
      "[step: 13050] loss: 0.005741633474826813\n",
      "[step: 13100] loss: 0.005712070036679506\n",
      "[step: 13150] loss: 0.0056763323955237865\n",
      "[step: 13200] loss: 0.0056515047326684\n",
      "[step: 13250] loss: 0.005628177896142006\n",
      "[step: 13300] loss: 0.012116438709199429\n",
      "[step: 13350] loss: 0.005645348224788904\n",
      "[step: 13400] loss: 0.005578798241913319\n",
      "[step: 13450] loss: 0.005555308889597654\n",
      "[step: 13500] loss: 0.00553171755746007\n",
      "[step: 13550] loss: 0.0055172741413116455\n",
      "[step: 13600] loss: 0.009918521158397198\n",
      "[step: 13650] loss: 0.005482970736920834\n",
      "[step: 13700] loss: 0.005458571016788483\n",
      "[step: 13750] loss: 0.0054357522167265415\n",
      "[step: 13800] loss: 0.005413987673819065\n",
      "[step: 13850] loss: 0.013094636611640453\n",
      "[step: 13900] loss: 0.00541079044342041\n",
      "[step: 13950] loss: 0.005366056691855192\n",
      "[step: 14000] loss: 0.0053438907489180565\n",
      "[step: 14050] loss: 0.005321882665157318\n",
      "[step: 14100] loss: 0.006838429719209671\n",
      "[step: 14150] loss: 0.005349557381123304\n",
      "[step: 14200] loss: 0.0052773491479456425\n",
      "[step: 14250] loss: 0.005256330128759146\n",
      "[step: 14300] loss: 0.0052351816557347775\n",
      "[step: 14350] loss: 0.022719236090779305\n",
      "[step: 14400] loss: 0.006404118612408638\n",
      "[step: 14450] loss: 0.005198884289711714\n",
      "[step: 14500] loss: 0.005177160259336233\n",
      "[step: 14550] loss: 0.005157104227691889\n",
      "[step: 14600] loss: 0.005136990454047918\n",
      "[step: 14650] loss: 0.005155512597411871\n",
      "[step: 14700] loss: 0.008669397793710232\n",
      "[step: 14750] loss: 0.005117832683026791\n",
      "[step: 14800] loss: 0.005078405141830444\n",
      "[step: 14850] loss: 0.005059008486568928\n",
      "[step: 14900] loss: 0.00503953080624342\n",
      "[step: 14950] loss: 0.055161818861961365\n",
      "[step: 15000] loss: 0.005297346506267786\n",
      "[step: 15050] loss: 0.005001880694180727\n",
      "[step: 15100] loss: 0.004981855861842632\n",
      "[step: 15150] loss: 0.004963036626577377\n",
      "[step: 15200] loss: 0.11863675713539124\n",
      "[step: 15250] loss: 0.005508133210241795\n",
      "[step: 15300] loss: 0.004928157664835453\n",
      "[step: 15350] loss: 0.004908379632979631\n",
      "[step: 15400] loss: 0.004890217445790768\n",
      "[step: 15450] loss: 0.005130895413458347\n",
      "[step: 15500] loss: 0.0070617650635540485\n",
      "[step: 15550] loss: 0.004858584143221378\n",
      "[step: 15600] loss: 0.004835758823901415\n",
      "[step: 15650] loss: 0.004818050656467676\n",
      "[step: 15700] loss: 0.004929021000862122\n",
      "[step: 15750] loss: 0.0050407894887030125\n",
      "[step: 15800] loss: 0.004785115830600262\n",
      "[step: 15850] loss: 0.004768128972500563\n",
      "[step: 15900] loss: 0.0047510708682239056\n",
      "[step: 15950] loss: 0.004733951296657324\n",
      "[step: 16000] loss: 0.017486555501818657\n",
      "[step: 16050] loss: 0.004728959407657385\n",
      "[step: 16100] loss: 0.004701785743236542\n",
      "[step: 16150] loss: 0.004684647545218468\n",
      "[step: 16200] loss: 0.004668028559535742\n",
      "[step: 16250] loss: 0.11161472648382187\n",
      "[step: 16300] loss: 0.004899988882243633\n",
      "[step: 16350] loss: 0.0046399859711527824\n",
      "[step: 16400] loss: 0.004620329476892948\n",
      "[step: 16450] loss: 0.004604178946465254\n",
      "[step: 16500] loss: 0.005815469194203615\n",
      "[step: 16550] loss: 0.006124700885266066\n",
      "[step: 16600] loss: 0.004575592465698719\n",
      "[step: 16650] loss: 0.004557876847684383\n",
      "[step: 16700] loss: 0.004542050417512655\n",
      "[step: 16750] loss: 0.004526908043771982\n",
      "[step: 16800] loss: 0.004546404350548983\n",
      "[step: 16850] loss: 0.0045372843742370605\n",
      "[step: 16900] loss: 0.004500135779380798\n",
      "[step: 16950] loss: 0.004484852310270071\n",
      "[step: 17000] loss: 0.004469608888030052\n",
      "[step: 17050] loss: 0.004454226698726416\n",
      "[step: 17100] loss: 0.004536137916147709\n",
      "[step: 17150] loss: 0.00857270136475563\n",
      "[step: 17200] loss: 0.004474543500691652\n",
      "[step: 17250] loss: 0.004433021880686283\n",
      "[step: 17300] loss: 0.004418784752488136\n",
      "[step: 17350] loss: 0.004404881969094276\n",
      "[step: 17400] loss: 0.0043909624218940735\n",
      "[step: 17450] loss: 0.004376954399049282\n",
      "[step: 17500] loss: 0.004362767096608877\n",
      "[step: 17550] loss: 0.004348393529653549\n",
      "[step: 17600] loss: 0.004334904719144106\n",
      "[step: 17650] loss: 0.0049175601452589035\n",
      "[step: 17700] loss: 0.004347918089479208\n",
      "[step: 17750] loss: 0.004314139951020479\n",
      "[step: 17800] loss: 0.004300192464143038\n",
      "[step: 17850] loss: 0.004286259412765503\n",
      "[step: 17900] loss: 0.00427226722240448\n",
      "[step: 17950] loss: 0.018652623519301414\n",
      "[step: 18000] loss: 0.005200110841542482\n",
      "[step: 18050] loss: 0.004248843528330326\n",
      "[step: 18100] loss: 0.004233642015606165\n",
      "[step: 18150] loss: 0.0042198882438242435\n",
      "[step: 18200] loss: 0.0042546275071799755\n",
      "[step: 18250] loss: 0.004334139637649059\n",
      "[step: 18300] loss: 0.00421122508123517\n",
      "[step: 18350] loss: 0.004181142896413803\n",
      "[step: 18400] loss: 0.004167677368968725\n",
      "[step: 18450] loss: 0.00419335812330246\n",
      "[step: 18500] loss: 0.004159239120781422\n",
      "[step: 18550] loss: 0.004163080360740423\n",
      "[step: 18600] loss: 0.004130690824240446\n",
      "[step: 18650] loss: 0.00411746371537447\n",
      "[step: 18700] loss: 0.004105001222342253\n",
      "[step: 18750] loss: 0.013523004949092865\n",
      "[step: 18800] loss: 0.004150301683694124\n",
      "[step: 18850] loss: 0.004082289524376392\n",
      "[step: 18900] loss: 0.004069375805556774\n",
      "[step: 18950] loss: 0.004056599922478199\n",
      "[step: 19000] loss: 0.00565378088504076\n",
      "[step: 19050] loss: 0.004209668375551701\n",
      "[step: 19100] loss: 0.0040341024287045\n",
      "[step: 19150] loss: 0.004020201973617077\n",
      "[step: 19200] loss: 0.004011082462966442\n",
      "[step: 19250] loss: 0.009423301555216312\n",
      "[step: 19300] loss: 0.0040039257146418095\n",
      "[step: 19350] loss: 0.003985337447375059\n",
      "[step: 19400] loss: 0.0039730919525027275\n",
      "[step: 19450] loss: 0.003962365444749594\n",
      "[step: 19500] loss: 0.003984133247286081\n",
      "[step: 19550] loss: 0.003978597931563854\n",
      "[step: 19600] loss: 0.003941416274756193\n",
      "[step: 19650] loss: 0.003929512575268745\n",
      "[step: 19700] loss: 0.0039175720885396\n",
      "[step: 19750] loss: 0.12375226616859436\n",
      "[step: 19800] loss: 0.004404592793434858\n",
      "[step: 19850] loss: 0.0039011791814118624\n",
      "[step: 19900] loss: 0.0038861348293721676\n",
      "[step: 19950] loss: 0.0038744856137782335\n",
      "[step: 20000] loss: 0.0038628329057246447\n",
      "[step: 20050] loss: 0.005409069824963808\n",
      "[step: 20100] loss: 0.003862502286210656\n",
      "[step: 20150] loss: 0.0038501988165080547\n",
      "[step: 20200] loss: 0.0038390138652175665\n",
      "[step: 20250] loss: 0.00382793415337801\n",
      "[step: 20300] loss: 0.003816791344434023\n",
      "[step: 20350] loss: 0.0038054841570556164\n",
      "[step: 20400] loss: 0.09565022587776184\n",
      "[step: 20450] loss: 0.0038515455089509487\n",
      "[step: 20500] loss: 0.003799282945692539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 20550] loss: 0.00378809729591012\n",
      "[step: 20600] loss: 0.003777463687583804\n",
      "[step: 20650] loss: 0.003766907611861825\n",
      "[step: 20700] loss: 0.0037563194055110216\n",
      "[step: 20750] loss: 0.0037456396967172623\n",
      "[step: 20800] loss: 0.003902699099853635\n",
      "[step: 20850] loss: 0.0040644314140081406\n",
      "[step: 20900] loss: 0.003726349910721183\n",
      "[step: 20950] loss: 0.0037156546022742987\n",
      "[step: 21000] loss: 0.003705037524923682\n",
      "[step: 21050] loss: 0.07673534750938416\n",
      "[step: 21100] loss: 0.0036989760119467974\n",
      "[step: 21150] loss: 0.003687989665195346\n",
      "[step: 21200] loss: 0.003675735555589199\n",
      "[step: 21250] loss: 0.00366602442227304\n",
      "[step: 21300] loss: 0.015479227527976036\n",
      "[step: 21350] loss: 0.0036903286818414927\n",
      "[step: 21400] loss: 0.0036472624633461237\n",
      "[step: 21450] loss: 0.0036366041749715805\n",
      "[step: 21500] loss: 0.004326165188103914\n",
      "[step: 21550] loss: 0.00523380609229207\n",
      "[step: 21600] loss: 0.0036257272586226463\n",
      "[step: 21650] loss: 0.0036083466839045286\n",
      "[step: 21700] loss: 0.0035981948021799326\n",
      "[step: 21750] loss: 0.0038500542286783457\n",
      "[step: 21800] loss: 0.003605265635997057\n",
      "[step: 21850] loss: 0.0035829569678753614\n",
      "[step: 21900] loss: 0.0035721000749617815\n",
      "[step: 21950] loss: 0.0035622217692434788\n",
      "[step: 22000] loss: 0.10873054713010788\n",
      "[step: 22050] loss: 0.003707999363541603\n",
      "[step: 22100] loss: 0.0035460363142192364\n",
      "[step: 22150] loss: 0.003536347998306155\n",
      "[step: 22200] loss: 0.003526637563481927\n",
      "[step: 22250] loss: 0.02725018374621868\n",
      "[step: 22300] loss: 0.00392040004953742\n",
      "[step: 22350] loss: 0.0035109480377286673\n",
      "[step: 22400] loss: 0.0035011274740099907\n",
      "[step: 22450] loss: 0.0034916200675070286\n",
      "[step: 22500] loss: 0.05321049690246582\n",
      "[step: 22550] loss: 0.0037279597017914057\n",
      "[step: 22600] loss: 0.003476778045296669\n",
      "[step: 22650] loss: 0.003467151429504156\n",
      "[step: 22700] loss: 0.0034578004851937294\n",
      "[step: 22750] loss: 0.004069251473993063\n",
      "[step: 22800] loss: 0.0035326655488461256\n",
      "[step: 22850] loss: 0.0034450918901711702\n",
      "[step: 22900] loss: 0.003433656645938754\n",
      "[step: 22950] loss: 0.003424441907554865\n",
      "[step: 23000] loss: 0.10252068191766739\n",
      "[step: 23050] loss: 0.003647157922387123\n",
      "[step: 23100] loss: 0.003411778248846531\n",
      "[step: 23150] loss: 0.0034020610619336367\n",
      "[step: 23200] loss: 0.003393045160919428\n",
      "[step: 23250] loss: 0.0037798748817294836\n",
      "[step: 23300] loss: 0.005394544918090105\n",
      "[step: 23350] loss: 0.0033862260170280933\n",
      "[step: 23400] loss: 0.003369087353348732\n",
      "[step: 23450] loss: 0.003360143629834056\n",
      "[step: 23500] loss: 0.11280042678117752\n",
      "[step: 23550] loss: 0.0034524938091635704\n",
      "[step: 23600] loss: 0.0033475409727543592\n",
      "[step: 23650] loss: 0.0033378410153090954\n",
      "[step: 23700] loss: 0.0033290849532932043\n",
      "[step: 23750] loss: 0.006335112731903791\n",
      "[step: 23800] loss: 0.003401397494599223\n",
      "[step: 23850] loss: 0.003324467921629548\n",
      "[step: 23900] loss: 0.0033081460278481245\n",
      "[step: 23950] loss: 0.00329954968765378\n",
      "[step: 24000] loss: 0.0032909740693867207\n",
      "[step: 24050] loss: 0.00462940102443099\n",
      "[step: 24100] loss: 0.003295840695500374\n",
      "[step: 24150] loss: 0.003281200537458062\n",
      "[step: 24200] loss: 0.0032722868490964174\n",
      "[step: 24250] loss: 0.0032639384735375643\n",
      "[step: 24300] loss: 0.0032555805519223213\n",
      "[step: 24350] loss: 0.003496547695249319\n",
      "[step: 24400] loss: 0.003323035314679146\n",
      "[step: 24450] loss: 0.003243903862312436\n",
      "[step: 24500] loss: 0.003235455835238099\n",
      "[step: 24550] loss: 0.0032271964009851217\n",
      "[step: 24600] loss: 0.003218837082386017\n",
      "[step: 24650] loss: 0.16798816621303558\n",
      "[step: 24700] loss: 0.0035849784035235643\n",
      "[step: 24750] loss: 0.0032262506429105997\n",
      "[step: 24800] loss: 0.003213702468201518\n",
      "[step: 24850] loss: 0.0032057510688900948\n",
      "[step: 24900] loss: 0.0031979824416339397\n",
      "[step: 24950] loss: 0.0031902301125228405\n",
      "[step: 25000] loss: 0.003182439599186182\n",
      "[step: 25050] loss: 0.003174565499648452\n",
      "[step: 25100] loss: 0.00316658616065979\n",
      "[step: 25150] loss: 0.003158833831548691\n",
      "[step: 25200] loss: 0.009644988924264908\n",
      "[step: 25250] loss: 0.003168412484228611\n",
      "[step: 25300] loss: 0.0031601409427821636\n",
      "[step: 25350] loss: 0.00315210223197937\n",
      "[step: 25400] loss: 0.003144495887681842\n",
      "[step: 25450] loss: 0.003136904677376151\n",
      "[step: 25500] loss: 0.00312931090593338\n",
      "[step: 25550] loss: 0.0031216712668538094\n",
      "[step: 25600] loss: 0.0031142455991357565\n",
      "[step: 25650] loss: 0.01569652557373047\n",
      "[step: 25700] loss: 0.0031894866842776537\n",
      "[step: 25750] loss: 0.003102742601186037\n",
      "[step: 25800] loss: 0.0030949402134865522\n",
      "[step: 25850] loss: 0.003087200690060854\n",
      "[step: 25900] loss: 0.03263653442263603\n",
      "[step: 25950] loss: 0.00334505015052855\n",
      "[step: 26000] loss: 0.003081455361098051\n",
      "[step: 26050] loss: 0.0030712576117366552\n",
      "[step: 26100] loss: 0.003063659183681011\n",
      "[step: 26150] loss: 0.0030561129096895456\n",
      "[step: 26200] loss: 0.1294238567352295\n",
      "[step: 26250] loss: 0.0035756160505115986\n",
      "[step: 26300] loss: 0.0030477866530418396\n",
      "[step: 26350] loss: 0.003038759110495448\n",
      "[step: 26400] loss: 0.0030312957242131233\n",
      "[step: 26450] loss: 0.003023760160431266\n",
      "[step: 26500] loss: 0.003380194306373596\n",
      "[step: 26550] loss: 0.00641994783654809\n",
      "[step: 26600] loss: 0.0030417307280004025\n",
      "[step: 26650] loss: 0.003017720766365528\n",
      "[step: 26700] loss: 0.0030103158205747604\n",
      "[step: 26750] loss: 0.003003119956701994\n",
      "[step: 26800] loss: 0.0029960048850625753\n",
      "[step: 26850] loss: 0.0029888686258345842\n",
      "[step: 26900] loss: 0.0029816459864377975\n",
      "[step: 26950] loss: 0.0029743737541139126\n",
      "[step: 27000] loss: 0.1901901513338089\n",
      "[step: 27050] loss: 0.004047636408358812\n",
      "[step: 27100] loss: 0.0029767178930342197\n",
      "[step: 27150] loss: 0.0029683299362659454\n",
      "[step: 27200] loss: 0.002961224876344204\n",
      "[step: 27250] loss: 0.0029542569536715746\n",
      "[step: 27300] loss: 0.0029473004397004843\n",
      "[step: 27350] loss: 0.0029403201770037413\n",
      "[step: 27400] loss: 0.0029332656413316727\n",
      "[step: 27450] loss: 0.025506872683763504\n",
      "[step: 27500] loss: 0.003108973614871502\n",
      "[step: 27550] loss: 0.0029223263263702393\n",
      "[step: 27600] loss: 0.0029152175411581993\n",
      "[step: 27650] loss: 0.002908156020566821\n",
      "[step: 27700] loss: 0.03385213762521744\n",
      "[step: 27750] loss: 0.0029502883553504944\n",
      "[step: 27800] loss: 0.002900818595662713\n",
      "[step: 27850] loss: 0.002892645075917244\n",
      "[step: 27900] loss: 0.0028857295401394367\n",
      "[step: 27950] loss: 0.0032186012249439955\n",
      "[step: 28000] loss: 0.004189528524875641\n",
      "[step: 28050] loss: 0.0028748451732099056\n",
      "[step: 28100] loss: 0.0028678779490292072\n",
      "[step: 28150] loss: 0.0028609461151063442\n",
      "[step: 28200] loss: 0.029621712863445282\n",
      "[step: 28250] loss: 0.0030966566409915686\n",
      "[step: 28300] loss: 0.002853408921509981\n",
      "[step: 28350] loss: 0.002844685921445489\n",
      "[step: 28400] loss: 0.0028378241695463657\n",
      "[step: 28450] loss: 0.0077224536798894405\n",
      "[step: 28500] loss: 0.0034548239782452583\n",
      "[step: 28550] loss: 0.002830222714692354\n",
      "[step: 28600] loss: 0.0028229993768036366\n",
      "[step: 28650] loss: 0.002816250314936042\n",
      "[step: 28700] loss: 0.002809607656672597\n",
      "[step: 28750] loss: 0.015080981887876987\n",
      "[step: 28800] loss: 0.002809482626616955\n",
      "[step: 28850] loss: 0.0028013314586132765\n",
      "[step: 28900] loss: 0.0027945362962782383\n",
      "[step: 28950] loss: 0.0027879762928932905\n",
      "[step: 29000] loss: 0.08279706537723541\n",
      "[step: 29050] loss: 0.0028126153629273176\n",
      "[step: 29100] loss: 0.002779286587610841\n",
      "[step: 29150] loss: 0.002771625528112054\n",
      "[step: 29200] loss: 0.0027654776349663734\n",
      "[step: 29250] loss: 0.010958528146147728\n",
      "[step: 29300] loss: 0.002807375742122531\n",
      "[step: 29350] loss: 0.0027562277391552925\n",
      "[step: 29400] loss: 0.0027493592351675034\n",
      "[step: 29450] loss: 0.0027448534965515137\n",
      "[step: 29500] loss: 0.007496118079870939\n",
      "[step: 29550] loss: 0.002783195348456502\n",
      "[step: 29600] loss: 0.0027349251322448254\n",
      "[step: 29650] loss: 0.0027285858523100615\n",
      "[step: 29700] loss: 0.0027223066426813602\n",
      "[step: 29750] loss: 0.006542102433741093\n",
      "[step: 29800] loss: 0.0029171935748308897\n",
      "[step: 29850] loss: 0.00271619507111609\n",
      "[step: 29900] loss: 0.002708577085286379\n",
      "[step: 29950] loss: 0.002702406607568264\n",
      "[step: 30000] loss: 0.00623871386051178\n",
      "[step: 30050] loss: 0.003736774902790785\n",
      "[step: 30100] loss: 0.0026950393803417683\n",
      "[step: 30150] loss: 0.002688253065571189\n",
      "[step: 30200] loss: 0.002682069083675742\n",
      "[step: 30250] loss: 0.0033032794017344713\n",
      "[step: 30300] loss: 0.003168037859722972\n",
      "[step: 30350] loss: 0.0026746601797640324\n",
      "[step: 30400] loss: 0.0026677444111555815\n",
      "[step: 30450] loss: 0.0026616519317030907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 30500] loss: 0.006043482571840286\n",
      "[step: 30550] loss: 0.0038187927566468716\n",
      "[step: 30600] loss: 0.002655531745404005\n",
      "[step: 30650] loss: 0.0026479160878807306\n",
      "[step: 30700] loss: 0.002641932340338826\n",
      "[step: 30750] loss: 0.002649498637765646\n",
      "[step: 30800] loss: 0.0040956041775643826\n",
      "[step: 30850] loss: 0.0026591969653964043\n",
      "[step: 30900] loss: 0.0026302561163902283\n",
      "[step: 30950] loss: 0.0026242334861308336\n",
      "[step: 31000] loss: 0.0026183833833783865\n",
      "[step: 31050] loss: 0.0034867534413933754\n",
      "[step: 31100] loss: 0.004311629571020603\n",
      "[step: 31150] loss: 0.00262302253395319\n",
      "[step: 31200] loss: 0.0026063371915370226\n",
      "[step: 31250] loss: 0.0026005543768405914\n",
      "[step: 31300] loss: 0.0025948036927729845\n",
      "[step: 31350] loss: 0.053183671087026596\n",
      "[step: 31400] loss: 0.00260361866094172\n",
      "[step: 31450] loss: 0.0025904481299221516\n",
      "[step: 31500] loss: 0.002582871587947011\n",
      "[step: 31550] loss: 0.0025771812070161104\n",
      "[step: 31600] loss: 0.0025797744747251272\n",
      "[step: 31650] loss: 0.007348580751568079\n",
      "[step: 31700] loss: 0.002596771577373147\n",
      "[step: 31750] loss: 0.0025652849581092596\n",
      "[step: 31800] loss: 0.0025595889892429113\n",
      "[step: 31850] loss: 0.0025539707858115435\n",
      "[step: 31900] loss: 0.053028859198093414\n",
      "[step: 31950] loss: 0.002615333301946521\n",
      "[step: 32000] loss: 0.0025528946425765753\n",
      "[step: 32050] loss: 0.002544576535001397\n",
      "[step: 32100] loss: 0.002539030509069562\n",
      "[step: 32150] loss: 0.002533523365855217\n",
      "[step: 32200] loss: 0.0025293659418821335\n",
      "[step: 32250] loss: 0.0033597496803849936\n",
      "[step: 32300] loss: 0.0025415446143597364\n",
      "[step: 32350] loss: 0.0025222760159522295\n",
      "[step: 32400] loss: 0.0025167730636894703\n",
      "[step: 32450] loss: 0.002511317143216729\n",
      "[step: 32500] loss: 0.02516022138297558\n",
      "[step: 32550] loss: 0.002950585912913084\n",
      "[step: 32600] loss: 0.0025119290221482515\n",
      "[step: 32650] loss: 0.0025002791080623865\n",
      "[step: 32700] loss: 0.002494840882718563\n",
      "[step: 32750] loss: 0.0024899921845644712\n",
      "[step: 32800] loss: 0.009154785424470901\n",
      "[step: 32850] loss: 0.002554241567850113\n",
      "[step: 32900] loss: 0.002483210526406765\n",
      "[step: 32950] loss: 0.0024773557670414448\n",
      "[step: 33000] loss: 0.0024719960056245327\n",
      "[step: 33050] loss: 0.006500716786831617\n",
      "[step: 33100] loss: 0.004664269275963306\n",
      "[step: 33150] loss: 0.002480702940374613\n",
      "[step: 33200] loss: 0.0024673836305737495\n",
      "[step: 33250] loss: 0.0024620776530355215\n",
      "[step: 33300] loss: 0.0024568696971982718\n",
      "[step: 33350] loss: 0.002451705513522029\n",
      "[step: 33400] loss: 0.002446521073579788\n",
      "[step: 33450] loss: 0.002744255820289254\n",
      "[step: 33500] loss: 0.004399093333631754\n",
      "[step: 33550] loss: 0.0024566128849983215\n",
      "[step: 33600] loss: 0.0024364637210965157\n",
      "[step: 33650] loss: 0.0024312064051628113\n",
      "[step: 33700] loss: 0.002426022430881858\n",
      "[step: 33750] loss: 0.0024740376975387335\n",
      "[step: 33800] loss: 0.002437747549265623\n",
      "[step: 33850] loss: 0.002425258979201317\n",
      "[step: 33900] loss: 0.0024194971192628145\n",
      "[step: 33950] loss: 0.0024142360780388117\n",
      "[step: 34000] loss: 0.002409159205853939\n",
      "[step: 34050] loss: 0.00240407045930624\n",
      "[step: 34100] loss: 0.00240532960742712\n",
      "[step: 34150] loss: 0.008005079813301563\n",
      "[step: 34200] loss: 0.0024250419810414314\n",
      "[step: 34250] loss: 0.0023939681705087423\n",
      "[step: 34300] loss: 0.002388731110841036\n",
      "[step: 34350] loss: 0.0023836372420191765\n",
      "[step: 34400] loss: 0.002435307251289487\n",
      "[step: 34450] loss: 0.003355394583195448\n",
      "[step: 34500] loss: 0.0023970857728272676\n",
      "[step: 34550] loss: 0.0023777002934366465\n",
      "[step: 34600] loss: 0.002372551243752241\n",
      "[step: 34650] loss: 0.0023675893899053335\n",
      "[step: 34700] loss: 0.0023626082111150026\n",
      "[step: 34750] loss: 0.0023583860602229834\n",
      "[step: 34800] loss: 0.009927520528435707\n",
      "[step: 34850] loss: 0.0024071948137134314\n",
      "[step: 34900] loss: 0.0023523017298430204\n",
      "[step: 34950] loss: 0.0023472649045288563\n",
      "[step: 35000] loss: 0.0023423011880367994\n",
      "[step: 35050] loss: 0.002395479241386056\n",
      "[step: 35100] loss: 0.005876154638826847\n",
      "[step: 35150] loss: 0.0023540223482996225\n",
      "[step: 35200] loss: 0.0023370503913611174\n",
      "[step: 35250] loss: 0.002332051983103156\n",
      "[step: 35300] loss: 0.0023271734826266766\n",
      "[step: 35350] loss: 0.002322309883311391\n",
      "[step: 35400] loss: 0.002317431615665555\n",
      "[step: 35450] loss: 0.0048977951519191265\n",
      "[step: 35500] loss: 0.003192841075360775\n",
      "[step: 35550] loss: 0.002317379927262664\n",
      "[step: 35600] loss: 0.0023122106213122606\n",
      "[step: 35650] loss: 0.0023072760086506605\n",
      "[step: 35700] loss: 0.0023024743422865868\n",
      "[step: 35750] loss: 0.0022976750042289495\n",
      "[step: 35800] loss: 0.0022928465623408556\n",
      "[step: 35850] loss: 0.026227371767163277\n",
      "[step: 35900] loss: 0.0026417160406708717\n",
      "[step: 35950] loss: 0.0022917704191058874\n",
      "[step: 36000] loss: 0.002285667462274432\n",
      "[step: 36050] loss: 0.0022808436769992113\n",
      "[step: 36100] loss: 0.0022760736756026745\n",
      "[step: 36150] loss: 0.002271281322464347\n",
      "[step: 36200] loss: 0.056866928935050964\n",
      "[step: 36250] loss: 0.0022798620630055666\n",
      "[step: 36300] loss: 0.0022722878493368626\n",
      "[step: 36350] loss: 0.0022659776732325554\n",
      "[step: 36400] loss: 0.0022612069733440876\n",
      "[step: 36450] loss: 0.002256489358842373\n",
      "[step: 36500] loss: 0.002251757076010108\n",
      "[step: 36550] loss: 0.003878652350977063\n",
      "[step: 36600] loss: 0.002729280386120081\n",
      "[step: 36650] loss: 0.0022538616321980953\n",
      "[step: 36700] loss: 0.002242504619061947\n",
      "[step: 36750] loss: 0.0022377469576895237\n",
      "[step: 36800] loss: 0.002233189297839999\n",
      "[step: 36850] loss: 0.002935620956122875\n",
      "[step: 36900] loss: 0.0023368652909994125\n",
      "[step: 36950] loss: 0.0022278649266809225\n",
      "[step: 37000] loss: 0.002222419483587146\n",
      "[step: 37050] loss: 0.002217938657850027\n",
      "[step: 37100] loss: 0.010151195339858532\n",
      "[step: 37150] loss: 0.0022508760448545218\n",
      "[step: 37200] loss: 0.0022132096346467733\n",
      "[step: 37250] loss: 0.002207969781011343\n",
      "[step: 37300] loss: 0.002203328302130103\n",
      "[step: 37350] loss: 0.01651696115732193\n",
      "[step: 37400] loss: 0.002234079409390688\n",
      "[step: 37450] loss: 0.0021971443202346563\n",
      "[step: 37500] loss: 0.0021923286840319633\n",
      "[step: 37550] loss: 0.0021887430921196938\n",
      "[step: 37600] loss: 0.010842457413673401\n",
      "[step: 37650] loss: 0.002231443766504526\n",
      "[step: 37700] loss: 0.0021827686578035355\n",
      "[step: 37750] loss: 0.0021779206581413746\n",
      "[step: 37800] loss: 0.002173262881115079\n",
      "[step: 37850] loss: 0.08658896386623383\n",
      "[step: 37900] loss: 0.0023815229069441557\n",
      "[step: 37950] loss: 0.002172259148210287\n",
      "[step: 38000] loss: 0.002165598329156637\n",
      "[step: 38050] loss: 0.0021609393879771233\n",
      "[step: 38100] loss: 0.0021563731133937836\n",
      "[step: 38150] loss: 0.011573102325201035\n",
      "[step: 38200] loss: 0.0021635571029037237\n",
      "[step: 38250] loss: 0.0021517700515687466\n",
      "[step: 38300] loss: 0.0021464538294821978\n",
      "[step: 38350] loss: 0.0021417902316898108\n",
      "[step: 38400] loss: 0.011412172578275204\n",
      "[step: 38450] loss: 0.0030975707340985537\n",
      "[step: 38500] loss: 0.002136220457032323\n",
      "[step: 38550] loss: 0.0021311042364686728\n",
      "[step: 38600] loss: 0.002126451348885894\n",
      "[step: 38650] loss: 0.030261438339948654\n",
      "[step: 38700] loss: 0.0025312756188213825\n",
      "[step: 38750] loss: 0.0021226562093943357\n",
      "[step: 38800] loss: 0.002117748372256756\n",
      "[step: 38850] loss: 0.002113174181431532\n",
      "[step: 38900] loss: 0.002109645400196314\n",
      "[step: 38950] loss: 0.002255715662613511\n",
      "[step: 39000] loss: 0.002159595489501953\n",
      "[step: 39050] loss: 0.002103052567690611\n",
      "[step: 39100] loss: 0.002098330296576023\n",
      "[step: 39150] loss: 0.002115052193403244\n",
      "[step: 39200] loss: 0.005758278537541628\n",
      "[step: 39250] loss: 0.0021162184420973063\n",
      "[step: 39300] loss: 0.002088063396513462\n",
      "[step: 39350] loss: 0.0020835157483816147\n",
      "[step: 39400] loss: 0.003009555395692587\n",
      "[step: 39450] loss: 0.002270895754918456\n",
      "[step: 39500] loss: 0.002080862643197179\n",
      "[step: 39550] loss: 0.002073233248665929\n",
      "[step: 39600] loss: 0.0020687184296548367\n",
      "[step: 39650] loss: 0.01722411997616291\n",
      "[step: 39700] loss: 0.0028555800672620535\n",
      "[step: 39750] loss: 0.0020679922308772802\n",
      "[step: 39800] loss: 0.0020593886729329824\n",
      "[step: 39850] loss: 0.0020548743195831776\n",
      "[step: 39900] loss: 0.0020505469292402267\n",
      "[step: 39950] loss: 0.01619429886341095\n",
      "[step: 40000] loss: 0.0020883665420114994\n",
      "[step: 40050] loss: 0.002047889167442918\n",
      "[step: 40100] loss: 0.002043271902948618\n",
      "[step: 40150] loss: 0.002038896083831787\n",
      "[step: 40200] loss: 0.0020345845259726048\n",
      "[step: 40250] loss: 0.0039399778470396996\n",
      "[step: 40300] loss: 0.002189665101468563\n",
      "[step: 40350] loss: 0.002029889263212681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 40400] loss: 0.0020245579071342945\n",
      "[step: 40450] loss: 0.0020202072337269783\n",
      "[step: 40500] loss: 0.0025834739208221436\n",
      "[step: 40550] loss: 0.0020845551043748856\n",
      "[step: 40600] loss: 0.0020169883500784636\n",
      "[step: 40650] loss: 0.00201222556643188\n",
      "[step: 40700] loss: 0.002007913775742054\n",
      "[step: 40750] loss: 0.0020038350485265255\n",
      "[step: 40800] loss: 0.011317586526274681\n",
      "[step: 40850] loss: 0.0020099999383091927\n",
      "[step: 40900] loss: 0.002000769367441535\n",
      "[step: 40950] loss: 0.001996035221964121\n",
      "[step: 41000] loss: 0.0019918116740882397\n",
      "[step: 41050] loss: 0.001998401479795575\n",
      "[step: 41100] loss: 0.006299569271504879\n",
      "[step: 41150] loss: 0.001987363211810589\n",
      "[step: 41200] loss: 0.0019828900694847107\n",
      "[step: 41250] loss: 0.001978654880076647\n",
      "[step: 41300] loss: 0.001977133797481656\n",
      "[step: 41350] loss: 0.009178830310702324\n",
      "[step: 41400] loss: 0.0019795072730630636\n",
      "[step: 41450] loss: 0.001969641540199518\n",
      "[step: 41500] loss: 0.001965255243703723\n",
      "[step: 41550] loss: 0.0020037649665027857\n",
      "[step: 41600] loss: 0.004880208056420088\n",
      "[step: 41650] loss: 0.001974929589778185\n",
      "[step: 41700] loss: 0.001955917105078697\n",
      "[step: 41750] loss: 0.0019517894834280014\n",
      "[step: 41800] loss: 0.0027618405874818563\n",
      "[step: 41850] loss: 0.0034412993118166924\n",
      "[step: 41900] loss: 0.0019478753674775362\n",
      "[step: 41950] loss: 0.0019435378490015864\n",
      "[step: 42000] loss: 0.001939433510415256\n",
      "[step: 42050] loss: 0.0019628817681223154\n",
      "[step: 42100] loss: 0.002135450951755047\n",
      "[step: 42150] loss: 0.0019547995179891586\n",
      "[step: 42200] loss: 0.0019296073587611318\n",
      "[step: 42250] loss: 0.0019255011575296521\n",
      "[step: 42300] loss: 0.045308344066143036\n",
      "[step: 42350] loss: 0.002041915664449334\n",
      "[step: 42400] loss: 0.0019222479313611984\n",
      "[step: 42450] loss: 0.0019174462649971247\n",
      "[step: 42500] loss: 0.0019134467002004385\n",
      "[step: 42550] loss: 0.0027339900843799114\n",
      "[step: 42600] loss: 0.0036714039742946625\n",
      "[step: 42650] loss: 0.001921584247611463\n",
      "[step: 42700] loss: 0.0019059859914705157\n",
      "[step: 42750] loss: 0.0019020568579435349\n",
      "[step: 42800] loss: 0.0018981272587552667\n",
      "[step: 42850] loss: 0.0018942478345707059\n",
      "[step: 42900] loss: 0.004851195961236954\n",
      "[step: 42950] loss: 0.0019428231753408909\n",
      "[step: 43000] loss: 0.001897539827041328\n",
      "[step: 43050] loss: 0.0018931851955130696\n",
      "[step: 43100] loss: 0.001889323815703392\n",
      "[step: 43150] loss: 0.0018855585949495435\n",
      "[step: 43200] loss: 0.0018817880190908909\n",
      "[step: 43250] loss: 0.001878010923974216\n",
      "[step: 43300] loss: 0.001882235985249281\n",
      "[step: 43350] loss: 0.001920120557770133\n",
      "[step: 43400] loss: 0.0019060004269704223\n",
      "[step: 43450] loss: 0.0018715433543547988\n",
      "[step: 43500] loss: 0.001867538783699274\n",
      "[step: 43550] loss: 0.001863781362771988\n",
      "[step: 43600] loss: 0.0018600118346512318\n",
      "[step: 43650] loss: 0.031830012798309326\n",
      "[step: 43700] loss: 0.002104438841342926\n",
      "[step: 43750] loss: 0.0018615289591252804\n",
      "[step: 43800] loss: 0.0018575828289613128\n",
      "[step: 43850] loss: 0.0018538166768848896\n",
      "[step: 43900] loss: 0.0018501433078199625\n",
      "[step: 43950] loss: 0.0018465047469362617\n",
      "[step: 44000] loss: 0.0018431143835186958\n",
      "[step: 44050] loss: 0.011718533001840115\n",
      "[step: 44100] loss: 0.0018440367421135306\n",
      "[step: 44150] loss: 0.0018386861775070429\n",
      "[step: 44200] loss: 0.001834385795518756\n",
      "[step: 44250] loss: 0.001881164382211864\n",
      "[step: 44300] loss: 0.001869111554697156\n",
      "[step: 44350] loss: 0.0018486137269064784\n",
      "[step: 44400] loss: 0.001826662104576826\n",
      "[step: 44450] loss: 0.0018229526467621326\n",
      "[step: 44500] loss: 0.001820346456952393\n",
      "[step: 44550] loss: 0.0033578432630747557\n",
      "[step: 44600] loss: 0.0018412495264783502\n",
      "[step: 44650] loss: 0.0018163025379180908\n",
      "[step: 44700] loss: 0.0018125196220353246\n",
      "[step: 44750] loss: 0.001808895613066852\n",
      "[step: 44800] loss: 0.0018064737087115645\n",
      "[step: 44850] loss: 0.0066414824686944485\n",
      "[step: 44900] loss: 0.001819753204472363\n",
      "[step: 44950] loss: 0.0018045854521915317\n",
      "[step: 45000] loss: 0.0018008423503488302\n",
      "[step: 45050] loss: 0.0017972505884245038\n",
      "[step: 45100] loss: 0.001793739851564169\n",
      "[step: 45150] loss: 0.002022326225414872\n",
      "[step: 45200] loss: 0.003755968064069748\n",
      "[step: 45250] loss: 0.0018028782214969397\n",
      "[step: 45300] loss: 0.0017859813524410129\n",
      "[step: 45350] loss: 0.0017823521047830582\n",
      "[step: 45400] loss: 0.001779236365109682\n",
      "[step: 45450] loss: 0.007227216847240925\n",
      "[step: 45500] loss: 0.0018347708974033594\n",
      "[step: 45550] loss: 0.0017781152855604887\n",
      "[step: 45600] loss: 0.0017743082717061043\n",
      "[step: 45650] loss: 0.0017708019586279988\n",
      "[step: 45700] loss: 0.0017673310358077288\n",
      "[step: 45750] loss: 0.002498091198503971\n",
      "[step: 45800] loss: 0.0030944158788770437\n",
      "[step: 45850] loss: 0.0017727339873090386\n",
      "[step: 45900] loss: 0.0017594518139958382\n",
      "[step: 45950] loss: 0.00175593257881701\n",
      "[step: 46000] loss: 0.11356395483016968\n",
      "[step: 46050] loss: 0.002125880913808942\n",
      "[step: 46100] loss: 0.0017562629655003548\n",
      "[step: 46150] loss: 0.0017489143647253513\n",
      "[step: 46200] loss: 0.001745484652929008\n",
      "[step: 46250] loss: 0.0030279553029686213\n",
      "[step: 46300] loss: 0.0018217541510239244\n",
      "[step: 46350] loss: 0.0017472229665145278\n",
      "[step: 46400] loss: 0.0017377897165715694\n",
      "[step: 46450] loss: 0.0017343282233923674\n",
      "[step: 46500] loss: 0.015173405408859253\n",
      "[step: 46550] loss: 0.0018224320374429226\n",
      "[step: 46600] loss: 0.0017316356534138322\n",
      "[step: 46650] loss: 0.0017264982452616096\n",
      "[step: 46700] loss: 0.0017236710991710424\n",
      "[step: 46750] loss: 0.010988330468535423\n",
      "[step: 46800] loss: 0.0017299196915701032\n",
      "[step: 46850] loss: 0.0017190189100801945\n",
      "[step: 46900] loss: 0.001715586637146771\n",
      "[step: 46950] loss: 0.0017136534443125129\n",
      "[step: 47000] loss: 0.008932949975132942\n",
      "[step: 47050] loss: 0.0017594753298908472\n",
      "[step: 47100] loss: 0.001708820229396224\n",
      "[step: 47150] loss: 0.0017053234623745084\n",
      "[step: 47200] loss: 0.0017019693041220307\n",
      "[step: 47250] loss: 0.017152242362499237\n",
      "[step: 47300] loss: 0.001806655083782971\n",
      "[step: 47350] loss: 0.0017005938570946455\n",
      "[step: 47400] loss: 0.0016955595929175615\n",
      "[step: 47450] loss: 0.0016922298818826675\n",
      "[step: 47500] loss: 0.0016928974073380232\n",
      "[step: 47550] loss: 0.007046629209071398\n",
      "[step: 47600] loss: 0.001690965611487627\n",
      "[step: 47650] loss: 0.0016862675547599792\n",
      "[step: 47700] loss: 0.001682880101725459\n",
      "[step: 47750] loss: 0.001679602894000709\n",
      "[step: 47800] loss: 0.002235502703115344\n",
      "[step: 47850] loss: 0.0035241141449660063\n",
      "[step: 47900] loss: 0.0016848095692694187\n",
      "[step: 47950] loss: 0.0016733909724280238\n",
      "[step: 48000] loss: 0.0016700041014701128\n",
      "[step: 48050] loss: 0.001666756928898394\n",
      "[step: 48100] loss: 0.026901092380285263\n",
      "[step: 48150] loss: 0.0017918396042659879\n",
      "[step: 48200] loss: 0.001665243529714644\n",
      "[step: 48250] loss: 0.0016605654964223504\n",
      "[step: 48300] loss: 0.0016572820022702217\n",
      "[step: 48350] loss: 0.0016642286209389567\n",
      "[step: 48400] loss: 0.006411683280020952\n",
      "[step: 48450] loss: 0.001666293479502201\n",
      "[step: 48500] loss: 0.0016507997643202543\n",
      "[step: 48550] loss: 0.0016473880968987942\n",
      "[step: 48600] loss: 0.0016442452324554324\n",
      "[step: 48650] loss: 0.006028083153069019\n",
      "[step: 48700] loss: 0.0016646282747387886\n",
      "[step: 48750] loss: 0.0016411117976531386\n",
      "[step: 48800] loss: 0.0016374532133340836\n",
      "[step: 48850] loss: 0.0016342314193025231\n",
      "[step: 48900] loss: 0.01481194794178009\n",
      "[step: 48950] loss: 0.001697688945569098\n",
      "[step: 49000] loss: 0.0016339252470061183\n",
      "[step: 49050] loss: 0.0016279123956337571\n",
      "[step: 49100] loss: 0.0016246726736426353\n",
      "[step: 49150] loss: 0.0016215097857639194\n",
      "[step: 49200] loss: 0.016562415286898613\n",
      "[step: 49250] loss: 0.0017582933651283383\n",
      "[step: 49300] loss: 0.001620899303816259\n",
      "[step: 49350] loss: 0.0016170705202966928\n",
      "[step: 49400] loss: 0.0016138573409989476\n",
      "[step: 49450] loss: 0.0016107144765555859\n",
      "[step: 49500] loss: 0.0016648321179673076\n",
      "[step: 49550] loss: 0.001702535548247397\n",
      "[step: 49600] loss: 0.001616060035303235\n",
      "[step: 49650] loss: 0.001603824202902615\n",
      "[step: 49700] loss: 0.0016006302321329713\n",
      "[step: 49750] loss: 0.001599682029336691\n",
      "[step: 49800] loss: 0.0076448796316981316\n",
      "[step: 49850] loss: 0.0016016217414289713\n",
      "[step: 49900] loss: 0.0015948176151141524\n",
      "[step: 49950] loss: 0.0015916051343083382\n",
      "[step: 50000] loss: 0.0015884627355262637\n"
     ]
    }
   ],
   "source": [
    "for i in range(iterations):\n",
    "    _  , cost = sess.run([train ,loss], feed_dict={X: trainX, Y: trainY})\n",
    "    if (i+1) % (iterations/1000) == 0:\n",
    "        print(\"[step: {}] loss: {}\".format(i+1, cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06598569191932\n"
     ]
    }
   ],
   "source": [
    "a=0\n",
    "length=len(dataY)\n",
    "for k in range(0,length-1):\n",
    "    a+=dataY[k][0]\n",
    "real_avg=a/length\n",
    "\n",
    "test_predict = sess.run(Y_pred, feed_dict={X: testX}) #예측값\n",
    "length_=len(test_predict)\n",
    "\n",
    "b=0\n",
    "for j in range(0,length_-1):\n",
    "    b+=test_predict[j][0]\n",
    "pred_avg=b/length_\n",
    "    \n",
    "#(절대값(실제값의 평균 - 예측값의 평균)/실제값의 평균) *100 =평균 오차율 산정 방식\n",
    "accuracy=(abs(real_avg-pred_avg)/real_avg)*100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEJCAYAAABhbdtlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOy9d3ikV3m/f5/pkmbURnWLyu5q+663ed3N2sa44JgSiolJQkIwISEQyDeAA6EFAj9IKCGmhRaqbVpsgzEGbNy9zfaut1ftrro06tPL+f1xpqhrRpp3ZrQ693Xpuqa878yZ0Tvnc55ynkdIKdFoNBqNZipM+R6ARqPRaAoXLRIajUajmRYtEhqNRqOZFi0SGo1Go5kWLRIajUajmRYtEhqNRqOZFsNEQgjxHSFEjxDi0DTPCyHEfwkhTgkhDgohthk1Fo1Go9HMDSMtie8BN8/w/C1AS/zvLuBrBo5Fo9FoNHPAYtQLSymfFEI0zXDIa4DvS7Wb73khRLkQol5K2TnT61ZVVcmmppleVqPRaDQT2b9/f5+UsjrT8wwTiTRYClwYc78t/tiMItHU1MS+ffuMHJdGo9FcdAghzs3lvHwGrsUUj01ZI0QIcZcQYp8QYl9vb6/Bw9JoNBpNgnyKRBuwfMz9ZUDHVAdKKb8ppdwhpdxRXZ2xtaTRaDSaOZJPkXgQ+It4ltPlwNBs8QiNRqPR5BbDYhJCiJ8Au4AqIUQb8DHACiCl/DrwMHArcArwAX9l1Fg0Go1GMzeMzG56yyzPS+DvjXp/jUaj0cwfveNao9FoNNOiRUKj0Wg006JFQqPRaDJk4ImDdP782XwPIyfkczOdRqPRLDykpHrXeqJYWAzdn7UlodFoNBkQ2H+Y6CJaX2uR0Gg0mgx4ebcv30PIKVokNBqNJgM8HcHk7Wg4lseR5AYtEhqNRpMBA72R5O3hjpE8jiQ3aJHQaDSaDBjoiyZvD7Z58ziS3KBFQqPRaDJgcDB1e6jLn7+B5AgtEhqNRpMBA4OpLgeDHZkHsbv/cIjhwxdmP7BA0CKh0Wg0GTAwkkp//ev3lGR8ft0rN3LZpoXjptIiodFoNBkw6LWylDYAzrIio3MDvSrQfUyuzfq4jEKLhEaj0WRAv89BU2k/d5s/h4UwMpb+tuvTj583cGTGoEVCo9FoMqDH76TG5af8VTuJYMXnST94ffy5fgBqTT1GDS/raJHQaDSaDOgJV1BTHqasQgWwh9rS3yvRejIMwLIijyFjMwItEhqNRpMmEV+IPummtjpGeaUZyCzDqb1DCUuxOWTI+IxAi4RGU2DIUJh/ufZpnv320XwPRTOBvhP9SEzU1gnKq60ADHam725q77UBEI6ZDRmfEWiR0GgKjPve+RifeepqPv6v0dkP1uSUnuMDANQss1FWYwdgsDs40ynjaBtyAhCOLZypd+GMVKNZJOw/qFaoDaWDsxypyTVt+7sBqF9TSnl9EQBDfem7jtp9lQCEYgun1LgWCY2mwOjuVyLhD4pZjtTkmr0PdSKIsekNa6hYokTC0zt9CuypR07x5H/uBUBGY3REa4CFJRILZ6QazSKhZ9gBgDeof56Fxp72pax3XaC0vBHnWjc2glw4P71IXHFLGX2swvc3IUY7hglRBUB4AYmEtiQ0mgKjx6f81t6QNc8j0UykLVjDSlcvACZXCQ3mdlrbpp/w+6gG4PdfO0n7QZX2WsYgIalFQqPRzJHuUDkAPi0SBYc/ZqPYluon0eTs45xnivpNUvLZ255O3n3x8UHajw6rcxxdhOTC+d9qkdBoCggZjdEbcwPgjdjyPBrNRAIxGw57qhtdU5WXU6N1RCckoh35xTHu/vXVyfunW820nQoAsKJ8gLC2JDQazVzoeKGLMEocvBEHw51ePvsnzxAN6XTYfBKLSi7s7cIvHRSNEYlXvSKIJ1bJw184Nu74c4dHAfir4vvYYjrA6R4XzzwtqRT9NNYGtCWh0WjmxolnlL+7yXweX8zO3155kLt/dRWPf/lgnke2uPn0TU/SsLOOPllFkT0VqH7N564C4MVHuscd39+p9k588Oc72drQz4mROh45t45bVp7E4ZCE0SKh0WjmwMkXVB2gLVVteGNF/O5cCwBWu/6p5pOfP1ObvF3kSImEze3CRhDvmPYQ+777Mm/9unI1uZtL2bVjhN6om16qWb8ygM0KIezI9IvH5hV95Wk0BcThw5IifKxeHsAri+mTKmXSPxKZ5UyNkfgi9uTtoqLxz5UIH15fak/LQ9/tS96uaCrjdZ/clrxf6TZhjRsRkVDKbVXIaJHIkFMPHOazl/+S1sfO5HsomouQJ47WcEXZUVwuiIxxSfiGtUjEYuRt9e2LzSQSfryB1FRa6koN0my34Fq3LHnfXWPGFs9HCPvCxgw2y2iRyJB3vNXP3btfR/MNK2jf25Hv4WguIg79y4856G/huu3DlEzIqlyIIuEdCPHOS/dz/sBAVl7vTZuP0eDMT4ltbyylDEXF43fCl5gDeAOpgn3+4PTTamW9PSkSIa8WiYuSYkcqy2T/A215HInmYuOrP6+lGB/v/O7lXLZ9vCj4vAvDNTGWJ796iG/u2851l6dfSns6gsNBfn54LW0+d0ad4LLF2EBzUcn4abPEEhi3O97TP305lcqlDqw29XzItzCEX4tEhjjHiMQLz6RfIlijmY2hgI06q4fqhiKu+PD13FT7Eu+7/DkAfKMLJMo5hpMvqWjumcBSAt75pfAeeijl3u07m36Tn2wQDUbw4kzedxRPEAlraNzueM+QsioO/OUXJr1W5XIntrjnSrubLlJ6Rx1cVXqQ1dYzvHzSke/haC4i/CELRfFmNMJq4ZGuLXzmN1sA8HkXnkgcP566/fxPzs7rtVoPDCdvdxzsm+HI7DPcPl6Uipzje0GUWMN4w6mNj30jdrYXH2Hz994/6bUqG13akrjY6fW7qC7x0VLeyy/aL+Pkb07le0iaiwR/2EyRefzq0lbqwEQU3/w9Njnn0IVSlop2AI4/Nb+ezmePpXo2tB/LrSUx1D467v5EkXDaw3jHZD95fEW4i8b/w76y4guUMYhzSSm2eDpzyL8wNkgaKhJCiJuFEMeFEKeEEB+a4vkGIcTjQogXhRAHhRC3GjmebNAbLqO6LMSKOuVqWn3rqjyPSHOx4A9bKbKMFwlhEhTjw+dfWGXDA4MBdg+u4bXrlDnR0zm/CfFsa+rzd5zOrZt3qHP8hD/JknBE8UZSXgVP0IXbOb4R0btfeDuD+88grBasdvVZFr27SQhhBu4BbgHWA28RQqyfcNhHgPullFuBO4CvGjWebBANRemLVVJdGSMaXXjmv6aw8UesFFknuyCKhX/BicQL954giIMbX+ukjEF6e+f3eic7nTSaLgDgH8ptf+jBLlVzyYQSulHfhJiEI4o3NkYkomW4Syf8H8vKYJvaL2G2KpFZKKVWjLQkdgKnpJRnpJQh4F7gNROOkUBp/HYZUNA5pW17OohioWmlmX/6UkPy8cHzwzOcpdGkhz9qpcg2hUiYAviCC6cnMkDHSRW0bt5aTo2ln56BuZeh+NkH9/C7vq1srToPQDSS2wXaUK8SpY+v/gkAG9aNzzQrKZZ4ZTEAkUCEQVlOlXv6MVqsSvD1ZjpYClwYc78t/thYPg68VQjRBjwM/IOB45k3p59Xy6GVlzhZceNKHrj7eQBOPN4+p9eTMUn/6ezkkGsWPv6onSLb5NVlsTm44ETC063Ezt3opMYxTM/I3JM83vi5nQCsXKJW9JEcx3uH+pRb6I4vX4F87nkabt8y7vmSYkmAIqLhWPL37K6a3vIzx0VCWxIw1bc0UV7fAnxPSrkMuBX4gRBi0piEEHcJIfYJIfb1ztdunQdnDqiA2corVR2XulUqLa737Oi058zEBy57AveqCobOD2VngJoFjT9mo8g2eXVZbA7hCy0wkehVE6B7VQXVJT56fc5ZzpgeK2olf8efqalhYlluoxnyKFUqW+aCyy+f9HyJU011Po8fz1nlVXDXTW85mS3xz5Fji2iuGCkSbcDyMfeXMdmd9HbgfgAp5XOAA+L9/cYgpfymlHKHlHJHdXW1QcOdnbOno1gIs2xHHQBVzS4A+jqCM502Lf+xbxcA/a3aXaVhUhnqBHZLhEB4YYlEn0dQwiiOiiKqS0P0hkpnP2kaKsQQ71zxOza9Q03QuRaJ1nMCOwHcqyqmfD4hEt4+P33nlJutasn0vUDMlrglEdbupr1AixCiWQhhQwWmH5xwzHngBgAhxDqUSOTPVJiFzl4LdeZezFb1tVWtUh3E+royv2pjkdQF4u2fm8hoLi780jGuwmgCuzlKMLpwmtQAeAYtuM2DAJSXxhiKueb0OjIm6ZflVNZaMdviAd8cr8CPnStijb0Vs2Nq66DEpeYDb58fT5vKvHI3FE/7elok4kgpI8C7gd8CR1FZTIeFEJ8UQtweP+yfgHcIIQ4APwHeJmXhFtDtGnRQZx9M3nctLcVKiL6+zIc8NhbhG9AisdiRMYmfYoqLphAJS4RgZOGIxOHHe/j+6asYjCnrobxM+eyD3syDCcPtI0Sw4naTEokcWxJH+2tYWzV9zaiSMjUuryeAp1O5xtxN01tOyc+x2EUCQEr5sJRytZRypZTy0/HHPiqlfDB++4iU8iop5SVSyi1SykeNHM986fK6qHOl4g/CJKgy9dPXP7Mr4IH3/ZGH/t8T4x57+r9fSt72DS2MfGmNcQSHVFB2YoVRALslRiiWubvpvk+dTPZVziX3fvIEADGpVsxlFWqaGWrLfBNc/1kVr3PXmBFmE4JYTkUiEozSGl7K6sbpF3IlZcrC8A6E8PTEYzEry6c9PpndFC7Y9fA49I7rDOgKVlBXPv5iqbIO0zc8cy/iT36jhs/9T+qiiYWjvO6/b0je1yKh8Q+q62pihVEAuzVKMJZZCukfvn6SO/61hb/YdS4r40uXwXYvX31CbYfa960DAJS7lcB1nvJOe950eM6pRZm73gZCYCZKNIcL8K7DHmKYWdYw/VRZUp4SiT4P2AlQUlMy7fFJd5MOXF9cRENRemJV1FWPX8bUFI/QNTL9BQFKXEZDqW37QxfGr+4WYhloTXbxD8QtiSlFIpaRSAycG+KV71Id7SL+3F5bP/vIS/TLSt635mHWvF11ZyuvUYuoLbctm+nUKek+o4SlukGZWGaiObUkOl5WbqalK+zTHlNSEe9JPhjGM2jGbRpAmGZKgY1nN2l308WF54SHKBbqloz/ypqrvZz11Uw6PtbeSWzURzQUpTtWzWg0lSc+eGG82e0dWRj50hrjeOLeTgBq6+cvEm37U3WSVtfn1t3UeUGJ0mee25V8rLw2NcFmWub77GFVEqNpp/qNWYgQieRu93n7cWXJLFk7fYyhpFJ9Pu9wFM+wDbd1ZrdavgLwc0WLRJr8/tutANQ1jP+xrmiK0h2rwdsz3pQ2L6vnL5qfTIrLSDSV7TDYro4tQwXBfSMLY0WhMY7vfU+y2nKa2z5+6aTn7DYIypldmmPpPJGapELR3KbO9vQJyhnEXpG63svqUoGWwFBmSRpnz6h2rrWblEgod1MOReKMGu/STZXTHlPiVgtA73CUPl8RVUUz75vSlsRFyMk/nOfOL6ldn3UrxruWmteoVcTZZ8ZsAYknaP2o72Y6j6gsptFY6kcz0KlcCz/4sCp+5htdGBeLxjgOepZxRUM75pLJO5PtNpmRSHS1qjRMG0FGA7nNiurut1FrHZ8J5KxKfSZvb2blbM92Omi2tSPMaqoyi9wGrs+cAQd+qtdPvz+rpEqJoHckRm/AhbtkZiHUMYmLDNnn4YOv3Je8X7embNzzDRtU/veFl1OpsaHRVAGyrpNqVefFmdwbMditLqKECbsQy0BrskfvqSG6YjVsXjd1/MBulwSZ3ic+kc4LahZdZTs/rmNaLugZcVBbNN7F1Xh5PUtRXRxHezK72M8PltJYmkoXz3Xg+sXTpWwuOoXJNv33WFKtFoDv++W1nAw1snnVzJ/RYlPTbkSLxMXBC98/xC95ffJ+7Xr3uOdLa9QqaWQglaE02p1yPf3uoUDytq9PXTyDverYqmYXNoJaJBY5z/9YdV3bdu3UpSvsNohiSds9cb7NhJMRaotHxnVMMxopoXW0mhrn+FLewmHnC+9VxflGPZm5m/pCLqpcqUWXmRjRaG7cTVLCSwMNbF028/5eW2nKUpKYuObWmTcOJt1NCyRfRYvELJw/Oj7W4Kwb/0N21ShTc2QgZQOPFYn/3H118vZIt1KDAY/6sZcvdy3IMtCa7PL7B70U4ePyt2+Y8nl7fA4KDs88wT7zH8/xv3/5GN86eCmvrD5AiS3CaDh9C2S+3Hf3S5yLLGXrysm1yJzlaiWeaXUBT6QMd1lqNjWL3FkSR/7QyaAsZ+f2WWZzIfjIih8l726+vWnGw/VmuouMCyfUqujpr73Mrz/y3KTnXXUqRjE6lBKJkd7ApOMARnvVa/X0qtr0riVKJLx+/W9YzOw/WcYO1wkc7qlTqe3xJjXBkZn7KNz4z5fwtu9fTwg7//Kpkkkd04zm0Qf8lDPABx+8etJzzgpl0Yz2p98LIuQNM4oLd2XKLWMWsZwFrh/7rtpjcv1fNsxyJNzwd2uTtytaJpWfG4fObrrION9uwiECXPnOTdz6b1dMej5hWYyMyXpLmNQrreM3MiVE4vcHa7is5DAmiwmbKUIohyl9msLjjLeGVTXTp6raHFOLxP67f0rfE4eT98tIvcYlb1pDSVEUb3SKLdwGsfd8DVdUHMdcPtndMnYvQbokdltXVqWmqVy6m376GydrLSdpumnNrMfWrEh5GBJB9ulIupsWSOa7FolZuOCtZLmtBzHNdWkrsWIjOKVIbKkZX/R21BPkxZ8c4wXfOl73in4ALCKas4teU3j4h8N0RmtZsXz6ydPuUD/TcSIhJTs++0ZesSv1UJFFPf/BlT/FVl5MSVFsXFadkUQjkmO+Bi5pnlrsEhlOo4PpO+I9Z+IlOWpTQWOLiBIx2JJ44et7uO/tj/LUwEbuvOoc0/74x1CzevoyHBNJxSQWhiWxcKqG5YnRiIMyy8zlBFxilBFvSm9H40Hs19wWZd//nGdn7Tl+2nkN1/7DJdzs3ouTEe762lZAmc+RmNbqxUrr7m5gGc0t0+9nsBep6yPkTQmJv3cUcHGEVBxjIFrKP2x+gs8eeCMAzhKJl2KkTGuemxe9Z0eJ4GJ549TX8ti9BOnS36ZieO4lKZeZikkY+2G2v2tn8vbWa9Lrg1G5avp9FBOx2NX/WtduukiIRE1YTDNf2C6zjxGfia++bQ+eM0OMDKrjL3/DclqjDdx7/qrksY94LmVN8QXKGlQqrfKxGjd+TWFzdp/aU9C8afrJyF482ZKY2IMkFokxJEspL0tNPCXFKitqbEq2UXQc7ANgSdPU+zmcNcqiGR1O/2K/cFyJRNWYstu5jEkANF1SNvtBgMmuYi6vK5m9Rmm+qtnOFS0SsxCRJsxiZsUvMof44dmr+fv/3cl7X3UkaVInMp9MFhPv3vJU8vjG8lT2hyUHKyNN4ZIoO7Fi5/SbtexFalIZW2p74ILa1StQk+5IxwgSE+Vj+uIkm+H0ZF5YL1M6jinRWtIydfDdWVuCmQh9nvSv9Z//0kSt6Gbj7SuSj+VaJBovnVxyZzoCL5/kp62Td8xPxGxLxCS0JXFREImZsJhmXv0cDa5M3vb7BaMj6p/vrE39YL7y4jWss50GoKF2TN63djctas6cjqmyE1uXTHuMvXgKkehQSRAOVCbdYJsSjfLKlNvKWaquq0TChJF0nFHjWLJh6u5tlmIbq61nOXQ6/UD68x3LuanxOBZnah9CrkXC2ZC+G8m+sQVz1dSffyxmayK7ac7Dyil6dpqFaBoiMZbq8lBSJIrd438QdcVqtdXYkHo9s5DakljEnG2302xtQ9im3/SWaJN7en9qV3+itEuRiItEvB5YeVUqzFhSmmqGYzTnzkQxE6F24/QW0ebqTvb2NLDnN9M38EkQDcfoilazfMn4mTSXv5ctvGhIMMdsV/8j7W66SIhIExbzzCLx4CdeTN72B0yMeFV/X5Nl/Nf7rx+R/N3ax3jTv7QkH7OYotqSWMRcGCihwdk/4zFb39RCo/kC9z+UWnQM9KggtkOoTLqBduW2GltxtaQsvoEtByLxwgETG+ynsFZOv9t4+zo/nZEaLrvVnbR8pqP3mOrjsGTp+N+GsiSM/b0Uob7L3/24z5DX1/skLjLScTf9yUe34h+JsNJ6Hs+onVGvCZdpsh/4un/axj1Hr2fJzlRdfW1JLG48ASdVpTPvHTCZBZfWtXFqIOX66O9Vy1C7SZ3bfU65lGpXpgLgc9nANhdkKMy+gZXsWDvzxP+mf25M3u46NPMEnOjjsKR5/GZAoxdVsahqI/uxax+n6i03GvIeyeymBZL6rkViFiLSjNk0u+I7nBaaXb30+xyM+s04zen5gS2mqOErI03h4omUUlk+uzuzuixEbziViz80qK5Jq1Bi0XlOiUX9hpSQzGUD21wIDgfpo5pV9TMXIWu8aS3vXP5rAPpaZxaUZCB8zXjLxOhFVaIAoWvm8kvzwmTV2U0XFZGYGUsaIgFQWRLEE3QyErDgtKRn4puFJCL1v2ExEh4JMEwZ7qo0NmtVxfBIN5Ggmlm8vvgu7Hgzoq4uiZUQFSvHiERib4LB7XHDPvX6tjSqmb/zM00A9J6bXlDOnYPnn1GfM9FsKIHZFCMqjROJkS7lAXCVGbjKFwIzES0SFwvRNGISCSpdEfojLkaDNpzW9AqZmU3a3bRYGTihqotW1s6+p7W6Vv1UPSdV/CIhEv6Ycsd09VqoM/eOKwnhdKvnRoeM3YgTjrdItdpmv46rV6p9B30dU7vAYlFJUxN8+rEruLzoADUbJ4iEkEQNXFSN9CgPQGm5sc2aVBtWHZO4KIhIU9qWhLs8Sr+sYDhkx2VPzw9sMcWIxHLbPUxTGPSfVn0S3PWzF+GrWaoshp4TKsNpNKCuGZ908PKDZ/nfU1dRZxsYd87YZjhGEvIlRGL2Y6taVIpob/fUy+i9j6Y+w64Nk0t0q0WVcdPWaJ8SiUTVWqNQIrEwFodaJGYhIs1YzGm6m6oEMcx0BCpx2tNLgjabjF0ZaQoXT6sq+FW5fOoNaGOpblS7jnvPKl++N95xzk8Rb32zcvfctObsuHNKatTrGi0SKUti9uvY4S7ByQjdPZOP9Q2GuPzWlLts5erJiyej3U2h+F4UW3EuRMLQt8gaenaahYi0YE5XJKrVhdUZq8NZlJ5IWAy+6DWFi6dDuSTdjbPXB1q+SQWtzxyMi0S841wUC0cDTfzjJY/zby/cNu4cR7kDE1FGZ44Rz5tM3E0AO11H+a8Xr+E1646Pq+V0+sn2ccet2Dz5ezHa3RQOKkG1Ooy17i1EdXbTxUKE9C0Jd33K3nYWpbd6M5skEandTYuR/h41uaYjEs3XLsfFMAcOqGvRG05tvgtjY9lyMWnjlzAJKsQgfQPG/szDATXRJ0qaz8Y73qjK0jx4bA2P/n+pPUYnnxufFtuwdXwXSDDe3ZT4LFa7sd9ZLpsnzRddBXYWVOA6TUtiSap8gLMkvXPMZu1uWqx4+tQ1Utk4e76lyWrmEucZXjhbwTtXP87u4avGPb+0eeqAQIOjhwu9xvaUyMTdBLDqimr4jrrtrkktkA7uVZbVZ3b+gojFwcobbp50rsUUM3RRlbAkEnsZjEL1xTD0LbKGFolZiEgLljSvl7G+5co0S75YdExi0dLfD2YilFak9zNcUT3MD85ezbPD6nq5tOhl9vo3AbB09dRxjcayIU4NTF6RZ5NMV9+NO2uTtxOZfUMdXv7jD1vYbn+ZD+1+/XSnxmN4xrlpcuVuMi+gPjJ6dpqFCGYsljTdTc2lydtVteldZNrdtHjxDFmoNA2lXR6osiyKHPOT3bS0HwcqG2flVXVTntNQE+B8MP1KpnMh5M9MJKo2pEQi4FXn7v3JKbw4+dQHRqY7DUiIhPGWhPEikdtChfNBWxKzEMHCLN0Ik7hbUubD2PjETGh30+Klf9SG2zYMpLfSr5jQ/KykSPLi1/cgi4pZsnXqEtUNyyXDB8sYujBM2fLSKY+ZL5laEmP3cgR96tx9f1CpvZf9xcytQo3OBoyE4iJhdEyChdNHRs9OsxDBgiVNKU0U7gKoakjPD2zRlsSixeMrotI+cymLsVS6x688D7WVsfadr2DdX0zfw6ChRS1Wzu/rmdsg02Auq+/DPz4AjBGJl+2stLRSsWpmwTSbJVEDp63kZykydv1sETq76eJASqKY0xaJsbgb0mt7qC56LRKLkf5gCZVF6fd6qKwZfyG+asXpWc9p2KCsh/MHB2c5cu4kJlabI/3pxF6jdl4H/ercfV3L2FHfPtMpgPGWRDgUr4llsEjkog1rttAiMQOxSIxYBimwY6lalV5jdIsZ7W5apPhidkps6ddVqqhL7cw+9Z0n+eDjk7N/JtKwrQqA88eNazwUjteTysSSsLuUhRP0x+g50se5yDK2b5r9uzCbjP29hON9p3VMIoWenWYgGlIXfyaWxCqL2vVa3pheb1yzWRKROjS0GInKzBpaVS5NuTDrX70N4ZrdWq3dVIOVEOdbjXOAhwKZr74dLrXPI+CX/OTjxwF41Z/XznQKABZzjIiBodTcWRILRyT07DQDkUAEsGG2pP/PfGpvEYd/sweTdWdax5tNGOpj1RQuEWnGnMGCtaopJQpFVbOX8gC1v6Le3ElHj4ET6xxiEvZSZRUFA5In99lYYz3NJW9eO+t5hlsS8ZJrlqLpOwVmg4XUR0aLxAwokcjMkqjbUkfdlqnTEafCYsHQlZGmcIlKc9oVhgFW7GpI3ham9CeYSusoA6PGTXpzCfYmRSIIPaPFLHcOptUq1OgYXq4sCaspQji6MBaHho5SCHGzEOK4EOKUEOJD0xzzJiHEESHEYSHEj40cT6ZEw/HdlwZeLzpwvXiJYE47vRpU6ujuf/89P/yT+zJ6n0qHj36/Y/YD50hyYs3AkrCWxGMSAUlvwEW1K73+K3ZLjKBML718LoTjYRGjRaLYEsYfNtZayS8OUxgAACAASURBVBaGfRNCCDNwD3Aj0AbsFUI8KKU8MuaYFuBu4Cop5YAQwthdPxmStCSsxpmFFjNaJBYpmZR8SbDz7leSniMzRWVJgMN9s/v758pcVt/CJHDgJxAU9EbKqak4k9Z5JfYIQRxEo2TkqkuXpEgUGzuBF1vDeALpuQzzjZGWxE7glJTyjJQyBNwLvGbCMe8A7pFSDgBIKY1L5p4DiS5gxloSEMOMjC2MBiSa7BEh/QrD86HSFaY/bFw/zoRIZFpe206Q4VETI5RSXZXe91BcohZsvlFjAvHhMJiIJluMGkWxLYIvYpxFlE2MFImlwIUx99vij41lNbBaCPGMEOJ5IcSUOX1CiLuEEPuEEPt6eyc3IjGKxO7LTALXmZJYDSVcW5rFg7IkjH+fyrIY/bFywxYioYQlkalIiDBtHuUGq65L74soccZFwmNMSm8kIrESTis+Mh9K7BF8US0SU33LE69SC9AC7ALeAnxLCDFpg4GU8ptSyh1Syh3V1dVZH+h05MKSSLx2It1Ws3hQloTx71NZqcqJe/uMmVgTGUHWDDOC7KYQhz0qyaO2YfbufADFLvWFGfZZwkKJhMEUO2L4YsbFibKJkSLRBiwfc38Z0DHFMQ9IKcNSyrPAcZRoFATRYDwmkWYzlblgjotEIv6hWTxEMygeOR8qq9XE2nNiYJYj58Zc/fgOU4jWyHLKxSDX/8OGtM4pKYu3bTXIkghHVEMgoykuklokgL1AixCiWQhhA+4AHpxwzP8B1wEIIapQ7qf0Ilg5IOFusliN+5rMZiVA2pJYfGRSPHI+bL5KxSP2PtBpyOvPNSPIblIn3tpwGFd9emVsikuVEHk96WVDZUo4YsIqcmBJFEl8FCMXQCjSsEtUShkB3g38FjgK3C+lPCyE+KQQ4vb4Yb8FPEKII8DjwD9LKT1GjSlTkiKRA3dT4r00iwQpiWZQPHI+bH3LWkoY5enHgoa8fjgsVbDXnJnFPRpTfbu3rUu/yGFxmRIJ34BBnyUCVmG8VV9crFrPhn3GC9J8MXQdI6V8WEq5Wkq5Ukr56fhjH5VSPhi/LaWU75dSrpdSbpJS3mvkeDIlGbg20pKIB8UTri3N4iAWiV9bOYhJWFxFrC06x6kLxrg3wmGBjVDG5/3Nst8CcOXl6S+nSypV7MI3ZMzkqiyJHIhEPPvVKLdZNlkYW/7yRLJ2k5H7JBKBa53dtKiYy27++VDnHKXba0xefjgisJD5xPrhZ1/NmTffzRXvTX/nR3GFEgnvoFEiIbAK412/JU419RrlNssmWiRmIBWTMD4FNpFJpVkcJBYg5hyJRG15kK5AhSGvHYmq/ggZU1dH872fgfL0KiYDlLiVNeQbNkgkoiasphxYEvEsLV+/FokFTbIpepoN3ueC2aoD14uRZHp1jjbb11VF6Ym5iUWzHymNRgXmHGQEARRXqTiGb9iY9wtHBVZTDrKbEiIxmLmbLtdokZiBoFetKBxO45Z7lnhMQgeuFxcpSyI3lUBr6wRRLHhOZT8NNhpTpa9zQUm1EgnviEE7rqOmnLibikvVnOLtNyYAn020SMxAokm7o8S45V4ycK1jEouKXGzUHEt9o9rdu/eBiVuV5k8kKnKytwCguLoEMxEGDWq0F4lm1uNjriQWnomFaCGjRWIGkiLhMq7Yl90x/r00i4NcWxI3v28dDaYLfPE/sj8pRaMCcw5W36AKA1aKAfoHjZm6QlEzVrPxn8VerBaeWiQWOLkQCVe5WlGMegrf7NRkj6QlkaNq0a6GCq5d3sqJgeyXtYnGRM7cTQBu6zCeYWO+uFDUjN1s/MRtL4lbEr7CXxymJRJCiN+NrakkhKgQQvzWuGEVBgGfuvAdpcYV4nJVqot9xFP4ASxN9ki4F80ZbkCbDysbwlyI1BMcye61Fo2JuWU3zZFKu5d+b3q1njIlGLVgt+TAknCq333QX/hu5nQtiSopZdILGC/tXVC9H4wgIRKJLlpG4KxS/iYtEouLlCWRO5FYtc6KxMTZp9uz+rqRaI4tiWI/nkCxIa8dipmx58LddBGKREwIkeydKIRoZHJF14uOYLzBu6GWRI1qbj86WPi+SU32SFoSOYpJACxfr2o4tR/KboZTzt1NzhCekDH9MYIxKzar8Z8lIRKJhWghk25uxYeBp4UQT8TvXwvcZcyQCodAQiTKjavW6KpVK6KRocL3TWqyR65jEgCl1coiHh3I7ka0nLubyqP0R8sMee1QzILdkjuRWAiWRFoiIaV8RAixDbgc1SfifVLKPkNHVgAE4mVVbC7j3E0pkbjoDTPNGFKWRO5yR5zVymodGciu1RqJmTCL3F2/NdUSL05Gu704a7NbaiQYs2GzGv9Z7K54j+9g4f/u0w1cC+BmYJuU8iGgWAiRaavdBUcgCA78CJNxLgFbpRMLYUZGDHsLTQGSi5IvE0m6NrO8WznX7qZlTWoV3v5i9rsdB6UVey7cTfE4Z7Dwq3KkHZP4KnAFqnscwAhwjyEjKiACQYEDY1NThdmEixFGvbmbLDT5J2lJGFhheCLOGmOs1qgUmHOwAS3BstXqc7QdHsr6a4ekFVsOuoomvBPBBZD5nm5M4jIp5TYhxIugspvijYQuagJBgcNk/H/RZfIx4tVbVhYT+bAkiqtLEMQYHcmuSERiJiy5tCQ2qmz8C8fT70ORLkHs2G3GfxZhNmEjuCBEIt2ZKSyEMBPPaBJCVAOFH3GZJ4GQCXsOulSVWrwM+nIYwdTknXxYEiaLiRK8WbdaozFTTi2JpVtV9v2Fs9mNrUipeoHbDGxXPBY7QYILIPM93Sv0v4BfAjVCiE8DTwP/btioCoRgyITDbPx/cXVpN4f7ag1/H03hkBCJXFoSAE4DrNaoFDkNXDuqnFQwQE9vdl835FOiY7fl5rPYRYhgqPDdzOlmN/1ICLEfuAGV3fRaKeVRQ0dWAATCZhwm4y2JrRvD/OKPDQy3DVO6rNTw99Pkn0hYTUS53CcB4DL7GPVnt2BlNGaiyJLbfT5uyyCeoexWRwyNhgALNnuOLAkRIhgqfDfzjCMUQlQm/oAe4CfAj4Hu+GMXNYGwGYfZeJHYfI0ShmOPtBr+XprCIGlJGNirZCqclgAjgexOrhGZW3cTgNs+imc0u6npiXIlduMy3sdhN4UJLACRmO1q2Y+KQwjG77BO3F9h0LgKAn/YgiMHK6SaRpWt4WlfAPlwmqyQtCRyGJMAcFmDjAazm3OiYhK5zfd3F/no9mXX6g551YLQ7siRJWEKE4wscJGQUjYDCCFMwJ1As5Tyk/ESHfU5GF9e6Q042VCdZcfnFFQsUxuC+rsWQBRLkxXyZUm4i30cH8hu/EulwOZYJJwhDg9mtzRHwpKwOXLzP7GbIgTDM7v+Oh59GWKSJTdvzsmYpiLdb+Me1G7rsfsk/tuQERUQXWE3dZXGT9yVjepiH+jV9ZsWC/myJJpr/bQG65BZnNMj0owl1yJRHsETyW5pjmTgOleWhDlMMDKzSCy9aRNLb8mfQED6InGZlPLvgQAkq8Be1PskgsNBBmQFdTXG+1rLG9XF3u8p/C36muwQjaj/da4tiaYmiY8Sek4NZ+01ozIP7qYKySiurDbtCY4qd5OtKDf/E4c5QiCSXnwon50r9T6Jaeg+4gGgbqnxneqtRRZcDDMwWPjpcJrskNhMZ7YZf32NpXmtKlZ5dnf2SlrkQyQamtTUdfrJ7JU9TzQAsufI3VRfMkybP738n5NPZL/tbLrofRLT0HVMtc+obcxNqkOFeZj+4Rw1PDaAH7/rKSwiwq1LXsTXH+CxrxzO95AKmqQlkeN9Ess3Kqu17VD2mkRHpQmLObdrxh2vVnGVPQ90Zu01E4FrW3Fufoerl3o5F6pPa9f16eeyX6cqXdISCSnlj4APAJ8BOlH7JH5q5MDyTW+rF0hlHhlNhdXLgHfh7rq+8+vXEMXCbzq38prVR7nhPRs4+Yfz+R5WwRKJe0lybUlUNqmMoIHu7NWDiEgz5hwn6ay5rQUXw+x9Lnsp6omy3bkKXK9ukcQwc+bFqWtQxcKpQowJV1g+SPvbkFIek1LeI6X878WwkW7Yo/4ppXW5EYlKh5cBv3F9K4zEd/TcuPu/92wF4PmfteVjOAuCULxEtL04tyJR0aQsiYG+7K38o9Kcc3eT2W5hR9lJ9p52Z+01w36l3Lai3PxPVm1Wc8up56fuujDUlioNHQwUfkxi0TEyqFQ8UV7ZaCqKg/QHs1sbP1c89/2TAPzoI+PXDs89HWHk/ADf+/M/EPHp9N6xJPunl+Vo51ackvpSLIQZ6M/epB7FhMWc+6SLS1cP85J3VdZ6doeD6n9ideRGJJZvqwKg/cjUlsTghZRIhPLYnGjhOsENZnhQ/VNK63MzcVc6wwx0OXPyXtnmj78LYSLKbe9fzU+CL7DmuiX841u6ee50DbdtPMuTIzdQUvUMA50B9u+Hb5y8Id9DzjsBf9ySMLB/+lQIs4kK4aF/MHvrw4g0Y86tQQTA5u1WwnttnH36LGtvaZ736yWSCXIlErU7GjAToe301K6/wXZv8nailXI+0JbENCSaAJXU5mbiriiN0h8rz8l7ZZsnjtWxvfQkpRVm7vjcNrbeUscrNvVzwL+aJ0e2AfDQg5J33ncD3zx1A3t/qd1QuWiNOx0VlhEGRrK3PoySe3cTQP1K5a7pOp6dvhIJS8Jiz41ImCvLqBM9tE+ToDXY6U/e1iJRgIyMgpMRTNbcXDAVlRDEgX9gYZXm8Hn87PZuYNf68dkXV9zoRI65vH5w5urk7Z2vX0Y4sLh7egf8YCKKpTj3240qbV4GvNl736g05cWSqF+tNqF2nvHPcmR6hENqIrYW5c7BsrSon7a+qa1J/2jqNxLKY5tTLRLTMDxqptQ0mrP3q3Srf0X/2ex32zKS5390mhB2dt04ftK5/K2rkrf/+08fn3TemScuGD62Qka1xg0Y2hp3OiqKAhwdXpLc9T1fIljyEpOo36iC1p3ns5P5kxQJe+6mxZbaIfb1r6D/gnfSc2MXUvlsTqRFYhpG/GZc5ux3vpqOimq1ehm4kDthygZ//NUoJqJc/efjfcIVKyooxssSUyd//7PrOPKbc/z+S4ewoH7Qx/7YlY/hFgyBoMAh8vPLf8XmAdojdfzswy9m5fWi5CcmUdZUgZ0AXV1ZErtEt0BH7iyJf7y7mEFZzi//7eVJz4WCsTG3tSVRcIz4rbisuXP9VNQpk7O/LXfClA32H7az0Xqc0pbJRePOH/Nz7LwK/K+7uZEb3rsRT7uaGF/evbA+Z7bJp0i872dXIYhxZE92FiT5EglhEtSbe+jszc7+ony4m7b++UaK8HHkwGRrKDwm7TWfzYkMFQkhxM1CiONCiFNCiA/NcNwbhBBSCLHDyPFkwkjQRqktdyJRv0pNph2nFtbkOeS1UFU8tU/YvaYK19Lx5ZxLlzjZXnyUX+2tycXwCpZAyITDlJ+0YFtFCQ3mds5eyM7MHsWcF3cTQJ1jiK6h7AT/8yESZoeVtY5WDrdOzqIcaz1clO6meK2ne4BbgPXAW4QQ66c4zgW8B9ht1FjmwnDYgcueux9x0+V1ALQeXwCd0ccwErLjcmT2Pd2ytYvdoxuzlt++EAmEctP1cDqanX2c7Z1/5l4sKpHkJ3ANUO8apdObnb4SiV3wuRQJgA01fRzpr5v0eDg0xt0UvjgtiZ3AKSnlGSllCLgXeM0Ux/0b8DniFWYLhYGQk4qS3P2IncsrqBJ9nD23sDyAI5EiXI7MKnFWVKoLPjBYUP/ynBII56Z/+nSsqh/l6OhynvivA/z16qeRsblZAtGQCq7mug1rgrqKIF3Biqy8VsKSyGVMAmD9qhAXIksmNR0LxdeLFsIXrbtpKTA2haUt/lgSIcRWYLmU8lcGjmNODERdVLhy29+huaiL1p7c7PDOFiPRYlzFmaWzJur1JwqqLUYCYQsOc/76h1x/g4l+Wcmu917Cd09eTetTc8s2iwbVZ7BY8uNuqq+J4pFuQv75p1SH45ejtTi3NdTWb1PxyKpl491mCdFyMkownL/Fo5HvPJX0Ja+keLe7LwL/NOsLCXGXEGKfEGJfb6/xneLCwRijuCjP8d62lqoBHu3bzkevfiy3bzwPRmIluEoyKxlgi4vEonY3RXLTGnc6bvzblePuH/jN3EpRR4JxS8KcJ0uiXr3v31350rxfKyESlqLcisSGV1Qnb48t5BeK/zycJh+hyMVpSbQBy8fcXwaMvRJdwEbgj0KIVlTnuwenCl5LKb8ppdwhpdxRXV098emsM3heNWRJuEVyxbZNatL4t2euz+n7zpVwMEYQB64Mu0gmLIl8VrbMN4GoBYc1fyJRtaGWcgaS9599fG6uv6S7KU8xiatfqVbf335pO7HI/OobRSJqg6PJkttV+6pbWthlfhKArkOpYn9JS8Lsn7XNqZEY+W3sBVqEEM1CCBtwB/Bg4kkp5ZCUskpK2SSlbAKeB26XUu4zcExpMXAuLhLVOQ5gXZEKwAWHCz+APdKpUiidrszE1B7v/JXNrmILjUDUisOSx13nQtDkSO1VuWfPpXgHMrfsEh3T8hWTWPfXV/D9tzwMwKP/OXmvQSaEw2AlDCLHn8Vs5v/9i9qMmuhjAylLosQSJBS9CN1NUsoI8G7gt8BR4H4p5WEhxCeFELcb9b7ZYKBN7X6sqM1tyYQbP7CVLSUnAOg8kL8mI+ky0qW+J1dZZpdRovPXoo5JRK3Yrflt7thYrnb314tOfJTMqctboi+0zZqnzV5C8PovXssK8zk+89n5Te7hSFwk8kDdqniJkZOpvStKtELYzZFZe2EbiaHyJKV8WEq5Wkq5Ukr56fhjH5VSPjjFsbsKwYoAGGhXexUqluQ2iGy2mvjM3aqyYMfhgVmOzj8jPWp/hKsiM4sr0UMh0S5yMRKI2bDb8isSG5uVyL917X4Afv7tzK+54W51DZSW52+lW1Lr5NL6Nrp8Gfo9JxAOC6wiP9Zt/ToVAO1sTbn9QmGBlXBcJPJXsHth5VvmiO7zytVT1Zj7/g5L1iqX09H9hb+pbqRXXdCuyswCfYmmLovZ3TQaK8ZVlF+R/Jef7+DE/S9x9/9dBsAnH9pG6+7ujF5jpFtdp6Xu/HZVdJeG8YTnt18iEhVYyM81WbuphnIG+NGj1Xz8tS8Ri0rCYbARxmaOEopepJbEQmXPXlUBduWu5bMfnGVWXbecZeYOPvLdFQVfEfbMYbWKbFyT2Y7XxW5JyJhkWLoodeWvHg9AcX0ZLW/cQkVLVfKxk09kluU03KOu0dKq3FezHYu7Ika/rEj2Dp8L4Qh5syTMDiu31e/nmb61fOKBLRz4v7PKkhBh7JYowai2JAqK3aer2Ok8irko9xd+caWD//33DrqiNXz4+udy/v6ZcPRQBDMRVl27JKPz7CXqgl+sIuH1BIhhpqwsvyKRRAh++neqUu+Zg5nVc0q0+XVV57f1rrtKIDElMxPnQjhiyptIALz+ltSi8LEfdhCOCGwiQpE1gj+aPxHWIjEFXV4XTTX5c/dc9887uKN5N1986Tq6Dxm/L2SuHDntoMV0Glt9Zn2GE5ZENjZALUSG21XcqTTDgL+RvO6L12IlxOmTmcVJkr3ga/O7CdRdqxYenjNzL7UfiYJF5C9OdNPHLk/e3vOChVBctIptUXyx3HYwHEvhXKUFRCBmw57HhZEQ8P6PqSDcY984mb+BzEJHv51lxQMZpwzancp/nc/m7vlkuFMFjEsrC6d7sNlmZp39LPfsvZQLLw/OfkKc4QEl9Llq8zsd7nq10j53dO6Lu3DEhNWUP0uiuKGK3r2t3FzxPId7qpQlYQpT7Iji1yJRWASlDbstv66AbW9Zg5MRnvlj4aaJBkMCxxzENCkSeWzunk8SfvyyqvwGeyfyva/5CUkrX/ir9PcbJHvBL8lvf/Zlq1Ur03d9aO7B63DUhFXk17qt2tHElpWjnAg04A1ZsZqiFDti+GL5W7VqkZiCIHbs+Y3DYbaZ2VJ6hhfPZad4mREEoxbs1sx/VLbieEwij31788lQdzzYW52/1eFUbP2rLVxddoi9J8rSPmdkWKpc/opiA0c2OxvftJ7bKp7mvK9qzq1xw9H8WhIJtu+AMDb+2L0WmylCkQP8FBPL05pKi8QEYlFJGBsOe/4nsK1NgxwYWTHvcgNGEYpZsFsyH5vdpRQ4tEhFYrhPbaUtrclvsHcqlpR76QqkLxLDo4JSMZKXNqxjERYzd7wxRhgbJ35/ns/deQAhUjvC0yESE1hM+f+t3frBzTgZYUS6lCUR19+ALz9j0yIxgcQuYHsBLPJWrQIvTvpPF+bGumBsbruGEyJRyJZELCp56getGfWB/p+/2U21pX/WiWm4X61Wy5bk148/FbWVEU6HGzn0hCet44e9ZkpNk/sz54NNr1TdEff8Xwcf/PElABntIleWRP6TKYqbanj98r2AauhUXKIE2Nefn5R4LRITSNRMShShyyfZyNgwEhW7mYdIFHB5qg9dv5tr/6KJD1yTXhpy+yk/d337MvqilXhO9s947FB/YQR7p6KuVonipl3pZawN+6yUWgtj4+em17ewynKWv/72VcnHDj+W/ubAcMyMtQAsCYA7/lz9RvZ712mRKDQCw8oVUAgiUbVUmTOe84WxUptIUNqwz6Fmj6XYhiBW0CLx0B61Kv3i7it5z87n6Dw0/cr62+/ax7KWVApo97GZLb9+j0QQo6whfbdOrihzZ5ZxNRywUprDXvAzIcwmPv/Xx7jckqruc+D5qVvrTkUkZsJiLgyRuOadqSaexU41Tedrc60WiQkkylc7ivIvEu5lauLxtKV/oeeSuWaBCYsZGyGCBdpOIhiEk4HlvH/tw9zufpqv7L2Cz7/t8LTHP/TA+Inllz+LICUc2z+1uPcPCCrEICZr/kotTEdZSWaB2+Ggg1JH4aj9a79xC8+FthM8dpbLHS9x/3PLkGleouFo4VgSzoZKVnOcO8seotilrhPfQH6+Zy0SE0g0wkmUs84n7ia1V6Kvo/BmUykhiAPbHLPAHAQIBPMvxFNxYt8wUSxcuj3KA31X8+rK5/jlgeZpJ5tzg2XsLHqZCpPaX/Cxn6zjo7e/xLodJTz29RNEJ7i5+0csVJrnvjPYSN705atYJ47iIL2FyfAc2tcajhDY1jTzt7e3c9TfzC8/Pb3Aj2U06sCZw772s3F8sI4fdtyQEomh/KTD538mLDASloS9OP9fjXuFckd4evIfTJtIOKhWXHN1yzlNPrz+/H/HU5EI2m7YodJKbnpFgNbIctr2T+3fbg3UsqPZw5lTKRX51K+2AHDDu1az0tmVdGMC9I/aqbRlVv4iV5gcNv7sui4CFBH2zT4pjUSKKC0uMJGIc+c3d9FousD3v5aeu3Y4kv+ii+MoK4PiYopK1X4a31B+BKwwf6V5JCUS+d8N61paioUwHk/hZQElLa45ZoE5TX5G/fn/jqfi8O5RLIRZs6segMtuVUHcp350nrMvjI83DLWPMijLaWqMUdY4db/bc4E6HvzEi8n7/f4iKooK04UIUFauhH+obWTWY4djTkqdheGimYilrITrm8/ydOdKYtHZf0Mj0RJKSwpIJOIUlyVEIj9irEViAsFECmxx/v3FwiRosHRwpr0A8nEnEBqdX4DfaQkwGihMkTh0WNAiTmHbuBqALW9swY2HO790KSu2V4zLNju3W3V3a2yxI0yCJ//9aY787Mik13zzFy7jxV93sK60jb3e9VSWFI5bYyJlbnXtD7XPbO1EQjF8lOS9mu1MXHmlxCPdtD51YcbjYlHJKCW4nIX3WRIVdj19+RmbFokJBEbVSsJRkn+RANjs7uBgd+2kxw/+sX9eZZHnS9KSmKtIWIOMBgurLEWCwx3lbCxvA4sSMVtZEY9+sxUr6jMfeeR88thzB1QcommzKgdxzd1Xs+5P1/PCN/byWveT4153221LODayDIAKV2G6aABK3WpSGuqcnNo6OgqBeJJNon1taVlhxpYAVm1Rcb3W/TPv+/B6AkhMlM6vJYUhrFhro5Qh9h7ITxkILRITSJSvTpSzzjebW/ycCDbiH0xlNnQf7eeS6yp5zYq59fSNRiQ//dihZFxhLsxbJGwhRkKFZyH5vJLT/iVsaBo/QW57x3aOP6Z6LRx+fpg3r36BD1z7PK1H4z01Lq0Zd/zWuy7l6z+r5nbTQ1O+z7ZNhVuTq6w6LhJdk11iLhdc23BWPd8Rb19bXhgLqqlo3FoJQOuRmfdyJAQv01a8ucC0ehWXl7zMM/vz83spvG8kzyS6pRWKSLSstxLDzPndncnHuo+oVdGvL2wm7M9sRTrQ7uPO9S/wpk9u5H1XPk/v8Zk3fk1HMnYzxywwpyPMaKTwROLYU71ITGzcNnnV1nClsgLe9YOruP/kNj7/1OV88hcbAKjZUD3p+Npd63gg+icc/U5qQ967VzwMwOv+daMRw88Kie6IB59PTaydZwN85X1nANjb20z3GS/NV6qYTWNLYVqEAMsurcdElNbTM8caEh32ClLwhOC6K0O87FtF58Hctw7QIjGBRGXSRKXSfLN0tdqV2344Vb557ArPWRwlFpWceLydN6/ax2jv+NXfqd0ejjzWxSdv28NfbnqBymXF3HdyOwD3vHAlV2yaPTg5FfMXiSijkcKrXXTod0qMN+yaPOmb7RY+c/WvsRPAjvK59EUr2WA7gTBP/z2s/asreOl7L3H/Xb/jiy/uoveZE1SurzPmA2SBNbes4DLHAb7961qkVOnOd15xmvd8aUXymC3rU5btplty38ExXaxOO8tNHew9Md6PdPKEag+aINlhL89tWKfjlrerxl6PfPl4zt9bi8QEEiLhcBXGxbJ0g8qYaT+RSuMb7B7TLB07j91zlHf8aT/3n97BXVceYnvFaZpsHXznvQdoudzNhhvq+Niv+jPR5gAAFYRJREFUd/L9Q9uS572r+REATocbiQQzz+hIWlxzDPA7i6KMxuZfOfTk09185JZ9aWWvpMPhfT5sBFl1S8uUz3/oqVfj9QpGBqK8ruKPXGHdxxPPzy52l/zlFt74jRuxlBZTdeXqrIzVMITgdbv6edm3indt34PZFOPx7g3jDukKViZv12xdmusRZsQ7drzII51buPdz5zm6Z4QXH+lm9RrBF/7mMLEYjI5IPv+faip0ufNc/nkaNr9xDZ94xWNc+uqa2Q/ONlLKBfW3fft2aSTfeOuTEqRs29Nu6Puky2jXiAQpP3Pz48nHfvC3T0uQ8puvvC++zkvvby1HJUj58v1HpJRSfvsvn5Ag5ekn2zIe15NffVmClI9+es+cPtdHLntUmojIWGxOp0sppYzFpDQRkSDlN957eO4vFMfrlbLMMiI3W4+kd0I0KmOB4LzftxDZ81DXpOvn1A+eld+95tvyY2t+Ih+85avyS5d8R9775l/ke6izcuGPp5KfoUj45V9v3pO8X2/rk29c/3Ly/os/PJTv4RoGsE/OYc4tDMd7AdHdqSyJqpbC6ONQUuukjCF++UwNzy/fz6f+dzmDHrXyf+1/XU/jfc9y4OkRTpw2s3q5n/986lI+sGsvFoeF/3p0LbevPc4Xj9zM+zc8wn8ceBUXHj9BwyvXAbBis2oUc2Z3LyuuyWw1mAzwz9WScEIMMwFfjKKS2Q3alx7uYOurl/D0j8/zr//k5R/eY+JNd68ghrL43vnl9Vy26yyXvLZ5TuMB+PknDjEU2cgly7qAdbOfYDIh8t14xCB2vLqWS5ynOTC6EoBLzftZ+dYrWPnWK/I8ssxZdm3KTeaXDr5z8NLk/c6Qm58ecXOH81fsbO5l021vzMcQCxotEhNo6zBRI3qwl+fBrJuGP205yHdOXgMj8MANcL1bZdmULS/lVR+/kleNOfafQyGw/QkA7wGgmY+80EppyysQZhMNr0y5OlquUp/x6J4RXpnhmEK++QX4y8uUe+iZey/wzH1tvOPLG/n4n5/m9j9z0tceZNutdTz7iy5+eL+NL3+rmG99uBVYwtV/1gDA43er17nO8SyPB64EYO8vLsxLJLqPq41yX/lZ/Zxf42JBCHhuj5mBI89x/LF2trxtS76HNHeEYP/nH8Miorzi/+1gkPELQAd+Pnd/E8tvuS1PAyxw5mJ+5PNvPu6mp796QN61+jEZHA5Me8ytNXvktqL5uy6yScLlNPFvvsSiMbnU1C7vaHo243N/9oHdEqQ8cG+arpkJdHz3EWkhlPwsCbfRVH/LrJ1yQ9GpSY9/aMtvpJRSfmDdAxKk/JtVj89pLAk+dNWT0kJIxkLheb2OpnB54d9+JX/1hu/KPf/6oPzC6q/J4IuHZdf3f5vvYeUE5uhuyvukn+nfXEXiS294Kjm5WAjJ91769LjnYzEpP/Ua5au8zFl4fsn9PzwiTzx6Vn7vbY9lTSSklPLNS5+UDZbMYxLff9ezEqQ8+cipub1xLCavLn1JgpR2/BKkXEFKCLaYD8hP7fil/OIND00ShxP37pcHvvSYDPUNJV/utrq9cqXl7NzGEucd65+WtaJrXq+h0RQqcxWJReNuetWd1ZT+fJhhWUoEK1/eexWPFLVyWVM3u88v4bhvOaB8lU2lc9s7YCTb7lQ+8pYbm9hzZB89vQLYPu/XvWRdmPvalzLS7cNVm362kT++M724co5prELwP/eV8bF37eZbz27A0X2YSO1S3rbhd3zo30vZ+jfbwbKZyJCXDZ/4DY3b3ESiguBQgJY3XzPp5W66aoRf/XwHZ5/rovmKuaWXeoYtuK3DwOQd7hrNYmXRiMS6166hzx9j3zf28tXPDvH7ro2cCDRw4lgDMp4J/Ibap/jnD5lZdVNhpyjes3tH1l5r3VYH/B6O//48O+5cm/Z5Pq+KKRRVzH2vw9qbm7jvbJO6U78BK3Bf/43jjrGUlXDjF26Z9bU2X10GP4fTz8xDJLwO3PbCrM6q0eSLRSMSAFa7iSvecylXvAcCnQPAEI4qJ7u/tpfmS6uo3nnVjJuiLkbWXlMNn4ejT/ez4870z0uIRLG7aJYjc0PdGlVWvfP03FtpevwlrCrrydaQNJqLgkUlEmNx1KcyHC57z2V5HEl+ab5W7ZY9dzKzqqR+n8REFFtpYeyart9UBUDXhbnXRPKEXex0tmdrSBrNRcHiWjZrJmEvc1AhBujqyexS8PkFRfgLxvJyLimlGC9dXXM7X0roj5bhLi+8fgIaTT4pjF+4Jq/UWT109We2KcznFxSLwmmcI0yCOksfXZ65Gce+wRBBHLgrC6+fgEaTT7RIaKgrGqZrJLM6Sv6giWJTYPYDc0hd0RBdQ3OLkXhOqY107poCrAKq0eQRLRIa6sp8dPnLMjrHFzRTZC6s7mp1Ti+dvsw+RwJPq6qG664rjMKOGk2hoEVCQ11lmM5wFTKWvqvFFzJTbA7OfmAOqS/30xWqnP3AKfCcV1V23UsLIxCv0RQKhoqEEOJmIcRxIcQpIcSHpnj+/UKII0KIg0KIPwghGo0cj2ZqVrdIfJRwbl/6DU38IQvFlgKzJKoiDMgKgoHM4wqeDuU6czeUZHtYGs2CxjCREEKYgXuAW4D1wFuEEOsnHPYisENKuRn4GfA5o8ajmZ5LXqF6Vhz4dVva5/jCVoqshdWnuS6+h677VOaNlHo71GdxN7myOSSNZsFjpCWxEzglpTwjpQwB9wKvGXuAlPJxKWVi99PzwDIDx6OZhk23NyOIceBZ7+wHx/FG7BRZCytdtG6ZCjp3HR/K+NwzZyTFeKnZWDjVfzWaQsBIkVgKXBhzvy3+2HS8HfiNgePRTINzeQWrLK28dCy9ntO+4QjHAo2src98MjaSukYVT+g8lb7YJThxoYhVtvOY7DpwrdGMxUiREFM8NqWzWAjxVmAH8Plpnr9LCLFPCLGvtzf3jcAXA5dUd3KgO72aR8/94BRhbOy6KT1RyRV1LcpV1HUu89Tck/1uWio92R6SRrPgMVIk2oCxHdKXAR0TDxJCvBL4MHC7lHLKdBkp5TellDuklDuqqyc3qNfMn0vWBDgTbmC4e/YNckeeUpPptjesmOXI3FK7UV0bXeczK80RCUvOBJeyelnhbA7UaAoFI0ViL9AihGgWQtiAO4AHxx4ghNgKfAMlELqyWh7Zcrly1bz8UOusx7aejVGEj5otSwweVWZYl9ZQRS9dnZllN7Xu6SGClZa1eiOdRjMRw0RCShkB3g38FjgK3C+lPCyE+KQQ4vb4YZ8HnMBPhRAvCSEenOblNAZzya0qXHTg8dl7aZzrtNNo7UBYCmxSNZtVaY6+zEpznHxGrU9atunMJo1mIoZWgZVSPgw8POGxj465nWlrZY1BLLuygTIGOXwoNuux5/pdNLoKrzETxEuMDJVmdM7JF1TKbMvVutmQRjMRveNaA4Awm2h2dHKue+Ydx1LCaV8dzdWZZxDlgroyH52+zESi7VwUB35qts6UfKfRLE60SGiSNJQOcW6ofMZjuo8PMiArWLdmdosjH9S7w3SF3cgMwhJdfWbqzL2F5z7TaAoALRKaJI01fs4HZ3a5HP2d2pW9fmdhlq+oq4MgDoZ60q8r1TlQRJ1j0MBRaTQLFy0SmiSNy2MMy1I856bv83zkebWBbv0NhZXZlKCuQW2G6zrUl/Y5XV4XdU7d21qjmQotEpokO693AvDH/zk57TFHDktKGaL+0sKsoFLXrPpidB1L3zLoClVQV1FYFW01mkJBi4QmyeV3bcbFMH94cPqg9NELTtaXnCuYtqUTabncDcATv0qvyF/IH6Uv5qauqrDqUGk0hUJh/tI1ecFaWsT6knOc7Ji+S92RwSWsqxvI4agyY/l1q7ip7Hm+8/uGtILX5/aqPRJNK3XQWqOZCi0SmnE0VQxxdrhqyudGevx0x2pY3VxYJcIn8qZbhjkfWcKB33bNeuyZ3aoW2IrNTqOHpdEsSLRIaMbx/7d397FV1Xccx99fKE9FSi3YWqxQilVARUCU8jCZOg3D6ebGHwqZzrC5B5dJtmk0JstMtmxuZj7FmC06nXFzy3zY1GQ6Bz7F+QQCFqEItBWLlcrQIiCI9Ls/zq9Q2nvaXqw997SfV3Jz7/ndc5Pvlx7u957f75zfb3zpPjbvH8tbz3f8gq19sRGACZMG93ZYWbngu9GUYU/e3fX6GLXV0YB1xUzNCSaSiYqEHGZsWXT/w9fO63i1T+2K6C7riumd30uRtJJ5E5mct4Hn/tv1hAJ1mw4whL2UztCNdCKZqEjIYb7x42gF2U8OdOyjr1kTXQE0YXaOT19hxuxxW1m5resrsGobhlCet1XrSIjEUJGQwxTPPZHvTXyWZu84tcXjLxQybfCbFFbmftdMSbGzo6UQb+l89Lp2x0gqRnb/ngqR/kZFQjooO/ZTtreMYu+uQwPUu5r28PLOSVx05ntgmdaTyi1FRcYB8rpcH6NuTwnji3NzHiqRXKAiIR2UjYu6mrauOfQL+/Hf1uAM4LSZnU8AmCuKRkeH9o76nbH7fNCwmw+9kIry3JyHSiQXqEhIB2NPjApB3cpooHpnw04W3TwdgMlzixKLKxtFJdEYw44t8TfVtV6tVTEpt5ZhFcklKhLSQeXsaMxh48roV3j9y4cuh51w9thEYsrWqDHRF/+5l8SPn9S9Ht0UWDEtt6/WEkmSioR0MGbWOPLZzcaaaExiS3U0qd89c+8lb2Ruzv7aXlFZdNd4M4Vs35R5HqfadXsBGD8nNycrFMkFKhLSwYAhg6gcsoU1tQXs39fC5uo9ACy4fX7CkXVf0dhDd1BveC7zndd19cYo+x8F5enoQhNJgoqEZHTxlFqWb5/CCQXbWProPPLYT/EpxUmH1W0lJ4/mGn4DQM2rmQeva7flUzG0MRVXa4kkRUVCMlr6wAwm5b3Flk9KAfjB+H8xYFCKJsHLz+dXzVcxnF2sXpX5Xona5lGML2ru5cBE0kVFQjIaeWIJd167BYAHFz3ObbUXJRxR9gYWDGfmUet4cf3RB9u2r32P+pca2b51H29/UkrFmL0JRiiS+7qe3Eb6rbN/cS4NJz/HmIXpGYtob86pO/nlS6fzUdPHjCgexpRTW2jk0ED1hEp1NYl0RmcSEs+M4xbNwwand16jOefn08JAXv5LLY21Hx9WIAAu/FFFQpGJpIOKhPRpVZdWYLSw7LHdjJkw7LD35pesomRmeTKBiaSEupukTxt50rEsGPE8Nz1z1sG2+xc9ya4VNSz+0/kJRiaSDjqTkD7v1zcf/luofOJQvr9hKQVVkxOKSCQ9dCYhfd4pV85m84DlFI0ZSs0DK6i65sqkQxJJDRUJ6Rcqvn0OAFULZicciUi6qLtJRERiqUiIiEgsFQkREYmlIiEiIrFUJEREJJaKhIiIxFKREBGRWCoSIiISy9wzL8iSq8zsfeDtI/z4aGB7D4aTC5RTOiindOjLOY1z92Oy/XDqisRnYWYr3H1G0nH0JOWUDsopHZRTR+puEhGRWCoSIiISq78ViT8kHcDnQDmlg3JKB+XUTr8akxARkez0tzMJERHJQr8pEmY238w2mNkmM7su6Xi6y8z+aGZNZra2TVuRmT1tZhvD89Gh3czs9pDjG2Y2PbnIMzOz483sGTNbb2ZvmtnVoT3NOQ01s1fNbE3I6cbQPt7MXgk5/c3MBof2IWF7U3i/PMn4O2NmA81slZk9EbZTnZOZ1ZtZtZmtNrMVoS21xx6AmRWa2UNmVhP+X83qyZz6RZEws4HAncCXgcnApWaWlrUr7wPmt2u7Dljm7pXAsrANUX6V4XElcFcvxZiNT4GfuPskoAq4Kvwt0pzTPuAcdz8NmArMN7Mq4CbglpDTB8CSsP8S4AN3PwG4JeyXq64G1rfZ7gs5ne3uU9tcFprmYw/gNuBJd58InEb09+q5nNy9zz+AWcBTbbavB65POq4s4i8H1rbZ3gCUhtelwIbw+vfApZn2y9UH8E/gvL6SE5APvA7MJLqBKS+0HzwGgaeAWeF1XtjPko49Qy5l4QvmHOAJwPpATvXA6HZtqT32gAKgrv2/dU/m1C/OJIDjgHfabDeEtrQqcfdGgPBcHNpTlWfokpgGvELKcwrdMquBJuBpYDPwobt/GnZpG/fBnML7zcCo3o24W24FrgVawvYo0p+TA/82s5Vm1rrYeZqPvQrgfeDe0C14t5kNpwdz6i9FwjK09cXLulKTp5kdBTwMLHX3nZ3tmqEt53Jy9wPuPpXo1/eZwKRMu4XnnM/JzL4CNLn7yrbNGXZNTU7BHHefTtTtcpWZndXJvmnIKQ+YDtzl7tOA3RzqWsok65z6S5FoAI5vs10GvJtQLD1hm5mVAoTnptCeijzNbBBRgfizuz8SmlOdUyt3/xB4lmi8pdDM8sJbbeM+mFN4fySwo3cj7dIc4CIzqwf+StTldCvpzgl3fzc8NwGPEhX0NB97DUCDu78Sth8iKho9llN/KRKvAZXhyozBwCXAYwnH9Fk8BlweXl9O1K/f2n5ZuIKhCmhuPeXMFWZmwD3Aenf/XZu30pzTMWZWGF4PA75ENHj4DLAw7NY+p9ZcFwLLPXQQ5wp3v97dy9y9nOj/y3J3X0yKczKz4WY2ovU1cD6wlhQfe+7+HvCOmZ0Ums4F1tGTOSU98NKLAzwLgLeI+opvSDqeLOJ+EGgE9hP9ClhC1Ne7DNgYnovCvkZ0FddmoBqYkXT8GfKZS3R6+wawOjwWpDynKcCqkNNa4GehvQJ4FdgE/B0YEtqHhu1N4f2KpHPoIr8vAk+kPacQ+5rweLP1eyDNx16IcyqwIhx//wCO7smcdMe1iIjE6i/dTSIicgRUJEREJJaKhIiIxFKREBGRWCoSIiISS0VC5AiZ2c/N7KdJxyHyeVKREBGRWCoSIlkwsxssWpfkP8BJoe07ZvaaRetJPGxm+WY2wszqwhQkmFlBWMtgUKIJiGRJRUKkm8zsdKIpKqYBXwfOCG894u5neLSexHpgibt/RDSH0wVhn0uAh919f+9GLfLZqEiIdN8XgEfdfY9HM9e2zv91ipm9YGbVwGLg5NB+N3BFeH0FcG+vRivSA1QkRLKTaR6b+4AfuvupwI1E8xjh7i8C5WY2Dxjo7mszfFYkp6lIiHTf88DFZjYszCZ6YWgfATSG8YbF7T5zP9EkjTqLkFTSBH8iWTCzG4DLgLeJZuVdR7TQy7WhrRoY4e7fCvsfS7S8ZKlHa02IpIqKhMjnyMwWAl91928mHYvIkcjrehcRORJmdgfRMpkLko5F5EjpTEJERGJp4FpERGKpSIiISCwVCRERiaUiISIisVQkREQkloqEiIjE+j/D+Yv6JU68iAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#예측값 불러오기\n",
    "train_predict = sess.run(Y_pred, feed_dict={X: trainX})\n",
    "test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "\n",
    "\n",
    "plt.plot(testY,'r')\n",
    "plt.plot(test_predict,'b')\n",
    "plt.xlabel(\"day\")\n",
    "plt.ylabel(\"elec\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step2 (전력,온도, optim=Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmlee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\jmlee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\jmlee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\jmlee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\jmlee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\jmlee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#http://mjgim.me/2017/08/02/LSTM.html\n",
    "\n",
    "#라이브러리 불러오기\n",
    "import tensorflow as tf  #딥러닝에 유용\n",
    "from tensorflow.contrib import rnn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#랜덤한 겂을 다른 컴퓨터에도 동일하게 얻을 수 있음.\n",
    "tf.set_random_seed(999)  \n",
    "\n",
    "#데이터 불러오기\n",
    "jan = pd.read_csv(r\"C:/Users/jmlee/Desktop/4학년 2학기/딥러닝 전력 예측/전력 예측/생성data/1월.csv\", header=None)\n",
    "july = pd.read_csv(r\"C:/Users/jmlee/Desktop/4학년 2학기/딥러닝 전력 예측/전력 예측/생성data/7월.csv\", header=None)\n",
    "jan_=jan.loc[:,[1,2,4,6]][1:]\n",
    "# july_=july.loc[:,[1,2,4]][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All=pd.concat([jan_,july_])\n",
    "#str을 float로 바꾸기\n",
    "jan_.columns=['월','일','전력량','온도']\n",
    "jan_[['월','일','전력량','온도']]=jan_[['월','일','전력량','온도']].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#하이퍼 파라미터 설정\n",
    "\n",
    "#한개의 시퀀스길이(시계열데이터 입력갯수,몇개의 rnn을 연결할지 결정,input보다는 같거나 많아야함)\n",
    "timesteps = seq_length = 3  \n",
    "#variable개수\n",
    "data_dim = 4  \n",
    "#각셀의 출력크기(많을수록 특정 훈련데이터에 최적화 가능)\n",
    "hidden_dim = 3  \n",
    "#결과분류 총 수\n",
    "output_dim = 1  \n",
    "#학습률\n",
    "learing_rate = 0.001  \n",
    "#에폭횟수\n",
    "iterations = 50_000  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #데이터 조절\n",
    "# jan_[\"전력량\"] /= 1e5\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    numerator=data-np.min(data,0)\n",
    "    denominator=np.max(data,0)-np.min(data,0)\n",
    "    return numerator/(denominator+1e-7)\n",
    "\n",
    "#Framework 제작\n",
    "jan_['전력량']=MinMaxScaler(jan_['전력량'])\n",
    "x = jan_[['월','일','전력량','온도']].values\n",
    "y = jan_[\"전력량\"].values  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(y) - seq_length):  #seq_length=timesteps\n",
    "    _x = np.copy(x[i:i + seq_length + 1])\n",
    "#     _x[timesteps-2][data_dim-1] = 0\n",
    "#     _x[timesteps-1][data_dim-1] = 0\n",
    "#     _x[timesteps][data_dim-1] = 0\n",
    "    _y = [y[i + seq_length]] #다음 전력(정답)\n",
    "    dataX.append(_x)\n",
    "    dataY.append(_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습데이터와 테스트데이터 분류\n",
    "\n",
    "train_size = int(len(dataY) * 0.8)\n",
    "test_size = len(dataY) - train_size \n",
    "\n",
    "trainX = np.array(dataX[:train_size])  #2298\n",
    "testX = np.array(dataX[train_size : ])\n",
    "\n",
    "trainY = np.array(dataY[:train_size])  #575\n",
    "testY = np.array(dataY[train_size : ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-766411fb0c3b>:7: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-9-766411fb0c3b>:10: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-9-766411fb0c3b>:13: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\jmlee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#LSTM모델 구축\n",
    "#placeholder 초기화 후 텐서에 매핑\n",
    "X = tf.placeholder(tf.float32, [None, seq_length+1, data_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "def lstm_cell(): #cell생성\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(hidden_dim, reuse=tf.AUTO_REUSE)  #출력의 크기를 hidden dim=4\n",
    "    return cell \n",
    "## 5 layers for hidden layer\n",
    "multi_cells = rnn.MultiRNNCell([lstm_cell() for _ in range(5)], state_is_tuple=True)\n",
    "\n",
    "# dynamic rnn 구조(RNN 신경망)연결\n",
    "outputs, _states = tf.nn.dynamic_rnn(multi_cells, X, dtype=tf.float32) #결과값 output차원은 hidden_dim의 크기와 동일\n",
    "\n",
    "Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=None)\n",
    "\n",
    "# sum of the squares= 표준편차(예측값과 실제값의차이) 평균제곱오차\n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSprop 알고리즘->adam\n",
    "optimizer= tf.train.AdamOptimizer(learing_rate)\n",
    "train = optimizer.minimize(loss)\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 50] loss: 409.8702392578125\n",
      "[step: 100] loss: 179.7596435546875\n",
      "[step: 150] loss: 120.50979614257812\n",
      "[step: 200] loss: 119.07489013671875\n",
      "[step: 250] loss: 116.91841125488281\n",
      "[step: 300] loss: 112.92506408691406\n",
      "[step: 350] loss: 108.1546401977539\n",
      "[step: 400] loss: 97.98487091064453\n",
      "[step: 450] loss: 41.4083251953125\n",
      "[step: 500] loss: 10.962697982788086\n",
      "[step: 550] loss: 10.035967826843262\n",
      "[step: 600] loss: 9.580265998840332\n",
      "[step: 650] loss: 9.252286911010742\n",
      "[step: 700] loss: 8.983929634094238\n",
      "[step: 750] loss: 8.721768379211426\n",
      "[step: 800] loss: 8.455065727233887\n",
      "[step: 850] loss: 8.17896842956543\n",
      "[step: 900] loss: 7.8955888748168945\n",
      "[step: 950] loss: 7.6041412353515625\n",
      "[step: 1000] loss: 7.3135199546813965\n",
      "[step: 1050] loss: 7.02163553237915\n",
      "[step: 1100] loss: 6.723100662231445\n",
      "[step: 1150] loss: 6.4062581062316895\n",
      "[step: 1200] loss: 6.055419445037842\n",
      "[step: 1250] loss: 5.654284954071045\n",
      "[step: 1300] loss: 5.201866149902344\n",
      "[step: 1350] loss: 4.680814266204834\n",
      "[step: 1400] loss: 4.084848880767822\n",
      "[step: 1450] loss: 3.5093181133270264\n",
      "[step: 1500] loss: 2.9714720249176025\n",
      "[step: 1550] loss: 2.4920034408569336\n",
      "[step: 1600] loss: 2.059922695159912\n",
      "[step: 1650] loss: 1.6946216821670532\n",
      "[step: 1700] loss: 1.3872511386871338\n",
      "[step: 1750] loss: 1.1466102600097656\n",
      "[step: 1800] loss: 0.9689167737960815\n",
      "[step: 1850] loss: 0.8323579430580139\n",
      "[step: 1900] loss: 0.7239001393318176\n",
      "[step: 1950] loss: 0.669975996017456\n",
      "[step: 2000] loss: 0.5595767498016357\n",
      "[step: 2050] loss: 0.49413055181503296\n",
      "[step: 2100] loss: 0.4358006715774536\n",
      "[step: 2150] loss: 0.38366132974624634\n",
      "[step: 2200] loss: 0.3372751474380493\n",
      "[step: 2250] loss: 0.2966805100440979\n",
      "[step: 2300] loss: 0.26222971081733704\n",
      "[step: 2350] loss: 0.2322765737771988\n",
      "[step: 2400] loss: 0.20645354688167572\n",
      "[step: 2450] loss: 0.18587535619735718\n",
      "[step: 2500] loss: 0.16638045012950897\n",
      "[step: 2550] loss: 0.15106631815433502\n",
      "[step: 2600] loss: 0.1381732076406479\n",
      "[step: 2650] loss: 0.1272784322500229\n",
      "[step: 2700] loss: 0.11871755123138428\n",
      "[step: 2750] loss: 0.11110712587833405\n",
      "[step: 2800] loss: 0.10445652902126312\n",
      "[step: 2850] loss: 0.09906653314828873\n",
      "[step: 2900] loss: 0.09435304999351501\n",
      "[step: 2950] loss: 0.09020361304283142\n",
      "[step: 3000] loss: 0.08750899136066437\n",
      "[step: 3050] loss: 0.08345074951648712\n",
      "[step: 3100] loss: 0.08056677132844925\n",
      "[step: 3150] loss: 0.07790981978178024\n",
      "[step: 3200] loss: 0.07570768147706985\n",
      "[step: 3250] loss: 0.07337484508752823\n",
      "[step: 3300] loss: 0.07131008058786392\n",
      "[step: 3350] loss: 0.06938423961400986\n",
      "[step: 3400] loss: 0.06891509145498276\n",
      "[step: 3450] loss: 0.06603832542896271\n",
      "[step: 3500] loss: 0.06434344500303268\n",
      "[step: 3550] loss: 0.06283508241176605\n",
      "[step: 3600] loss: 0.06138058006763458\n",
      "[step: 3650] loss: 0.06118765100836754\n",
      "[step: 3700] loss: 0.05911695957183838\n",
      "[step: 3750] loss: 0.057493772357702255\n",
      "[step: 3800] loss: 0.05630498751997948\n",
      "[step: 3850] loss: 0.05514989793300629\n",
      "[step: 3900] loss: 0.05402602255344391\n",
      "[step: 3950] loss: 0.06125737726688385\n",
      "[step: 4000] loss: 0.05208391696214676\n",
      "[step: 4050] loss: 0.050977613776922226\n",
      "[step: 4100] loss: 0.05002473667263985\n",
      "[step: 4150] loss: 0.04909520596265793\n",
      "[step: 4200] loss: 0.06617416441440582\n",
      "[step: 4250] loss: 0.047459494322538376\n",
      "[step: 4300] loss: 0.04655396565794945\n",
      "[step: 4350] loss: 0.04575493931770325\n",
      "[step: 4400] loss: 0.04497356340289116\n",
      "[step: 4450] loss: 0.10241822153329849\n",
      "[step: 4500] loss: 0.04358576983213425\n",
      "[step: 4550] loss: 0.04284549877047539\n",
      "[step: 4600] loss: 0.04217398539185524\n",
      "[step: 4650] loss: 0.04151613265275955\n",
      "[step: 4700] loss: 0.040871020406484604\n",
      "[step: 4750] loss: 0.04703919589519501\n",
      "[step: 4800] loss: 0.03971196711063385\n",
      "[step: 4850] loss: 0.03911106288433075\n",
      "[step: 4900] loss: 0.03854723274707794\n",
      "[step: 4950] loss: 0.03799312561750412\n",
      "[step: 5000] loss: 0.05063653364777565\n",
      "[step: 5050] loss: 0.036985792219638824\n",
      "[step: 5100] loss: 0.03647680953145027\n",
      "[step: 5150] loss: 0.03598915785551071\n",
      "[step: 5200] loss: 0.035508621484041214\n",
      "[step: 5250] loss: 0.06825704127550125\n",
      "[step: 5300] loss: 0.03482541814446449\n",
      "[step: 5350] loss: 0.03417828679084778\n",
      "[step: 5400] loss: 0.033748239278793335\n",
      "[step: 5450] loss: 0.0333239883184433\n",
      "[step: 5500] loss: 0.04044683650135994\n",
      "[step: 5550] loss: 0.03256188705563545\n",
      "[step: 5600] loss: 0.03215482831001282\n",
      "[step: 5650] loss: 0.03177327662706375\n",
      "[step: 5700] loss: 0.031395722180604935\n",
      "[step: 5750] loss: 0.08357300609350204\n",
      "[step: 5800] loss: 0.03093642368912697\n",
      "[step: 5850] loss: 0.030348684638738632\n",
      "[step: 5900] loss: 0.030004676431417465\n",
      "[step: 5950] loss: 0.029664119705557823\n",
      "[step: 6000] loss: 0.08681392669677734\n",
      "[step: 6050] loss: 0.029278064146637917\n",
      "[step: 6100] loss: 0.02871834672987461\n",
      "[step: 6150] loss: 0.028404884040355682\n",
      "[step: 6200] loss: 0.028094448149204254\n",
      "[step: 6250] loss: 0.030827173963189125\n",
      "[step: 6300] loss: 0.027537893503904343\n",
      "[step: 6350] loss: 0.02723192609846592\n",
      "[step: 6400] loss: 0.02694525010883808\n",
      "[step: 6450] loss: 0.026659874245524406\n",
      "[step: 6500] loss: 0.05017005279660225\n",
      "[step: 6550] loss: 0.02627060003578663\n",
      "[step: 6600] loss: 0.02586781233549118\n",
      "[step: 6650] loss: 0.02560298517346382\n",
      "[step: 6700] loss: 0.025338856503367424\n",
      "[step: 6750] loss: 0.02510235831141472\n",
      "[step: 6800] loss: 0.024907320737838745\n",
      "[step: 6850] loss: 0.024605698883533478\n",
      "[step: 6900] loss: 0.024358555674552917\n",
      "[step: 6950] loss: 0.024194685742259026\n",
      "[step: 7000] loss: 0.025257743895053864\n",
      "[step: 7050] loss: 0.023672306910157204\n",
      "[step: 7100] loss: 0.023435581475496292\n",
      "[step: 7150] loss: 0.02320471592247486\n",
      "[step: 7200] loss: 0.026002030819654465\n",
      "[step: 7250] loss: 0.02304774709045887\n",
      "[step: 7300] loss: 0.022566359490156174\n",
      "[step: 7350] loss: 0.022351114079356194\n",
      "[step: 7400] loss: 0.022136159241199493\n",
      "[step: 7450] loss: 0.021969154477119446\n",
      "[step: 7500] loss: 0.022441431879997253\n",
      "[step: 7550] loss: 0.021544992923736572\n",
      "[step: 7600] loss: 0.0213336031883955\n",
      "[step: 7650] loss: 0.021131539717316628\n",
      "[step: 7700] loss: 0.020941998809576035\n",
      "[step: 7750] loss: 0.02100982703268528\n",
      "[step: 7800] loss: 0.02058275230228901\n",
      "[step: 7850] loss: 0.02039427123963833\n",
      "[step: 7900] loss: 0.020208248868584633\n",
      "[step: 7950] loss: 0.020021244883537292\n",
      "[step: 8000] loss: 0.019833244383335114\n",
      "[step: 8050] loss: 0.0269647054374218\n",
      "[step: 8100] loss: 0.019533207640051842\n",
      "[step: 8150] loss: 0.01935180090367794\n",
      "[step: 8200] loss: 0.019184736534953117\n",
      "[step: 8250] loss: 0.01901666261255741\n",
      "[step: 8300] loss: 0.01884736306965351\n",
      "[step: 8350] loss: 0.018676916137337685\n",
      "[step: 8400] loss: 0.018795333802700043\n",
      "[step: 8450] loss: 0.019193578511476517\n",
      "[step: 8500] loss: 0.018212785944342613\n",
      "[step: 8550] loss: 0.018055330961942673\n",
      "[step: 8600] loss: 0.017896946519613266\n",
      "[step: 8650] loss: 0.017737368121743202\n",
      "[step: 8700] loss: 0.08933331072330475\n",
      "[step: 8750] loss: 0.01773405820131302\n",
      "[step: 8800] loss: 0.01731070876121521\n",
      "[step: 8850] loss: 0.017161885276436806\n",
      "[step: 8900] loss: 0.01701369695365429\n",
      "[step: 8950] loss: 0.016864275559782982\n",
      "[step: 9000] loss: 0.022545253857970238\n",
      "[step: 9050] loss: 0.016751669347286224\n",
      "[step: 9100] loss: 0.016472378745675087\n",
      "[step: 9150] loss: 0.016335826367139816\n",
      "[step: 9200] loss: 0.016198528930544853\n",
      "[step: 9250] loss: 0.016059866175055504\n",
      "[step: 9300] loss: 0.015920616686344147\n",
      "[step: 9350] loss: 0.015842994675040245\n",
      "[step: 9400] loss: 0.015704285353422165\n",
      "[step: 9450] loss: 0.015579698607325554\n",
      "[step: 9500] loss: 0.01545677985996008\n",
      "[step: 9550] loss: 0.015332828275859356\n",
      "[step: 9600] loss: 0.015207603573799133\n",
      "[step: 9650] loss: 0.015081185847520828\n",
      "[step: 9700] loss: 0.02280046045780182\n",
      "[step: 9750] loss: 0.014988873153924942\n",
      "[step: 9800] loss: 0.014733541756868362\n",
      "[step: 9850] loss: 0.01461341604590416\n",
      "[step: 9900] loss: 0.01449745986610651\n",
      "[step: 9950] loss: 0.014819520525634289\n",
      "[step: 10000] loss: 0.014294221065938473\n",
      "[step: 10050] loss: 0.014176987111568451\n",
      "[step: 10100] loss: 0.014065002091228962\n",
      "[step: 10150] loss: 0.013951891101896763\n",
      "[step: 10200] loss: 0.051439352333545685\n",
      "[step: 10250] loss: 0.0141185587272048\n",
      "[step: 10300] loss: 0.013654511421918869\n",
      "[step: 10350] loss: 0.013550393283367157\n",
      "[step: 10400] loss: 0.013445965945720673\n",
      "[step: 10450] loss: 0.013340317644178867\n",
      "[step: 10500] loss: 0.01522086001932621\n",
      "[step: 10550] loss: 0.013384072110056877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 10600] loss: 0.013053951784968376\n",
      "[step: 10650] loss: 0.012954538688063622\n",
      "[step: 10700] loss: 0.01285396609455347\n",
      "[step: 10750] loss: 0.02465721406042576\n",
      "[step: 10800] loss: 0.01272040419280529\n",
      "[step: 10850] loss: 0.012589717283844948\n",
      "[step: 10900] loss: 0.012496408075094223\n",
      "[step: 10950] loss: 0.012402295134961605\n",
      "[step: 11000] loss: 0.012307196855545044\n",
      "[step: 11050] loss: 0.015819067135453224\n",
      "[step: 11100] loss: 0.012154725380241871\n",
      "[step: 11150] loss: 0.012070377357304096\n",
      "[step: 11200] loss: 0.011984509415924549\n",
      "[step: 11250] loss: 0.011897833086550236\n",
      "[step: 11300] loss: 0.011810035444796085\n",
      "[step: 11350] loss: 0.047051962465047836\n",
      "[step: 11400] loss: 0.012129315175116062\n",
      "[step: 11450] loss: 0.01157369278371334\n",
      "[step: 11500] loss: 0.011489898897707462\n",
      "[step: 11550] loss: 0.011406252160668373\n",
      "[step: 11600] loss: 0.0387241393327713\n",
      "[step: 11650] loss: 0.011415192857384682\n",
      "[step: 11700] loss: 0.011183949187397957\n",
      "[step: 11750] loss: 0.011105206795036793\n",
      "[step: 11800] loss: 0.011026023887097836\n",
      "[step: 11850] loss: 0.011043628677725792\n",
      "[step: 11900] loss: 0.010953597724437714\n",
      "[step: 11950] loss: 0.010815593414008617\n",
      "[step: 12000] loss: 0.01073952205479145\n",
      "[step: 12050] loss: 0.010664188303053379\n",
      "[step: 12100] loss: 0.044239673763513565\n",
      "[step: 12150] loss: 0.010719711892306805\n",
      "[step: 12200] loss: 0.010465153492987156\n",
      "[step: 12250] loss: 0.010393798351287842\n",
      "[step: 12300] loss: 0.010322289541363716\n",
      "[step: 12350] loss: 0.03927449509501457\n",
      "[step: 12400] loss: 0.010205451399087906\n",
      "[step: 12450] loss: 0.010130906477570534\n",
      "[step: 12500] loss: 0.01006322167813778\n",
      "[step: 12550] loss: 0.009994901716709137\n",
      "[step: 12600] loss: 0.02629740722477436\n",
      "[step: 12650] loss: 0.009885969571769238\n",
      "[step: 12700] loss: 0.009823060594499111\n",
      "[step: 12750] loss: 0.009760274551808834\n",
      "[step: 12800] loss: 0.009697220288217068\n",
      "[step: 12850] loss: 0.009633471257984638\n",
      "[step: 12900] loss: 0.03227803483605385\n",
      "[step: 12950] loss: 0.009629791602492332\n",
      "[step: 13000] loss: 0.009465192444622517\n",
      "[step: 13050] loss: 0.009404207579791546\n",
      "[step: 13100] loss: 0.009343461133539677\n",
      "[step: 13150] loss: 0.009798859246075153\n",
      "[step: 13200] loss: 0.00957344938069582\n",
      "[step: 13250] loss: 0.009194298647344112\n",
      "[step: 13300] loss: 0.00913726445287466\n",
      "[step: 13350] loss: 0.009081737138330936\n",
      "[step: 13400] loss: 0.009025639854371548\n",
      "[step: 13450] loss: 0.008981051854789257\n",
      "[step: 13500] loss: 0.011888348497450352\n",
      "[step: 13550] loss: 0.008875462226569653\n",
      "[step: 13600] loss: 0.008822333067655563\n",
      "[step: 13650] loss: 0.008768941275775433\n",
      "[step: 13700] loss: 0.008714959025382996\n",
      "[step: 13750] loss: 0.014097635634243488\n",
      "[step: 13800] loss: 0.00864979438483715\n",
      "[step: 13850] loss: 0.00859023630619049\n",
      "[step: 13900] loss: 0.008542115800082684\n",
      "[step: 13950] loss: 0.008494297973811626\n",
      "[step: 14000] loss: 0.008445977233350277\n",
      "[step: 14050] loss: 0.008397046476602554\n",
      "[step: 14100] loss: 0.00834843423217535\n",
      "[step: 14150] loss: 0.010422620922327042\n",
      "[step: 14200] loss: 0.008288637734949589\n",
      "[step: 14250] loss: 0.008219930343329906\n",
      "[step: 14300] loss: 0.008173132315278053\n",
      "[step: 14350] loss: 0.00812604557722807\n",
      "[step: 14400] loss: 0.008585798554122448\n",
      "[step: 14450] loss: 0.008949607610702515\n",
      "[step: 14500] loss: 0.008025973103940487\n",
      "[step: 14550] loss: 0.00798184797167778\n",
      "[step: 14600] loss: 0.007941466756165028\n",
      "[step: 14650] loss: 0.007900678552687168\n",
      "[step: 14700] loss: 0.00785934366285801\n",
      "[step: 14750] loss: 0.007817486301064491\n",
      "[step: 14800] loss: 0.007775189820677042\n",
      "[step: 14850] loss: 0.007732992991805077\n",
      "[step: 14900] loss: 0.013112270273268223\n",
      "[step: 14950] loss: 0.0076842233538627625\n",
      "[step: 15000] loss: 0.007629769388586283\n",
      "[step: 15050] loss: 0.00759050901979208\n",
      "[step: 15100] loss: 0.00755098694935441\n",
      "[step: 15150] loss: 0.007512362208217382\n",
      "[step: 15200] loss: 0.008012000471353531\n",
      "[step: 15250] loss: 0.007450654171407223\n",
      "[step: 15300] loss: 0.007407019846141338\n",
      "[step: 15350] loss: 0.007368974853307009\n",
      "[step: 15400] loss: 0.007705847732722759\n",
      "[step: 15450] loss: 0.007898695766925812\n",
      "[step: 15500] loss: 0.007276421412825584\n",
      "[step: 15550] loss: 0.0072381217032670975\n",
      "[step: 15600] loss: 0.007202549837529659\n",
      "[step: 15650] loss: 0.007166517432779074\n",
      "[step: 15700] loss: 0.007176889106631279\n",
      "[step: 15750] loss: 0.00942677166312933\n",
      "[step: 15800] loss: 0.007080685347318649\n",
      "[step: 15850] loss: 0.007046063430607319\n",
      "[step: 15900] loss: 0.007012884132564068\n",
      "[step: 15950] loss: 0.006979324389249086\n",
      "[step: 16000] loss: 0.006945349741727114\n",
      "[step: 16050] loss: 0.007973955944180489\n",
      "[step: 16100] loss: 0.00695535633713007\n",
      "[step: 16150] loss: 0.006859098095446825\n",
      "[step: 16200] loss: 0.006826403550803661\n",
      "[step: 16250] loss: 0.00680008577182889\n",
      "[step: 16300] loss: 0.007240853272378445\n",
      "[step: 16350] loss: 0.006764755584299564\n",
      "[step: 16400] loss: 0.006712143309414387\n",
      "[step: 16450] loss: 0.006681119557470083\n",
      "[step: 16500] loss: 0.006649899762123823\n",
      "[step: 16550] loss: 0.017298895865678787\n",
      "[step: 16600] loss: 0.006607212126255035\n",
      "[step: 16650] loss: 0.006578827276825905\n",
      "[step: 16700] loss: 0.006549847312271595\n",
      "[step: 16750] loss: 0.006520997732877731\n",
      "[step: 16800] loss: 0.0064917998388409615\n",
      "[step: 16850] loss: 0.006462154444307089\n",
      "[step: 16900] loss: 0.18605966866016388\n",
      "[step: 16950] loss: 0.006504443474113941\n",
      "[step: 17000] loss: 0.0064114900305867195\n",
      "[step: 17050] loss: 0.006381678394973278\n",
      "[step: 17100] loss: 0.006356475874781609\n",
      "[step: 17150] loss: 0.006331027485430241\n",
      "[step: 17200] loss: 0.006305219605565071\n",
      "[step: 17250] loss: 0.00627911975607276\n",
      "[step: 17300] loss: 0.006252647843211889\n",
      "[step: 17350] loss: 0.006225833669304848\n",
      "[step: 17400] loss: 0.006198636256158352\n",
      "[step: 17450] loss: 0.04266931861639023\n",
      "[step: 17500] loss: 0.0062441215850412846\n",
      "[step: 17550] loss: 0.006154696457087994\n",
      "[step: 17600] loss: 0.006131060887128115\n",
      "[step: 17650] loss: 0.006108410656452179\n",
      "[step: 17700] loss: 0.006085453554987907\n",
      "[step: 17750] loss: 0.006062275264412165\n",
      "[step: 17800] loss: 0.006038745865225792\n",
      "[step: 17850] loss: 0.006014893762767315\n",
      "[step: 17900] loss: 0.005990686360746622\n",
      "[step: 17950] loss: 0.006434629205614328\n",
      "[step: 18000] loss: 0.0061917733401060104\n",
      "[step: 18050] loss: 0.005931869149208069\n",
      "[step: 18100] loss: 0.0059081497602164745\n",
      "[step: 18150] loss: 0.005884889513254166\n",
      "[step: 18200] loss: 0.02699902281165123\n",
      "[step: 18250] loss: 0.005854457151144743\n",
      "[step: 18300] loss: 0.0058321706019341946\n",
      "[step: 18350] loss: 0.00580986961722374\n",
      "[step: 18400] loss: 0.005788045935332775\n",
      "[step: 18450] loss: 0.005765947513282299\n",
      "[step: 18500] loss: 0.005743515677750111\n",
      "[step: 18550] loss: 0.03232346847653389\n",
      "[step: 18600] loss: 0.006834980566054583\n",
      "[step: 18650] loss: 0.0057219769805669785\n",
      "[step: 18700] loss: 0.005703364033252001\n",
      "[step: 18750] loss: 0.005686111748218536\n",
      "[step: 18800] loss: 0.0056687467731535435\n",
      "[step: 18850] loss: 0.005651264451444149\n",
      "[step: 18900] loss: 0.005633512511849403\n",
      "[step: 18950] loss: 0.00561552494764328\n",
      "[step: 19000] loss: 0.005597279407083988\n",
      "[step: 19050] loss: 0.005578721407800913\n",
      "[step: 19100] loss: 0.005559926852583885\n",
      "[step: 19150] loss: 0.005540792364627123\n",
      "[step: 19200] loss: 0.005566622596234083\n",
      "[step: 19250] loss: 0.005597057286649942\n",
      "[step: 19300] loss: 0.005508366972208023\n",
      "[step: 19350] loss: 0.005476923193782568\n",
      "[step: 19400] loss: 0.005458570085465908\n",
      "[step: 19450] loss: 0.005456183571368456\n",
      "[step: 19500] loss: 0.005482053384184837\n",
      "[step: 19550] loss: 0.005422718822956085\n",
      "[step: 19600] loss: 0.005398528650403023\n",
      "[step: 19650] loss: 0.005381016060709953\n",
      "[step: 19700] loss: 0.0053632925264537334\n",
      "[step: 19750] loss: 0.00560410600155592\n",
      "[step: 19800] loss: 0.005345738027244806\n",
      "[step: 19850] loss: 0.005323560908436775\n",
      "[step: 19900] loss: 0.005306525155901909\n",
      "[step: 19950] loss: 0.005289589520543814\n",
      "[step: 20000] loss: 0.10090606659650803\n",
      "[step: 20050] loss: 0.005268469452857971\n",
      "[step: 20100] loss: 0.005253177136182785\n",
      "[step: 20150] loss: 0.005236693192273378\n",
      "[step: 20200] loss: 0.0052206190302968025\n",
      "[step: 20250] loss: 0.005204355344176292\n",
      "[step: 20300] loss: 0.00540562579408288\n",
      "[step: 20350] loss: 0.0068262978456914425\n",
      "[step: 20400] loss: 0.00517977075651288\n",
      "[step: 20450] loss: 0.005162260960787535\n",
      "[step: 20500] loss: 0.005147809628397226\n",
      "[step: 20550] loss: 0.005133149679750204\n",
      "[step: 20600] loss: 0.005118316039443016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 20650] loss: 0.005103214643895626\n",
      "[step: 20700] loss: 0.005087967496365309\n",
      "[step: 20750] loss: 0.18006058037281036\n",
      "[step: 20800] loss: 0.005865239072591066\n",
      "[step: 20850] loss: 0.005067605525255203\n",
      "[step: 20900] loss: 0.005049730651080608\n",
      "[step: 20950] loss: 0.005036060698330402\n",
      "[step: 21000] loss: 0.00502222403883934\n",
      "[step: 21050] loss: 0.005008177366107702\n",
      "[step: 21100] loss: 0.004993990063667297\n",
      "[step: 21150] loss: 0.004979545716196299\n",
      "[step: 21200] loss: 0.05989672988653183\n",
      "[step: 21250] loss: 0.004971024580299854\n",
      "[step: 21300] loss: 0.004958270583301783\n",
      "[step: 21350] loss: 0.004944956861436367\n",
      "[step: 21400] loss: 0.004932008218020201\n",
      "[step: 21450] loss: 0.004918964579701424\n",
      "[step: 21500] loss: 0.00490574212744832\n",
      "[step: 21550] loss: 0.004892336204648018\n",
      "[step: 21600] loss: 0.004878637380897999\n",
      "[step: 21650] loss: 0.028720242902636528\n",
      "[step: 21700] loss: 0.005228789523243904\n",
      "[step: 21750] loss: 0.004861012566834688\n",
      "[step: 21800] loss: 0.004849043674767017\n",
      "[step: 21850] loss: 0.004837017506361008\n",
      "[step: 21900] loss: 0.00482493219897151\n",
      "[step: 21950] loss: 0.004812642466276884\n",
      "[step: 22000] loss: 0.0048001655377447605\n",
      "[step: 22050] loss: 0.0047874487936496735\n",
      "[step: 22100] loss: 0.011238303035497665\n",
      "[step: 22150] loss: 0.004772156942635775\n",
      "[step: 22200] loss: 0.004760999698191881\n",
      "[step: 22250] loss: 0.004748030100017786\n",
      "[step: 22300] loss: 0.004735758062452078\n",
      "[step: 22350] loss: 0.05584395304322243\n",
      "[step: 22400] loss: 0.00517851859331131\n",
      "[step: 22450] loss: 0.004711492918431759\n",
      "[step: 22500] loss: 0.004699188284575939\n",
      "[step: 22550] loss: 0.00468731252476573\n",
      "[step: 22600] loss: 0.004675349686294794\n",
      "[step: 22650] loss: 0.0046631465665996075\n",
      "[step: 22700] loss: 0.030368439853191376\n",
      "[step: 22750] loss: 0.004715485032647848\n",
      "[step: 22800] loss: 0.004653051495552063\n",
      "[step: 22850] loss: 0.004642206709831953\n",
      "[step: 22900] loss: 0.0046318466775119305\n",
      "[step: 22950] loss: 0.004621339496225119\n",
      "[step: 23000] loss: 0.0046106018126010895\n",
      "[step: 23050] loss: 0.00459976214915514\n",
      "[step: 23100] loss: 0.004588773474097252\n",
      "[step: 23150] loss: 0.004577562212944031\n",
      "[step: 23200] loss: 0.019031165167689323\n",
      "[step: 23250] loss: 0.004723366815596819\n",
      "[step: 23300] loss: 0.004554062150418758\n",
      "[step: 23350] loss: 0.004543223883956671\n",
      "[step: 23400] loss: 0.0045322515070438385\n",
      "[step: 23450] loss: 0.010810963809490204\n",
      "[step: 23500] loss: 0.005324658006429672\n",
      "[step: 23550] loss: 0.0045154644176363945\n",
      "[step: 23600] loss: 0.004503707401454449\n",
      "[step: 23650] loss: 0.0044935536570847034\n",
      "[step: 23700] loss: 0.004483111668378115\n",
      "[step: 23750] loss: 0.004472639411687851\n",
      "[step: 23800] loss: 0.027237387374043465\n",
      "[step: 23850] loss: 0.004469827748835087\n",
      "[step: 23900] loss: 0.004450305365025997\n",
      "[step: 23950] loss: 0.004439659416675568\n",
      "[step: 24000] loss: 0.00443681376054883\n",
      "[step: 24050] loss: 0.005550720728933811\n",
      "[step: 24100] loss: 0.004439580254256725\n",
      "[step: 24150] loss: 0.004408267326653004\n",
      "[step: 24200] loss: 0.004398211371153593\n",
      "[step: 24250] loss: 0.004388315603137016\n",
      "[step: 24300] loss: 0.006490414030849934\n",
      "[step: 24350] loss: 0.004412014968693256\n",
      "[step: 24400] loss: 0.004370227921754122\n",
      "[step: 24450] loss: 0.004360591527074575\n",
      "[step: 24500] loss: 0.004350887145847082\n",
      "[step: 24550] loss: 0.004346206318587065\n",
      "[step: 24600] loss: 0.004846826195716858\n",
      "[step: 24650] loss: 0.004351054318249226\n",
      "[step: 24700] loss: 0.004321785178035498\n",
      "[step: 24750] loss: 0.004312118049710989\n",
      "[step: 24800] loss: 0.004302528686821461\n",
      "[step: 24850] loss: 0.02051103487610817\n",
      "[step: 24900] loss: 0.004376860335469246\n",
      "[step: 24950] loss: 0.0042866370640695095\n",
      "[step: 25000] loss: 0.004277313593775034\n",
      "[step: 25050] loss: 0.004268242511898279\n",
      "[step: 25100] loss: 0.004258980043232441\n",
      "[step: 25150] loss: 0.01974990777671337\n",
      "[step: 25200] loss: 0.005036946851760149\n",
      "[step: 25250] loss: 0.004248780198395252\n",
      "[step: 25300] loss: 0.004235083237290382\n",
      "[step: 25350] loss: 0.0042263176292181015\n",
      "[step: 25400] loss: 0.004217416979372501\n",
      "[step: 25450] loss: 0.004208414815366268\n",
      "[step: 25500] loss: 0.009645625948905945\n",
      "[step: 25550] loss: 0.00425894558429718\n",
      "[step: 25600] loss: 0.004193175118416548\n",
      "[step: 25650] loss: 0.004184442106634378\n",
      "[step: 25700] loss: 0.004175771959125996\n",
      "[step: 25750] loss: 0.0041683390736579895\n",
      "[step: 25800] loss: 0.010030880570411682\n",
      "[step: 25850] loss: 0.004167815204709768\n",
      "[step: 25900] loss: 0.004150331486016512\n",
      "[step: 25950] loss: 0.004141801968216896\n",
      "[step: 26000] loss: 0.004147810861468315\n",
      "[step: 26050] loss: 0.006161361467093229\n",
      "[step: 26100] loss: 0.004125114995986223\n",
      "[step: 26150] loss: 0.004115284886211157\n",
      "[step: 26200] loss: 0.0041073523461818695\n",
      "[step: 26250] loss: 0.004360864870250225\n",
      "[step: 26300] loss: 0.004098114091902971\n",
      "[step: 26350] loss: 0.00408961670473218\n",
      "[step: 26400] loss: 0.004081282764673233\n",
      "[step: 26450] loss: 0.0050648534670472145\n",
      "[step: 26500] loss: 0.0046269698068499565\n",
      "[step: 26550] loss: 0.004071746487170458\n",
      "[step: 26600] loss: 0.0040586404502391815\n",
      "[step: 26650] loss: 0.004050747491419315\n",
      "[step: 26700] loss: 0.004042728804051876\n",
      "[step: 26750] loss: 0.048024024814367294\n",
      "[step: 26800] loss: 0.004212797619402409\n",
      "[step: 26850] loss: 0.004029192961752415\n",
      "[step: 26900] loss: 0.004021419212222099\n",
      "[step: 26950] loss: 0.004013635218143463\n",
      "[step: 27000] loss: 0.004005793482065201\n",
      "[step: 27050] loss: 0.0040700738318264484\n",
      "[step: 27100] loss: 0.004102864768356085\n",
      "[step: 27150] loss: 0.003994629252701998\n",
      "[step: 27200] loss: 0.003986041527241468\n",
      "[step: 27250] loss: 0.003978495951741934\n",
      "[step: 27300] loss: 0.003970903344452381\n",
      "[step: 27350] loss: 0.009266180917620659\n",
      "[step: 27400] loss: 0.004152730572968721\n",
      "[step: 27450] loss: 0.003963298164308071\n",
      "[step: 27500] loss: 0.003951249644160271\n",
      "[step: 27550] loss: 0.0039439015090465546\n",
      "[step: 27600] loss: 0.003936531022191048\n",
      "[step: 27650] loss: 0.012504157610237598\n",
      "[step: 27700] loss: 0.003939768765121698\n",
      "[step: 27750] loss: 0.003922383766621351\n",
      "[step: 27800] loss: 0.003915110137313604\n",
      "[step: 27850] loss: 0.003907789010554552\n",
      "[step: 27900] loss: 0.04091737046837807\n",
      "[step: 27950] loss: 0.004235539585351944\n",
      "[step: 28000] loss: 0.00389404920861125\n",
      "[step: 28050] loss: 0.0038864626549184322\n",
      "[step: 28100] loss: 0.0038792488630861044\n",
      "[step: 28150] loss: 0.008757311850786209\n",
      "[step: 28200] loss: 0.0040105190128088\n",
      "[step: 28250] loss: 0.0038662103470414877\n",
      "[step: 28300] loss: 0.003858444979414344\n",
      "[step: 28350] loss: 0.003851381130516529\n",
      "[step: 28400] loss: 0.007888566702604294\n",
      "[step: 28450] loss: 0.003872260684147477\n",
      "[step: 28500] loss: 0.003838774049654603\n",
      "[step: 28550] loss: 0.0038317986764013767\n",
      "[step: 28600] loss: 0.0038248023483902216\n",
      "[step: 28650] loss: 0.005670065991580486\n",
      "[step: 28700] loss: 0.003934280946850777\n",
      "[step: 28750] loss: 0.003823264501988888\n",
      "[step: 28800] loss: 0.003808701876550913\n",
      "[step: 28850] loss: 0.003802047111093998\n",
      "[step: 28900] loss: 0.003795343218371272\n",
      "[step: 28950] loss: 0.0037885543424636126\n",
      "[step: 29000] loss: 0.0037906260695308447\n",
      "[step: 29050] loss: 0.004318627528846264\n",
      "[step: 29100] loss: 0.003782214829698205\n",
      "[step: 29150] loss: 0.0037754082586616278\n",
      "[step: 29200] loss: 0.0037690058816224337\n",
      "[step: 29250] loss: 0.003762683132663369\n",
      "[step: 29300] loss: 0.003756231162697077\n",
      "[step: 29350] loss: 0.0037496346049010754\n",
      "[step: 29400] loss: 0.009215496480464935\n",
      "[step: 29450] loss: 0.004258531145751476\n",
      "[step: 29500] loss: 0.003742300672456622\n",
      "[step: 29550] loss: 0.0037309625186026096\n",
      "[step: 29600] loss: 0.0037244604900479317\n",
      "[step: 29650] loss: 0.057908669114112854\n",
      "[step: 29700] loss: 0.004037951584905386\n",
      "[step: 29750] loss: 0.003715721657499671\n",
      "[step: 29800] loss: 0.0037063632626086473\n",
      "[step: 29850] loss: 0.0036999366711825132\n",
      "[step: 29900] loss: 0.03650781139731407\n",
      "[step: 29950] loss: 0.00412490451708436\n",
      "[step: 30000] loss: 0.0036890790797770023\n",
      "[step: 30050] loss: 0.0036825293209403753\n",
      "[step: 30100] loss: 0.0036762685049325228\n",
      "[step: 30150] loss: 0.003938428126275539\n",
      "[step: 30200] loss: 0.004053822252899408\n",
      "[step: 30250] loss: 0.0036696449387818575\n",
      "[step: 30300] loss: 0.00365896406583488\n",
      "[step: 30350] loss: 0.00365285761654377\n",
      "[step: 30400] loss: 0.003660207847133279\n",
      "[step: 30450] loss: 0.006310188211500645\n",
      "[step: 30500] loss: 0.0036507523618638515\n",
      "[step: 30550] loss: 0.0036359329242259264\n",
      "[step: 30600] loss: 0.003629841608926654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 30650] loss: 0.003623695345595479\n",
      "[step: 30700] loss: 0.0036174659617245197\n",
      "[step: 30750] loss: 0.12151304632425308\n",
      "[step: 30800] loss: 0.0036294872406870127\n",
      "[step: 30850] loss: 0.003620969830080867\n",
      "[step: 30900] loss: 0.003615035442635417\n",
      "[step: 30950] loss: 0.0036096705589443445\n",
      "[step: 31000] loss: 0.003604341298341751\n",
      "[step: 31050] loss: 0.0035989764146506786\n",
      "[step: 31100] loss: 0.003593534231185913\n",
      "[step: 31150] loss: 0.003587923012673855\n",
      "[step: 31200] loss: 0.003582299454137683\n",
      "[step: 31250] loss: 0.003576494287699461\n",
      "[step: 31300] loss: 0.0035706746857613325\n",
      "[step: 31350] loss: 0.0035646590404212475\n",
      "[step: 31400] loss: 0.24999740719795227\n",
      "[step: 31450] loss: 0.003901069751009345\n",
      "[step: 31500] loss: 0.003567429492250085\n",
      "[step: 31550] loss: 0.0035609807819128036\n",
      "[step: 31600] loss: 0.0035557628143578768\n",
      "[step: 31650] loss: 0.0035505741834640503\n",
      "[step: 31700] loss: 0.0035453089512884617\n",
      "[step: 31750] loss: 0.0035399605985730886\n",
      "[step: 31800] loss: 0.003534531919285655\n",
      "[step: 31850] loss: 0.003528974251821637\n",
      "[step: 31900] loss: 0.0035233809612691402\n",
      "[step: 31950] loss: 0.003517592791467905\n",
      "[step: 32000] loss: 0.003511760849505663\n",
      "[step: 32050] loss: 0.041555777192115784\n",
      "[step: 32100] loss: 0.003524457337334752\n",
      "[step: 32150] loss: 0.003517608158290386\n",
      "[step: 32200] loss: 0.0035118493251502514\n",
      "[step: 32250] loss: 0.0035069689620286226\n",
      "[step: 32300] loss: 0.003502040170133114\n",
      "[step: 32350] loss: 0.003497104160487652\n",
      "[step: 32400] loss: 0.003492046846076846\n",
      "[step: 32450] loss: 0.0034869378432631493\n",
      "[step: 32500] loss: 0.0034816835541278124\n",
      "[step: 32550] loss: 0.003476417623460293\n",
      "[step: 32600] loss: 0.0034709724131971598\n",
      "[step: 32650] loss: 0.0034659854136407375\n",
      "[step: 32700] loss: 0.012136703357100487\n",
      "[step: 32750] loss: 0.0034864081535488367\n",
      "[step: 32800] loss: 0.0034557455219328403\n",
      "[step: 32850] loss: 0.0034500565379858017\n",
      "[step: 32900] loss: 0.0034447861835360527\n",
      "[step: 32950] loss: 0.0042443531565368176\n",
      "[step: 33000] loss: 0.003442749846726656\n",
      "[step: 33050] loss: 0.0034373761154711246\n",
      "[step: 33100] loss: 0.0034319947008043528\n",
      "[step: 33150] loss: 0.003426707349717617\n",
      "[step: 33200] loss: 0.003421316621825099\n",
      "[step: 33250] loss: 0.0034158239141106606\n",
      "[step: 33300] loss: 0.003410270204767585\n",
      "[step: 33350] loss: 0.008588534779846668\n",
      "[step: 33400] loss: 0.0036111297085881233\n",
      "[step: 33450] loss: 0.003420374123379588\n",
      "[step: 33500] loss: 0.0034155629109591246\n",
      "[step: 33550] loss: 0.0034110061824321747\n",
      "[step: 33600] loss: 0.003406471572816372\n",
      "[step: 33650] loss: 0.0034019227605313063\n",
      "[step: 33700] loss: 0.003397370921447873\n",
      "[step: 33750] loss: 0.0033926789183169603\n",
      "[step: 33800] loss: 0.0033879834227263927\n",
      "[step: 33850] loss: 0.0033831805922091007\n",
      "[step: 33900] loss: 0.0033783407416194677\n",
      "[step: 33950] loss: 0.0033733099699020386\n",
      "[step: 34000] loss: 0.0033682361245155334\n",
      "[step: 34050] loss: 0.003363053547218442\n",
      "[step: 34100] loss: 0.01186191663146019\n",
      "[step: 34150] loss: 0.0035845921374857426\n",
      "[step: 34200] loss: 0.0033581266179680824\n",
      "[step: 34250] loss: 0.0033530136570334435\n",
      "[step: 34300] loss: 0.0033481193240731955\n",
      "[step: 34350] loss: 0.003343133023008704\n",
      "[step: 34400] loss: 0.003338098991662264\n",
      "[step: 34450] loss: 0.003340968629345298\n",
      "[step: 34500] loss: 0.004829476587474346\n",
      "[step: 34550] loss: 0.0033331632148474455\n",
      "[step: 34600] loss: 0.003325352445244789\n",
      "[step: 34650] loss: 0.003320242278277874\n",
      "[step: 34700] loss: 0.0033151949755847454\n",
      "[step: 34750] loss: 0.003310074098408222\n",
      "[step: 34800] loss: 0.00398890720680356\n",
      "[step: 34850] loss: 0.003332246793434024\n",
      "[step: 34900] loss: 0.003308950224891305\n",
      "[step: 34950] loss: 0.003303712699562311\n",
      "[step: 35000] loss: 0.0032990602776408195\n",
      "[step: 35050] loss: 0.0032943410333245993\n",
      "[step: 35100] loss: 0.0032895521726459265\n",
      "[step: 35150] loss: 0.0032846953254193068\n",
      "[step: 35200] loss: 0.0032797472085803747\n",
      "[step: 35250] loss: 0.0032746903598308563\n",
      "[step: 35300] loss: 0.0040776836685836315\n",
      "[step: 35350] loss: 0.0032970826141536236\n",
      "[step: 35400] loss: 0.0032757108565419912\n",
      "[step: 35450] loss: 0.00327083352021873\n",
      "[step: 35500] loss: 0.003266304964199662\n",
      "[step: 35550] loss: 0.0032617952674627304\n",
      "[step: 35600] loss: 0.0032571738120168447\n",
      "[step: 35650] loss: 0.003252539085224271\n",
      "[step: 35700] loss: 0.0032477856148034334\n",
      "[step: 35750] loss: 0.003242901060730219\n",
      "[step: 35800] loss: 0.003322779666632414\n",
      "[step: 35850] loss: 0.0061932522803545\n",
      "[step: 35900] loss: 0.003248760476708412\n",
      "[step: 35950] loss: 0.0032357836607843637\n",
      "[step: 36000] loss: 0.0032312546391040087\n",
      "[step: 36050] loss: 0.0032267102506011724\n",
      "[step: 36100] loss: 0.003222112078219652\n",
      "[step: 36150] loss: 0.0032173963263630867\n",
      "[step: 36200] loss: 0.00321265310049057\n",
      "[step: 36250] loss: 0.005605468526482582\n",
      "[step: 36300] loss: 0.0033828909508883953\n",
      "[step: 36350] loss: 0.0032173574436455965\n",
      "[step: 36400] loss: 0.0032045263797044754\n",
      "[step: 36450] loss: 0.0032000213395804167\n",
      "[step: 36500] loss: 0.0031954918522387743\n",
      "[step: 36550] loss: 0.003190916031599045\n",
      "[step: 36600] loss: 0.0031862168107181787\n",
      "[step: 36650] loss: 0.038066960871219635\n",
      "[step: 36700] loss: 0.0037428862415254116\n",
      "[step: 36750] loss: 0.003182305721566081\n",
      "[step: 36800] loss: 0.0031756798271089792\n",
      "[step: 36850] loss: 0.0031711359042674303\n",
      "[step: 36900] loss: 0.0031664716079831123\n",
      "[step: 36950] loss: 0.004479174502193928\n",
      "[step: 37000] loss: 0.003732086392119527\n",
      "[step: 37050] loss: 0.0031636743806302547\n",
      "[step: 37100] loss: 0.003154165344312787\n",
      "[step: 37150] loss: 0.003149526659399271\n",
      "[step: 37200] loss: 0.003201805055141449\n",
      "[step: 37250] loss: 0.0032011407893151045\n",
      "[step: 37300] loss: 0.0031558123882859945\n",
      "[step: 37350] loss: 0.0031387091148644686\n",
      "[step: 37400] loss: 0.0031341244466602802\n",
      "[step: 37450] loss: 0.0031296166125684977\n",
      "[step: 37500] loss: 0.006956149358302355\n",
      "[step: 37550] loss: 0.0032368586398661137\n",
      "[step: 37600] loss: 0.0031251010950654745\n",
      "[step: 37650] loss: 0.0031178428325802088\n",
      "[step: 37700] loss: 0.003113330341875553\n",
      "[step: 37750] loss: 0.003108807373791933\n",
      "[step: 37800] loss: 0.01381075568497181\n",
      "[step: 37850] loss: 0.0032102884724736214\n",
      "[step: 37900] loss: 0.00310434028506279\n",
      "[step: 37950] loss: 0.003099636407569051\n",
      "[step: 38000] loss: 0.003095254534855485\n",
      "[step: 38050] loss: 0.0030908274929970503\n",
      "[step: 38100] loss: 0.0030928216874599457\n",
      "[step: 38150] loss: 0.006711440626531839\n",
      "[step: 38200] loss: 0.0031068935059010983\n",
      "[step: 38250] loss: 0.003079519607126713\n",
      "[step: 38300] loss: 0.003075152635574341\n",
      "[step: 38350] loss: 0.00307072582654655\n",
      "[step: 38400] loss: 0.0032858499325811863\n",
      "[step: 38450] loss: 0.003076055785641074\n",
      "[step: 38500] loss: 0.0030752227175980806\n",
      "[step: 38550] loss: 0.003062825184315443\n",
      "[step: 38600] loss: 0.003058498492464423\n",
      "[step: 38650] loss: 0.0030542679596692324\n",
      "[step: 38700] loss: 0.0030499377753585577\n",
      "[step: 38750] loss: 0.003045625053346157\n",
      "[step: 38800] loss: 0.005046884994953871\n",
      "[step: 38850] loss: 0.003087898949161172\n",
      "[step: 38900] loss: 0.0030386175494641066\n",
      "[step: 38950] loss: 0.0030341290403157473\n",
      "[step: 39000] loss: 0.003029752755537629\n",
      "[step: 39050] loss: 0.11748086661100388\n",
      "[step: 39100] loss: 0.0035109275486320257\n",
      "[step: 39150] loss: 0.003024837700650096\n",
      "[step: 39200] loss: 0.003019931260496378\n",
      "[step: 39250] loss: 0.0030156634747982025\n",
      "[step: 39300] loss: 0.003011381020769477\n",
      "[step: 39350] loss: 0.0047786482609808445\n",
      "[step: 39400] loss: 0.0033550874795764685\n",
      "[step: 39450] loss: 0.003012001281604171\n",
      "[step: 39500] loss: 0.0030005360022187233\n",
      "[step: 39550] loss: 0.002996228402480483\n",
      "[step: 39600] loss: 0.002991949673742056\n",
      "[step: 39650] loss: 0.0072363438084721565\n",
      "[step: 39700] loss: 0.0030475377570837736\n",
      "[step: 39750] loss: 0.0029856774490326643\n",
      "[step: 39800] loss: 0.002981481608003378\n",
      "[step: 39850] loss: 0.002977273426949978\n",
      "[step: 39900] loss: 0.0030426315497606993\n",
      "[step: 39950] loss: 0.004743928089737892\n",
      "[step: 40000] loss: 0.002971939742565155\n",
      "[step: 40050] loss: 0.0029661948792636395\n",
      "[step: 40100] loss: 0.00296200392767787\n",
      "[step: 40150] loss: 0.002959636738523841\n",
      "[step: 40200] loss: 0.006970384158194065\n",
      "[step: 40250] loss: 0.002978258766233921\n",
      "[step: 40300] loss: 0.0029510706663131714\n",
      "[step: 40350] loss: 0.002946678549051285\n",
      "[step: 40400] loss: 0.002942851046100259\n",
      "[step: 40450] loss: 0.005029667168855667\n",
      "[step: 40500] loss: 0.00298990192823112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 40550] loss: 0.0029357587918639183\n",
      "[step: 40600] loss: 0.0029314253479242325\n",
      "[step: 40650] loss: 0.0029272870160639286\n",
      "[step: 40700] loss: 0.0084020821377635\n",
      "[step: 40750] loss: 0.0029314160346984863\n",
      "[step: 40800] loss: 0.002921842271462083\n",
      "[step: 40850] loss: 0.0029177775140851736\n",
      "[step: 40900] loss: 0.0029136326629668474\n",
      "[step: 40950] loss: 0.002909507369622588\n",
      "[step: 41000] loss: 0.017875196412205696\n",
      "[step: 41050] loss: 0.003127502044662833\n",
      "[step: 41100] loss: 0.0029038581997156143\n",
      "[step: 41150] loss: 0.0028995212633162737\n",
      "[step: 41200] loss: 0.002895462093874812\n",
      "[step: 41250] loss: 0.002894587116315961\n",
      "[step: 41300] loss: 0.006180247757583857\n",
      "[step: 41350] loss: 0.0028885831125080585\n",
      "[step: 41400] loss: 0.0028847153298556805\n",
      "[step: 41450] loss: 0.0028805003967136145\n",
      "[step: 41500] loss: 0.002876435639336705\n",
      "[step: 41550] loss: 0.024839451536536217\n",
      "[step: 41600] loss: 0.002994476119056344\n",
      "[step: 41650] loss: 0.0028729820623993874\n",
      "[step: 41700] loss: 0.00286860135383904\n",
      "[step: 41750] loss: 0.0028646159917116165\n",
      "[step: 41800] loss: 0.0028606937266886234\n",
      "[step: 41850] loss: 0.0028566804248839617\n",
      "[step: 41900] loss: 0.0333334319293499\n",
      "[step: 41950] loss: 0.004056386183947325\n",
      "[step: 42000] loss: 0.002861443441361189\n",
      "[step: 42050] loss: 0.0028511430136859417\n",
      "[step: 42100] loss: 0.0028473290149122477\n",
      "[step: 42150] loss: 0.0028435068670660257\n",
      "[step: 42200] loss: 0.0028396882116794586\n",
      "[step: 42250] loss: 0.0028358069248497486\n",
      "[step: 42300] loss: 0.002831860911101103\n",
      "[step: 42350] loss: 0.09841980785131454\n",
      "[step: 42400] loss: 0.003950927406549454\n",
      "[step: 42450] loss: 0.0028343976009637117\n",
      "[step: 42500] loss: 0.002827632473781705\n",
      "[step: 42550] loss: 0.0028239900711923838\n",
      "[step: 42600] loss: 0.002820289460942149\n",
      "[step: 42650] loss: 0.0028165762778371572\n",
      "[step: 42700] loss: 0.0028127634432166815\n",
      "[step: 42750] loss: 0.002808912890031934\n",
      "[step: 42800] loss: 0.0028055605944246054\n",
      "[step: 42850] loss: 0.008507736027240753\n",
      "[step: 42900] loss: 0.002838501473888755\n",
      "[step: 42950] loss: 0.002802284900099039\n",
      "[step: 43000] loss: 0.002798485802486539\n",
      "[step: 43050] loss: 0.002794750966131687\n",
      "[step: 43100] loss: 0.002790995640680194\n",
      "[step: 43150] loss: 0.002787171397358179\n",
      "[step: 43200] loss: 0.03915513679385185\n",
      "[step: 43250] loss: 0.003090165788307786\n",
      "[step: 43300] loss: 0.0027829569298774004\n",
      "[step: 43350] loss: 0.0027792821638286114\n",
      "[step: 43400] loss: 0.0027755273040384054\n",
      "[step: 43450] loss: 0.0027717556804418564\n",
      "[step: 43500] loss: 0.002771733794361353\n",
      "[step: 43550] loss: 0.007002345751971006\n",
      "[step: 43600] loss: 0.002786442171782255\n",
      "[step: 43650] loss: 0.0027642224449664354\n",
      "[step: 43700] loss: 0.002760463161394\n",
      "[step: 43750] loss: 0.0027567853685468435\n",
      "[step: 43800] loss: 0.002753023523837328\n",
      "[step: 43850] loss: 0.0166484247893095\n",
      "[step: 43900] loss: 0.002924351952970028\n",
      "[step: 43950] loss: 0.0027512286324054003\n",
      "[step: 44000] loss: 0.0027460146229714155\n",
      "[step: 44050] loss: 0.0027424213476479053\n",
      "[step: 44100] loss: 0.0027387479785829782\n",
      "[step: 44150] loss: 0.0027350171003490686\n",
      "[step: 44200] loss: 0.009515809826552868\n",
      "[step: 44250] loss: 0.0038224495947360992\n",
      "[step: 44300] loss: 0.002732356544584036\n",
      "[step: 44350] loss: 0.0027281243819743395\n",
      "[step: 44400] loss: 0.002724563702940941\n",
      "[step: 44450] loss: 0.002720901509746909\n",
      "[step: 44500] loss: 0.0027172728441655636\n",
      "[step: 44550] loss: 0.09604942798614502\n",
      "[step: 44600] loss: 0.0028021209873259068\n",
      "[step: 44650] loss: 0.0027164623606950045\n",
      "[step: 44700] loss: 0.002709645312279463\n",
      "[step: 44750] loss: 0.002706068567931652\n",
      "[step: 44800] loss: 0.0027024494484066963\n",
      "[step: 44850] loss: 0.002771651139482856\n",
      "[step: 44900] loss: 0.0033107048366218805\n",
      "[step: 44950] loss: 0.0027059155981987715\n",
      "[step: 45000] loss: 0.0026946724392473698\n",
      "[step: 45050] loss: 0.002691132016479969\n",
      "[step: 45100] loss: 0.002687497064471245\n",
      "[step: 45150] loss: 0.0029736740980297327\n",
      "[step: 45200] loss: 0.0028197879437357187\n",
      "[step: 45250] loss: 0.0026906209532171488\n",
      "[step: 45300] loss: 0.0026787712704390287\n",
      "[step: 45350] loss: 0.0026751772966235876\n",
      "[step: 45400] loss: 0.002671548631042242\n",
      "[step: 45450] loss: 0.037315476685762405\n",
      "[step: 45500] loss: 0.002844603033736348\n",
      "[step: 45550] loss: 0.002669448032975197\n",
      "[step: 45600] loss: 0.0026654317043721676\n",
      "[step: 45650] loss: 0.0026618936099112034\n",
      "[step: 45700] loss: 0.002658424898982048\n",
      "[step: 45750] loss: 0.0026548055466264486\n",
      "[step: 45800] loss: 0.004615184385329485\n",
      "[step: 45850] loss: 0.002659833524376154\n",
      "[step: 45900] loss: 0.002651335671544075\n",
      "[step: 45950] loss: 0.002647309098392725\n",
      "[step: 46000] loss: 0.00264379964210093\n",
      "[step: 46050] loss: 0.0026402364019304514\n",
      "[step: 46100] loss: 0.008981411345303059\n",
      "[step: 46150] loss: 0.0026573946233838797\n",
      "[step: 46200] loss: 0.0026364773511886597\n",
      "[step: 46250] loss: 0.0026329434476792812\n",
      "[step: 46300] loss: 0.0026295462157577276\n",
      "[step: 46350] loss: 0.002626037457957864\n",
      "[step: 46400] loss: 0.002814986975863576\n",
      "[step: 46450] loss: 0.00274830125272274\n",
      "[step: 46500] loss: 0.002621167106553912\n",
      "[step: 46550] loss: 0.002617160091176629\n",
      "[step: 46600] loss: 0.0026138375978916883\n",
      "[step: 46650] loss: 0.012709994800388813\n",
      "[step: 46700] loss: 0.0026318191085010767\n",
      "[step: 46750] loss: 0.002609165385365486\n",
      "[step: 46800] loss: 0.0026052796747535467\n",
      "[step: 46850] loss: 0.002601763466373086\n",
      "[step: 46900] loss: 0.04282805323600769\n",
      "[step: 46950] loss: 0.002801166847348213\n",
      "[step: 47000] loss: 0.0025981145445257425\n",
      "[step: 47050] loss: 0.0025944390799850225\n",
      "[step: 47100] loss: 0.0025910050608217716\n",
      "[step: 47150] loss: 0.0025878092274069786\n",
      "[step: 47200] loss: 0.012453383766114712\n",
      "[step: 47250] loss: 0.0025873612612485886\n",
      "[step: 47300] loss: 0.0025826068595051765\n",
      "[step: 47350] loss: 0.0025791069492697716\n",
      "[step: 47400] loss: 0.0025756473187357187\n",
      "[step: 47450] loss: 0.059941984713077545\n",
      "[step: 47500] loss: 0.0026552353519946337\n",
      "[step: 47550] loss: 0.002573582110926509\n",
      "[step: 47600] loss: 0.0025697501841932535\n",
      "[step: 47650] loss: 0.0025663869455456734\n",
      "[step: 47700] loss: 0.0025630276650190353\n",
      "[step: 47750] loss: 0.0025605943519622087\n",
      "[step: 47800] loss: 0.003994331229478121\n",
      "[step: 47850] loss: 0.0025919617619365454\n",
      "[step: 47900] loss: 0.002554840175434947\n",
      "[step: 47950] loss: 0.002551260171458125\n",
      "[step: 48000] loss: 0.002560069551691413\n",
      "[step: 48050] loss: 0.0029724501073360443\n",
      "[step: 48100] loss: 0.0025491458363831043\n",
      "[step: 48150] loss: 0.0025428268127143383\n",
      "[step: 48200] loss: 0.0025393636897206306\n",
      "[step: 48250] loss: 0.0032952374313026667\n",
      "[step: 48300] loss: 0.003908206708729267\n",
      "[step: 48350] loss: 0.0025374670512974262\n",
      "[step: 48400] loss: 0.0025317517574876547\n",
      "[step: 48450] loss: 0.002528408309444785\n",
      "[step: 48500] loss: 0.002525127725675702\n",
      "[step: 48550] loss: 0.013833664357662201\n",
      "[step: 48600] loss: 0.002590732416138053\n",
      "[step: 48650] loss: 0.0025207388680428267\n",
      "[step: 48700] loss: 0.002517351880669594\n",
      "[step: 48750] loss: 0.0025140810757875443\n",
      "[step: 48800] loss: 0.01692766696214676\n",
      "[step: 48850] loss: 0.0025221400428563356\n",
      "[step: 48900] loss: 0.0025089841801673174\n",
      "[step: 48950] loss: 0.002505633747205138\n",
      "[step: 49000] loss: 0.002502641174942255\n",
      "[step: 49050] loss: 0.003378075547516346\n",
      "[step: 49100] loss: 0.0025222518015652895\n",
      "[step: 49150] loss: 0.0024982248432934284\n",
      "[step: 49200] loss: 0.0024949535727500916\n",
      "[step: 49250] loss: 0.0024916399270296097\n",
      "[step: 49300] loss: 0.06108727678656578\n",
      "[step: 49350] loss: 0.0029328581877052784\n",
      "[step: 49400] loss: 0.0024880513083189726\n",
      "[step: 49450] loss: 0.0024846596643328667\n",
      "[step: 49500] loss: 0.0024813932832330465\n",
      "[step: 49550] loss: 0.0024780803360044956\n",
      "[step: 49600] loss: 0.023284105584025383\n",
      "[step: 49650] loss: 0.0025368747301399708\n",
      "[step: 49700] loss: 0.0024751180317252874\n",
      "[step: 49750] loss: 0.0024713275488466024\n",
      "[step: 49800] loss: 0.002468094229698181\n",
      "[step: 49850] loss: 0.0024650441482663155\n",
      "[step: 49900] loss: 0.011205475777387619\n",
      "[step: 49950] loss: 0.002477044705301523\n",
      "[step: 50000] loss: 0.0024617600720375776\n"
     ]
    }
   ],
   "source": [
    "for i in range(iterations):\n",
    "    _  , cost = sess.run([train ,loss], feed_dict={X: trainX, Y: trainY})\n",
    "    if (i+1) % (iterations/1000) == 0:\n",
    "        print(\"[step: {}] loss: {}\".format(i+1, cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4676764258640785\n"
     ]
    }
   ],
   "source": [
    "a=0\n",
    "length=len(dataY)\n",
    "for k in range(0,length-1):\n",
    "    a+=dataY[k][0]\n",
    "real_avg=a/length\n",
    "\n",
    "test_predict = sess.run(Y_pred, feed_dict={X: testX}) #예측값\n",
    "length_=len(test_predict)\n",
    "\n",
    "b=0\n",
    "for j in range(0,length_-1):\n",
    "    b+=test_predict[j][0]\n",
    "pred_avg=b/length_\n",
    "    \n",
    "#(절대값(실제값의 평균 - 예측값의 평균)/실제값의 평균) *100 =평균 오차율 산정 방식\n",
    "accuracy=(abs(real_avg-pred_avg)/real_avg)*100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#예측값 불러오기\n",
    "train_predict = sess.run(Y_pred, feed_dict={X: trainX})\n",
    "test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "\n",
    "\n",
    "plt.plot(testY,'r')\n",
    "plt.plot(test_predict,'b')\n",
    "plt.xlabel(\"day\")\n",
    "plt.ylabel(\"elec\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step3 (전력만,optim=RMSPropOptimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmlee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\jmlee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\jmlee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\jmlee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\jmlee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\jmlee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#http://mjgim.me/2017/08/02/LSTM.html\n",
    "\n",
    "#라이브러리 불러오기\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "tf.set_random_seed(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 불러오기\n",
    "jan = pd.read_csv(r\"C:/Users/jmlee/Desktop/4학년 2학기/딥러닝 전력 예측/전력 예측/생성data/1월.csv\", header=None)\n",
    "july = pd.read_csv(r\"C:/Users/jmlee/Desktop/4학년 2학기/딥러닝 전력 예측/전력 예측/생성data/7월.csv\", header=None)\n",
    "jan_=jan.loc[:,[1,2,4]][1:]\n",
    "# july_=july.loc[:,[1,2,4]][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>238.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>236.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>237.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>236.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>234.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2876</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>340.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2877</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>294.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2878</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>263.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2879</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2880</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>257.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2880 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1   2       4\n",
       "1     1   1  238.32\n",
       "2     1   1  236.16\n",
       "3     1   1  237.24\n",
       "4     1   1  236.16\n",
       "5     1   1  234.72\n",
       "...  ..  ..     ...\n",
       "2876  1  30   340.2\n",
       "2877  1  30  294.12\n",
       "2878  1  30  263.88\n",
       "2879  1  30     261\n",
       "2880  1  30  257.04\n",
       "\n",
       "[2880 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jan_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All=pd.concat([jan_,july_])\n",
    "#str을 float로 바꾸기\n",
    "jan_.columns=['월','일','전력량']\n",
    "jan_[['월','일','전력량']]=jan_[['월','일','전력량']].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinMaxScaler(data):\n",
    "    numerator=data-np.min(data,0)\n",
    "    denominator=np.max(data,0)-np.min(data,0)\n",
    "    return numerator/(denominator+1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#하이퍼 파라미터 설정\n",
    "timesteps = seq_length = 3  #한개의 시퀀스길이(시계열데이터 입력갯수)\n",
    "data_dim = 3  #variable개수\n",
    "hidden_dim = 3  #각셀의 출력크기\n",
    "output_dim = 1  #결과분류 총 수\n",
    "learing_rate = 0.001   #학습률\n",
    "iterations = 50_000   #에폭횟수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #데이터 조절\n",
    "# jan_[\"전력량\"] /= 1e5\n",
    "\n",
    "#Framework 제작\n",
    "jan_['전력량']=MinMaxScaler(jan_['전력량'])\n",
    "x = jan_[['월','일','전력량']].values\n",
    "y = jan_[\"전력량\"].values  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(y) - seq_length):  #seq_length=timesteps\n",
    "    _x = np.copy(x[i:i + seq_length + 1])\n",
    "#     _x[timesteps-2][data_dim-1] = 0\n",
    "#     _x[timesteps-1][data_dim-1] = 0\n",
    "#     _x[timesteps][data_dim-1] = 0\n",
    "    _y = [y[i + seq_length]] #다음 전력(정답)\n",
    "    dataX.append(_x)\n",
    "    dataY.append(_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습데이터와 테스트데이터 분류\n",
    "\n",
    "train_size = int(len(dataY) * 0.8)\n",
    "test_size = len(dataY) - train_size \n",
    "\n",
    "trainX = np.array(dataX[:train_size])  #2298\n",
    "testX = np.array(dataX[train_size : ])\n",
    "\n",
    "trainY = np.array(dataY[:train_size])  #575\n",
    "testY = np.array(dataY[train_size : ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-b7c77ad44290>:7: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-10-b7c77ad44290>:10: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    }
   ],
   "source": [
    "#LSTM모델 구축\n",
    "#placeholder 초기화 후 텐서에 매핑\n",
    "X = tf.placeholder(tf.float32, [None, seq_length+1, data_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "def lstm_cell(): #cell생성\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(hidden_dim, reuse=tf.AUTO_REUSE)  #출력의 크기를 hidden dim=4\n",
    "    return cell \n",
    "## 5 layers for hidden layer\n",
    "multi_cells = rnn.MultiRNNCell([lstm_cell() for _ in range(5)], state_is_tuple=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-d4faf3dbc2de>:2: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\jmlee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# dynamic rnn 구조(RNN 신경망)연결\n",
    "outputs, _states = tf.nn.dynamic_rnn(multi_cells, X, dtype=tf.float32) #결과값 output차원은 hidden_dim의 크기와 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=None)\n",
    "\n",
    "# sum of the squares= 표준편차(예측값과 실제값의차이)\n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSprop 알고리즘: 지수 가중 이동 평균 Exponentially weighted moving average 를 사용하여 최신 기울기들이 더 크게 반영\n",
    "train = tf.train.RMSPropOptimizer(learing_rate).minimize(loss)\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 50] loss: 196.5313262939453\n",
      "[step: 100] loss: 121.20664978027344\n",
      "[step: 150] loss: 118.00421142578125\n",
      "[step: 200] loss: 104.9573745727539\n",
      "[step: 250] loss: 68.87947082519531\n",
      "[step: 300] loss: 32.14753723144531\n",
      "[step: 350] loss: 20.981460571289062\n",
      "[step: 400] loss: 14.336662292480469\n",
      "[step: 450] loss: 11.33072566986084\n",
      "[step: 500] loss: 10.492339134216309\n",
      "[step: 550] loss: 10.223517417907715\n",
      "[step: 600] loss: 10.082488059997559\n",
      "[step: 650] loss: 9.973913192749023\n",
      "[step: 700] loss: 9.876992225646973\n",
      "[step: 750] loss: 9.786798477172852\n",
      "[step: 800] loss: 9.701591491699219\n",
      "[step: 850] loss: 9.620323181152344\n",
      "[step: 900] loss: 9.5421781539917\n",
      "[step: 950] loss: 9.466500282287598\n",
      "[step: 1000] loss: 9.392772674560547\n",
      "[step: 1050] loss: 9.320629119873047\n",
      "[step: 1100] loss: 9.2498140335083\n",
      "[step: 1150] loss: 9.180171012878418\n",
      "[step: 1200] loss: 9.111666679382324\n",
      "[step: 1250] loss: 9.04430866241455\n",
      "[step: 1300] loss: 8.978148460388184\n",
      "[step: 1350] loss: 8.913284301757812\n",
      "[step: 1400] loss: 8.849808692932129\n",
      "[step: 1450] loss: 8.787741661071777\n",
      "[step: 1500] loss: 8.72713851928711\n",
      "[step: 1550] loss: 8.667939186096191\n",
      "[step: 1600] loss: 8.61008071899414\n",
      "[step: 1650] loss: 8.553418159484863\n",
      "[step: 1700] loss: 8.497836112976074\n",
      "[step: 1750] loss: 8.443121910095215\n",
      "[step: 1800] loss: 8.389127731323242\n",
      "[step: 1850] loss: 8.335662841796875\n",
      "[step: 1900] loss: 8.28258228302002\n",
      "[step: 1950] loss: 8.229732513427734\n",
      "[step: 2000] loss: 8.176981925964355\n",
      "[step: 2050] loss: 8.124201774597168\n",
      "[step: 2100] loss: 8.071293830871582\n",
      "[step: 2150] loss: 8.018136978149414\n",
      "[step: 2200] loss: 7.964670658111572\n",
      "[step: 2250] loss: 7.910792350769043\n",
      "[step: 2300] loss: 7.856429576873779\n",
      "[step: 2350] loss: 7.801518440246582\n",
      "[step: 2400] loss: 7.745954513549805\n",
      "[step: 2450] loss: 7.689683437347412\n",
      "[step: 2500] loss: 7.632631301879883\n",
      "[step: 2550] loss: 7.574709415435791\n",
      "[step: 2600] loss: 7.515837669372559\n",
      "[step: 2650] loss: 7.455926418304443\n",
      "[step: 2700] loss: 7.394877910614014\n",
      "[step: 2750] loss: 7.332574367523193\n",
      "[step: 2800] loss: 7.268906593322754\n",
      "[step: 2850] loss: 7.203680515289307\n",
      "[step: 2900] loss: 7.136758804321289\n",
      "[step: 2950] loss: 7.067933559417725\n",
      "[step: 3000] loss: 6.996930122375488\n",
      "[step: 3050] loss: 6.923450946807861\n",
      "[step: 3100] loss: 6.8470635414123535\n",
      "[step: 3150] loss: 6.76727294921875\n",
      "[step: 3200] loss: 6.6833038330078125\n",
      "[step: 3250] loss: 6.59421968460083\n",
      "[step: 3300] loss: 6.498561859130859\n",
      "[step: 3350] loss: 6.394408702850342\n",
      "[step: 3400] loss: 6.279490947723389\n",
      "[step: 3450] loss: 6.153558731079102\n",
      "[step: 3500] loss: 6.02310037612915\n",
      "[step: 3550] loss: 5.896716117858887\n",
      "[step: 3600] loss: 5.774910926818848\n",
      "[step: 3650] loss: 5.654626846313477\n",
      "[step: 3700] loss: 5.532845973968506\n",
      "[step: 3750] loss: 5.406508445739746\n",
      "[step: 3800] loss: 5.272191524505615\n",
      "[step: 3850] loss: 5.132035732269287\n",
      "[step: 3900] loss: 4.9956512451171875\n",
      "[step: 3950] loss: 4.8586859703063965\n",
      "[step: 4000] loss: 4.721641540527344\n",
      "[step: 4050] loss: 4.583951473236084\n",
      "[step: 4100] loss: 4.449901580810547\n",
      "[step: 4150] loss: 4.319244861602783\n",
      "[step: 4200] loss: 4.189269542694092\n",
      "[step: 4250] loss: 4.059774875640869\n",
      "[step: 4300] loss: 3.9313249588012695\n",
      "[step: 4350] loss: 3.8054957389831543\n",
      "[step: 4400] loss: 3.6823959350585938\n",
      "[step: 4450] loss: 3.561735153198242\n",
      "[step: 4500] loss: 3.4431493282318115\n",
      "[step: 4550] loss: 3.326458215713501\n",
      "[step: 4600] loss: 3.21152400970459\n",
      "[step: 4650] loss: 3.097846746444702\n",
      "[step: 4700] loss: 2.984753370285034\n",
      "[step: 4750] loss: 2.871342182159424\n",
      "[step: 4800] loss: 2.757507085800171\n",
      "[step: 4850] loss: 2.6440536975860596\n",
      "[step: 4900] loss: 2.5330209732055664\n",
      "[step: 4950] loss: 2.426722764968872\n",
      "[step: 5000] loss: 2.3227195739746094\n",
      "[step: 5050] loss: 2.2181475162506104\n",
      "[step: 5100] loss: 2.1116607189178467\n",
      "[step: 5150] loss: 2.017703056335449\n",
      "[step: 5200] loss: 1.9470394849777222\n",
      "[step: 5250] loss: 1.8875863552093506\n",
      "[step: 5300] loss: 1.8357176780700684\n",
      "[step: 5350] loss: 1.7891372442245483\n",
      "[step: 5400] loss: 1.746737003326416\n",
      "[step: 5450] loss: 1.707400918006897\n",
      "[step: 5500] loss: 1.670250654220581\n",
      "[step: 5550] loss: 1.635292649269104\n",
      "[step: 5600] loss: 1.6022919416427612\n",
      "[step: 5650] loss: 1.570825219154358\n",
      "[step: 5700] loss: 1.5406620502471924\n",
      "[step: 5750] loss: 1.5105983018875122\n",
      "[step: 5800] loss: 1.4775707721710205\n",
      "[step: 5850] loss: 1.4485719203948975\n",
      "[step: 5900] loss: 1.423257827758789\n",
      "[step: 5950] loss: 1.3998234272003174\n",
      "[step: 6000] loss: 1.377582311630249\n",
      "[step: 6050] loss: 1.3562135696411133\n",
      "[step: 6100] loss: 1.335513710975647\n",
      "[step: 6150] loss: 1.3153111934661865\n",
      "[step: 6200] loss: 1.29547119140625\n",
      "[step: 6250] loss: 1.275858759880066\n",
      "[step: 6300] loss: 1.2562036514282227\n",
      "[step: 6350] loss: 1.2369457483291626\n",
      "[step: 6400] loss: 1.218667984008789\n",
      "[step: 6450] loss: 1.2010639905929565\n",
      "[step: 6500] loss: 1.1839466094970703\n",
      "[step: 6550] loss: 1.167210340499878\n",
      "[step: 6600] loss: 1.1508609056472778\n",
      "[step: 6650] loss: 1.1348150968551636\n",
      "[step: 6700] loss: 1.1190671920776367\n",
      "[step: 6750] loss: 1.1036356687545776\n",
      "[step: 6800] loss: 1.088494896888733\n",
      "[step: 6850] loss: 1.073610782623291\n",
      "[step: 6900] loss: 1.0589884519577026\n",
      "[step: 6950] loss: 1.0446149110794067\n",
      "[step: 7000] loss: 1.0304685831069946\n",
      "[step: 7050] loss: 1.016530156135559\n",
      "[step: 7100] loss: 1.002761960029602\n",
      "[step: 7150] loss: 0.9891176819801331\n",
      "[step: 7200] loss: 0.9757027626037598\n",
      "[step: 7250] loss: 0.9626462459564209\n",
      "[step: 7300] loss: 0.9501516819000244\n",
      "[step: 7350] loss: 0.9380286931991577\n",
      "[step: 7400] loss: 0.9262337684631348\n",
      "[step: 7450] loss: 0.914725124835968\n",
      "[step: 7500] loss: 0.9034557342529297\n",
      "[step: 7550] loss: 0.8923914432525635\n",
      "[step: 7600] loss: 0.8814507722854614\n",
      "[step: 7650] loss: 0.8705307245254517\n",
      "[step: 7700] loss: 0.8592987060546875\n",
      "[step: 7750] loss: 0.8475435376167297\n",
      "[step: 7800] loss: 0.8373993039131165\n",
      "[step: 7850] loss: 0.8277790546417236\n",
      "[step: 7900] loss: 0.8185212016105652\n",
      "[step: 7950] loss: 0.8095319867134094\n",
      "[step: 8000] loss: 0.8005448579788208\n",
      "[step: 8050] loss: 0.7916346192359924\n",
      "[step: 8100] loss: 0.7830589413642883\n",
      "[step: 8150] loss: 0.7748737931251526\n",
      "[step: 8200] loss: 0.7670783996582031\n",
      "[step: 8250] loss: 0.7598366141319275\n",
      "[step: 8300] loss: 0.7530630230903625\n",
      "[step: 8350] loss: 0.7465712428092957\n",
      "[step: 8400] loss: 0.7403722405433655\n",
      "[step: 8450] loss: 0.7344951629638672\n",
      "[step: 8500] loss: 0.7289328575134277\n",
      "[step: 8550] loss: 0.7236335277557373\n",
      "[step: 8600] loss: 0.7185514569282532\n",
      "[step: 8650] loss: 0.7136313915252686\n",
      "[step: 8700] loss: 0.7088039517402649\n",
      "[step: 8750] loss: 0.7041388750076294\n",
      "[step: 8800] loss: 0.6996631622314453\n",
      "[step: 8850] loss: 0.6953968405723572\n",
      "[step: 8900] loss: 0.6912981271743774\n",
      "[step: 8950] loss: 0.6873444318771362\n",
      "[step: 9000] loss: 0.6835529208183289\n",
      "[step: 9050] loss: 0.6798592209815979\n",
      "[step: 9100] loss: 0.676278293132782\n",
      "[step: 9150] loss: 0.672766923904419\n",
      "[step: 9200] loss: 0.6698383092880249\n",
      "[step: 9250] loss: 0.6666339635848999\n",
      "[step: 9300] loss: 0.6636289358139038\n",
      "[step: 9350] loss: 0.6606255173683167\n",
      "[step: 9400] loss: 0.6576840281486511\n",
      "[step: 9450] loss: 0.6548919677734375\n",
      "[step: 9500] loss: 0.6521838307380676\n",
      "[step: 9550] loss: 0.6495569944381714\n",
      "[step: 9600] loss: 0.6469981074333191\n",
      "[step: 9650] loss: 0.6444923281669617\n",
      "[step: 9700] loss: 0.6420441269874573\n",
      "[step: 9750] loss: 0.6396303176879883\n",
      "[step: 9800] loss: 0.6372557878494263\n",
      "[step: 9850] loss: 0.6349325180053711\n",
      "[step: 9900] loss: 0.6327164769172668\n",
      "[step: 9950] loss: 0.63055819272995\n",
      "[step: 10000] loss: 0.6284596920013428\n",
      "[step: 10050] loss: 0.6264055371284485\n",
      "[step: 10100] loss: 0.6243817210197449\n",
      "[step: 10150] loss: 0.622402012348175\n",
      "[step: 10200] loss: 0.6204461455345154\n",
      "[step: 10250] loss: 0.6185111403465271\n",
      "[step: 10300] loss: 0.6165995597839355\n",
      "[step: 10350] loss: 0.6146759390830994\n",
      "[step: 10400] loss: 0.6127991676330566\n",
      "[step: 10450] loss: 0.6109400391578674\n",
      "[step: 10500] loss: 0.6091397404670715\n",
      "[step: 10550] loss: 0.6073659658432007\n",
      "[step: 10600] loss: 0.6056023836135864\n",
      "[step: 10650] loss: 0.6038668155670166\n",
      "[step: 10700] loss: 0.6021469235420227\n",
      "[step: 10750] loss: 0.6004509925842285\n",
      "[step: 10800] loss: 0.5987555980682373\n",
      "[step: 10850] loss: 0.5970735549926758\n",
      "[step: 10900] loss: 0.5953975319862366\n",
      "[step: 10950] loss: 0.5937259793281555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 11000] loss: 0.5920599102973938\n",
      "[step: 11050] loss: 0.5904041528701782\n",
      "[step: 11100] loss: 0.588740348815918\n",
      "[step: 11150] loss: 0.587066113948822\n",
      "[step: 11200] loss: 0.5853763222694397\n",
      "[step: 11250] loss: 0.5837041139602661\n",
      "[step: 11300] loss: 0.5820680856704712\n",
      "[step: 11350] loss: 0.5804299712181091\n",
      "[step: 11400] loss: 0.5788232088088989\n",
      "[step: 11450] loss: 0.5769389271736145\n",
      "[step: 11500] loss: 0.5762054920196533\n",
      "[step: 11550] loss: 0.5739495158195496\n",
      "[step: 11600] loss: 0.5724710822105408\n",
      "[step: 11650] loss: 0.5712260603904724\n",
      "[step: 11700] loss: 0.569088339805603\n",
      "[step: 11750] loss: 0.5682721138000488\n",
      "[step: 11800] loss: 0.5658791065216064\n",
      "[step: 11850] loss: 0.5642157196998596\n",
      "[step: 11900] loss: 0.5632585287094116\n",
      "[step: 11950] loss: 0.5613360404968262\n",
      "[step: 12000] loss: 0.5594761371612549\n",
      "[step: 12050] loss: 0.5577852129936218\n",
      "[step: 12100] loss: 0.5561836361885071\n",
      "[step: 12150] loss: 0.5547003746032715\n",
      "[step: 12200] loss: 0.5531052947044373\n",
      "[step: 12250] loss: 0.5515428781509399\n",
      "[step: 12300] loss: 0.5499834418296814\n",
      "[step: 12350] loss: 0.5484323501586914\n",
      "[step: 12400] loss: 0.5468897819519043\n",
      "[step: 12450] loss: 0.5453536510467529\n",
      "[step: 12500] loss: 0.5438247323036194\n",
      "[step: 12550] loss: 0.5423070788383484\n",
      "[step: 12600] loss: 0.5407857298851013\n",
      "[step: 12650] loss: 0.5392773747444153\n",
      "[step: 12700] loss: 0.5377659797668457\n",
      "[step: 12750] loss: 0.5362712740898132\n",
      "[step: 12800] loss: 0.5347766280174255\n",
      "[step: 12850] loss: 0.5332801938056946\n",
      "[step: 12900] loss: 0.5317920446395874\n",
      "[step: 12950] loss: 0.5303066372871399\n",
      "[step: 13000] loss: 0.5288205146789551\n",
      "[step: 13050] loss: 0.52734375\n",
      "[step: 13100] loss: 0.5258590579032898\n",
      "[step: 13150] loss: 0.5243921279907227\n",
      "[step: 13200] loss: 0.522921621799469\n",
      "[step: 13250] loss: 0.5214589238166809\n",
      "[step: 13300] loss: 0.5199909210205078\n",
      "[step: 13350] loss: 0.5185189247131348\n",
      "[step: 13400] loss: 0.5170418620109558\n",
      "[step: 13450] loss: 0.5155621767044067\n",
      "[step: 13500] loss: 0.5140748023986816\n",
      "[step: 13550] loss: 0.5125741958618164\n",
      "[step: 13600] loss: 0.511052131652832\n",
      "[step: 13650] loss: 0.5095084309577942\n",
      "[step: 13700] loss: 0.5079312920570374\n",
      "[step: 13750] loss: 0.5063298344612122\n",
      "[step: 13800] loss: 0.5047106742858887\n",
      "[step: 13850] loss: 0.5032160878181458\n",
      "[step: 13900] loss: 0.5017581582069397\n",
      "[step: 13950] loss: 0.5003703832626343\n",
      "[step: 14000] loss: 0.4990180432796478\n",
      "[step: 14050] loss: 0.4976976215839386\n",
      "[step: 14100] loss: 0.49640703201293945\n",
      "[step: 14150] loss: 0.49512937664985657\n",
      "[step: 14200] loss: 0.49387073516845703\n",
      "[step: 14250] loss: 0.49263155460357666\n",
      "[step: 14300] loss: 0.4914150834083557\n",
      "[step: 14350] loss: 0.49019384384155273\n",
      "[step: 14400] loss: 0.48898985981941223\n",
      "[step: 14450] loss: 0.48780882358551025\n",
      "[step: 14500] loss: 0.486633837223053\n",
      "[step: 14550] loss: 0.48546499013900757\n",
      "[step: 14600] loss: 0.4843091070652008\n",
      "[step: 14650] loss: 0.48316869139671326\n",
      "[step: 14700] loss: 0.48206040263175964\n",
      "[step: 14750] loss: 0.4809395968914032\n",
      "[step: 14800] loss: 0.4798310101032257\n",
      "[step: 14850] loss: 0.4787248373031616\n",
      "[step: 14900] loss: 0.4776354134082794\n",
      "[step: 14950] loss: 0.47655177116394043\n",
      "[step: 15000] loss: 0.47547447681427\n",
      "[step: 15050] loss: 0.4744078516960144\n",
      "[step: 15100] loss: 0.4733496904373169\n",
      "[step: 15150] loss: 0.4722929894924164\n",
      "[step: 15200] loss: 0.4712532162666321\n",
      "[step: 15250] loss: 0.47021952271461487\n",
      "[step: 15300] loss: 0.46918314695358276\n",
      "[step: 15350] loss: 0.46815457940101624\n",
      "[step: 15400] loss: 0.46714094281196594\n",
      "[step: 15450] loss: 0.46612638235092163\n",
      "[step: 15500] loss: 0.4651217460632324\n",
      "[step: 15550] loss: 0.4641163647174835\n",
      "[step: 15600] loss: 0.4631291329860687\n",
      "[step: 15650] loss: 0.4621361792087555\n",
      "[step: 15700] loss: 0.4611538052558899\n",
      "[step: 15750] loss: 0.4601627290248871\n",
      "[step: 15800] loss: 0.4591790437698364\n",
      "[step: 15850] loss: 0.45821085572242737\n",
      "[step: 15900] loss: 0.45723384618759155\n",
      "[step: 15950] loss: 0.45625364780426025\n",
      "[step: 16000] loss: 0.45527973771095276\n",
      "[step: 16050] loss: 0.4543015658855438\n",
      "[step: 16100] loss: 0.45322322845458984\n",
      "[step: 16150] loss: 0.45264938473701477\n",
      "[step: 16200] loss: 0.45134907960891724\n",
      "[step: 16250] loss: 0.45063701272010803\n",
      "[step: 16300] loss: 0.4497101604938507\n",
      "[step: 16350] loss: 0.4487825930118561\n",
      "[step: 16400] loss: 0.4478580355644226\n",
      "[step: 16450] loss: 0.44691118597984314\n",
      "[step: 16500] loss: 0.4459739029407501\n",
      "[step: 16550] loss: 0.44502583146095276\n",
      "[step: 16600] loss: 0.44411197304725647\n",
      "[step: 16650] loss: 0.4432406425476074\n",
      "[step: 16700] loss: 0.44238972663879395\n",
      "[step: 16750] loss: 0.441573828458786\n",
      "[step: 16800] loss: 0.4407576322555542\n",
      "[step: 16850] loss: 0.4399601221084595\n",
      "[step: 16900] loss: 0.4391692876815796\n",
      "[step: 16950] loss: 0.43838611245155334\n",
      "[step: 17000] loss: 0.43761202692985535\n",
      "[step: 17050] loss: 0.4368456304073334\n",
      "[step: 17100] loss: 0.43608638644218445\n",
      "[step: 17150] loss: 0.4353382885456085\n",
      "[step: 17200] loss: 0.4345925450325012\n",
      "[step: 17250] loss: 0.43385687470436096\n",
      "[step: 17300] loss: 0.43312549591064453\n",
      "[step: 17350] loss: 0.4323984384536743\n",
      "[step: 17400] loss: 0.43167349696159363\n",
      "[step: 17450] loss: 0.4309541583061218\n",
      "[step: 17500] loss: 0.43024545907974243\n",
      "[step: 17550] loss: 0.42953193187713623\n",
      "[step: 17600] loss: 0.42882728576660156\n",
      "[step: 17650] loss: 0.4281240701675415\n",
      "[step: 17700] loss: 0.42736130952835083\n",
      "[step: 17750] loss: 0.4266579747200012\n",
      "[step: 17800] loss: 0.42596471309661865\n",
      "[step: 17850] loss: 0.4252854883670807\n",
      "[step: 17900] loss: 0.42461010813713074\n",
      "[step: 17950] loss: 0.42393213510513306\n",
      "[step: 18000] loss: 0.4232621490955353\n",
      "[step: 18050] loss: 0.42260345816612244\n",
      "[step: 18100] loss: 0.42194411158561707\n",
      "[step: 18150] loss: 0.4212920069694519\n",
      "[step: 18200] loss: 0.4206421673297882\n",
      "[step: 18250] loss: 0.41999709606170654\n",
      "[step: 18300] loss: 0.41935133934020996\n",
      "[step: 18350] loss: 0.41871437430381775\n",
      "[step: 18400] loss: 0.41807544231414795\n",
      "[step: 18450] loss: 0.41744524240493774\n",
      "[step: 18500] loss: 0.4168175756931305\n",
      "[step: 18550] loss: 0.4161953926086426\n",
      "[step: 18600] loss: 0.4155634343624115\n",
      "[step: 18650] loss: 0.41494032740592957\n",
      "[step: 18700] loss: 0.4143272638320923\n",
      "[step: 18750] loss: 0.4137060046195984\n",
      "[step: 18800] loss: 0.4130931496620178\n",
      "[step: 18850] loss: 0.4124869704246521\n",
      "[step: 18900] loss: 0.4118771255016327\n",
      "[step: 18950] loss: 0.4112776219844818\n",
      "[step: 19000] loss: 0.41066691279411316\n",
      "[step: 19050] loss: 0.4100682735443115\n",
      "[step: 19100] loss: 0.40947583317756653\n",
      "[step: 19150] loss: 0.4088805913925171\n",
      "[step: 19200] loss: 0.408293217420578\n",
      "[step: 19250] loss: 0.4077110290527344\n",
      "[step: 19300] loss: 0.4071218967437744\n",
      "[step: 19350] loss: 0.406546413898468\n",
      "[step: 19400] loss: 0.4059661030769348\n",
      "[step: 19450] loss: 0.40539342164993286\n",
      "[step: 19500] loss: 0.40482449531555176\n",
      "[step: 19550] loss: 0.40426039695739746\n",
      "[step: 19600] loss: 0.4036904275417328\n",
      "[step: 19650] loss: 0.4031095802783966\n",
      "[step: 19700] loss: 0.40256115794181824\n",
      "[step: 19750] loss: 0.40200385451316833\n",
      "[step: 19800] loss: 0.40145155787467957\n",
      "[step: 19850] loss: 0.4009093940258026\n",
      "[step: 19900] loss: 0.4003649055957794\n",
      "[step: 19950] loss: 0.3998214304447174\n",
      "[step: 20000] loss: 0.3992825150489807\n",
      "[step: 20050] loss: 0.3986963927745819\n",
      "[step: 20100] loss: 0.39821866154670715\n",
      "[step: 20150] loss: 0.39769870042800903\n",
      "[step: 20200] loss: 0.3971835672855377\n",
      "[step: 20250] loss: 0.39667612314224243\n",
      "[step: 20300] loss: 0.39617523550987244\n",
      "[step: 20350] loss: 0.39567649364471436\n",
      "[step: 20400] loss: 0.39517512917518616\n",
      "[step: 20450] loss: 0.3946835994720459\n",
      "[step: 20500] loss: 0.3941906988620758\n",
      "[step: 20550] loss: 0.393704354763031\n",
      "[step: 20600] loss: 0.3932182490825653\n",
      "[step: 20650] loss: 0.3927376866340637\n",
      "[step: 20700] loss: 0.3922537863254547\n",
      "[step: 20750] loss: 0.39177078008651733\n",
      "[step: 20800] loss: 0.3912924826145172\n",
      "[step: 20850] loss: 0.39081278443336487\n",
      "[step: 20900] loss: 0.39033275842666626\n",
      "[step: 20950] loss: 0.3898577392101288\n",
      "[step: 21000] loss: 0.3893846869468689\n",
      "[step: 21050] loss: 0.3889084756374359\n",
      "[step: 21100] loss: 0.38843172788619995\n",
      "[step: 21150] loss: 0.3879562020301819\n",
      "[step: 21200] loss: 0.38748282194137573\n",
      "[step: 21250] loss: 0.3870069682598114\n",
      "[step: 21300] loss: 0.3865280747413635\n",
      "[step: 21350] loss: 0.3860654830932617\n",
      "[step: 21400] loss: 0.3855591416358948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 21450] loss: 0.3850901424884796\n",
      "[step: 21500] loss: 0.3846123218536377\n",
      "[step: 21550] loss: 0.384136825799942\n",
      "[step: 21600] loss: 0.38365861773490906\n",
      "[step: 21650] loss: 0.383175790309906\n",
      "[step: 21700] loss: 0.38267290592193604\n",
      "[step: 21750] loss: 0.38209444284439087\n",
      "[step: 21800] loss: 0.38165757060050964\n",
      "[step: 21850] loss: 0.3812050223350525\n",
      "[step: 21900] loss: 0.38075050711631775\n",
      "[step: 21950] loss: 0.3803010880947113\n",
      "[step: 22000] loss: 0.3798518776893616\n",
      "[step: 22050] loss: 0.37939688563346863\n",
      "[step: 22100] loss: 0.37895065546035767\n",
      "[step: 22150] loss: 0.3785051703453064\n",
      "[step: 22200] loss: 0.3780583143234253\n",
      "[step: 22250] loss: 0.37761375308036804\n",
      "[step: 22300] loss: 0.3771687150001526\n",
      "[step: 22350] loss: 0.3767187297344208\n",
      "[step: 22400] loss: 0.376277893781662\n",
      "[step: 22450] loss: 0.3758271336555481\n",
      "[step: 22500] loss: 0.37538278102874756\n",
      "[step: 22550] loss: 0.37493789196014404\n",
      "[step: 22600] loss: 0.37448886036872864\n",
      "[step: 22650] loss: 0.3740466833114624\n",
      "[step: 22700] loss: 0.3735952377319336\n",
      "[step: 22750] loss: 0.37316247820854187\n",
      "[step: 22800] loss: 0.3727365732192993\n",
      "[step: 22850] loss: 0.3722969591617584\n",
      "[step: 22900] loss: 0.3718684911727905\n",
      "[step: 22950] loss: 0.37143591046333313\n",
      "[step: 23000] loss: 0.37100449204444885\n",
      "[step: 23050] loss: 0.3705756962299347\n",
      "[step: 23100] loss: 0.3701358437538147\n",
      "[step: 23150] loss: 0.369704931974411\n",
      "[step: 23200] loss: 0.3692690134048462\n",
      "[step: 23250] loss: 0.36883240938186646\n",
      "[step: 23300] loss: 0.3683987855911255\n",
      "[step: 23350] loss: 0.3679606318473816\n",
      "[step: 23400] loss: 0.36752286553382874\n",
      "[step: 23450] loss: 0.3670694828033447\n",
      "[step: 23500] loss: 0.36660200357437134\n",
      "[step: 23550] loss: 0.36613625288009644\n",
      "[step: 23600] loss: 0.36565062403678894\n",
      "[step: 23650] loss: 0.3651658594608307\n",
      "[step: 23700] loss: 0.3646959960460663\n",
      "[step: 23750] loss: 0.3642413914203644\n",
      "[step: 23800] loss: 0.3637821078300476\n",
      "[step: 23850] loss: 0.3633694052696228\n",
      "[step: 23900] loss: 0.36292433738708496\n",
      "[step: 23950] loss: 0.3624539375305176\n",
      "[step: 24000] loss: 0.3620326817035675\n",
      "[step: 24050] loss: 0.3616144061088562\n",
      "[step: 24100] loss: 0.3612055480480194\n",
      "[step: 24150] loss: 0.36079299449920654\n",
      "[step: 24200] loss: 0.360395610332489\n",
      "[step: 24250] loss: 0.35999640822410583\n",
      "[step: 24300] loss: 0.3596023917198181\n",
      "[step: 24350] loss: 0.35920658707618713\n",
      "[step: 24400] loss: 0.35881656408309937\n",
      "[step: 24450] loss: 0.35842737555503845\n",
      "[step: 24500] loss: 0.35804447531700134\n",
      "[step: 24550] loss: 0.3576553165912628\n",
      "[step: 24600] loss: 0.35726648569107056\n",
      "[step: 24650] loss: 0.3568827211856842\n",
      "[step: 24700] loss: 0.35650500655174255\n",
      "[step: 24750] loss: 0.3561207056045532\n",
      "[step: 24800] loss: 0.3557307720184326\n",
      "[step: 24850] loss: 0.3553474247455597\n",
      "[step: 24900] loss: 0.3549661934375763\n",
      "[step: 24950] loss: 0.35457944869995117\n",
      "[step: 25000] loss: 0.3541889488697052\n",
      "[step: 25050] loss: 0.3538031280040741\n",
      "[step: 25100] loss: 0.35339444875717163\n",
      "[step: 25150] loss: 0.35296177864074707\n",
      "[step: 25200] loss: 0.35255807638168335\n",
      "[step: 25250] loss: 0.3521267771720886\n",
      "[step: 25300] loss: 0.3515991270542145\n",
      "[step: 25350] loss: 0.35119035840034485\n",
      "[step: 25400] loss: 0.35082265734672546\n",
      "[step: 25450] loss: 0.35034623742103577\n",
      "[step: 25500] loss: 0.3499765396118164\n",
      "[step: 25550] loss: 0.3496059775352478\n",
      "[step: 25600] loss: 0.3492355942726135\n",
      "[step: 25650] loss: 0.34886693954467773\n",
      "[step: 25700] loss: 0.3484978973865509\n",
      "[step: 25750] loss: 0.34813326597213745\n",
      "[step: 25800] loss: 0.34776580333709717\n",
      "[step: 25850] loss: 0.34739774465560913\n",
      "[step: 25900] loss: 0.34703329205513\n",
      "[step: 25950] loss: 0.34666892886161804\n",
      "[step: 26000] loss: 0.34630322456359863\n",
      "[step: 26050] loss: 0.345935195684433\n",
      "[step: 26100] loss: 0.3455686569213867\n",
      "[step: 26150] loss: 0.3451988399028778\n",
      "[step: 26200] loss: 0.3448319137096405\n",
      "[step: 26250] loss: 0.34444674849510193\n",
      "[step: 26300] loss: 0.34408342838287354\n",
      "[step: 26350] loss: 0.343715637922287\n",
      "[step: 26400] loss: 0.34334635734558105\n",
      "[step: 26450] loss: 0.34297311305999756\n",
      "[step: 26500] loss: 0.3425995111465454\n",
      "[step: 26550] loss: 0.342229425907135\n",
      "[step: 26600] loss: 0.3418509364128113\n",
      "[step: 26650] loss: 0.341474324464798\n",
      "[step: 26700] loss: 0.3410937190055847\n",
      "[step: 26750] loss: 0.34071245789527893\n",
      "[step: 26800] loss: 0.34032514691352844\n",
      "[step: 26850] loss: 0.33993256092071533\n",
      "[step: 26900] loss: 0.3395400047302246\n",
      "[step: 26950] loss: 0.33912232518196106\n",
      "[step: 27000] loss: 0.3387100398540497\n",
      "[step: 27050] loss: 0.3382750153541565\n",
      "[step: 27100] loss: 0.3378197252750397\n",
      "[step: 27150] loss: 0.3373650014400482\n",
      "[step: 27200] loss: 0.33692267537117004\n",
      "[step: 27250] loss: 0.33648163080215454\n",
      "[step: 27300] loss: 0.336034893989563\n",
      "[step: 27350] loss: 0.3355952501296997\n",
      "[step: 27400] loss: 0.33515897393226624\n",
      "[step: 27450] loss: 0.334736168384552\n",
      "[step: 27500] loss: 0.33431050181388855\n",
      "[step: 27550] loss: 0.333898663520813\n",
      "[step: 27600] loss: 0.3334963619709015\n",
      "[step: 27650] loss: 0.33309656381607056\n",
      "[step: 27700] loss: 0.33269619941711426\n",
      "[step: 27750] loss: 0.33230239152908325\n",
      "[step: 27800] loss: 0.33190977573394775\n",
      "[step: 27850] loss: 0.3315165042877197\n",
      "[step: 27900] loss: 0.33112576603889465\n",
      "[step: 27950] loss: 0.3307284414768219\n",
      "[step: 28000] loss: 0.33033570647239685\n",
      "[step: 28050] loss: 0.32993558049201965\n",
      "[step: 28100] loss: 0.32954326272010803\n",
      "[step: 28150] loss: 0.3291536271572113\n",
      "[step: 28200] loss: 0.32877177000045776\n",
      "[step: 28250] loss: 0.3283942937850952\n",
      "[step: 28300] loss: 0.3280104994773865\n",
      "[step: 28350] loss: 0.32763466238975525\n",
      "[step: 28400] loss: 0.3272593021392822\n",
      "[step: 28450] loss: 0.32688507437705994\n",
      "[step: 28500] loss: 0.32651177048683167\n",
      "[step: 28550] loss: 0.32613807916641235\n",
      "[step: 28600] loss: 0.3257691264152527\n",
      "[step: 28650] loss: 0.3254016041755676\n",
      "[step: 28700] loss: 0.32502520084381104\n",
      "[step: 28750] loss: 0.32465752959251404\n",
      "[step: 28800] loss: 0.32428842782974243\n",
      "[step: 28850] loss: 0.3239026367664337\n",
      "[step: 28900] loss: 0.3235096335411072\n",
      "[step: 28950] loss: 0.32315096259117126\n",
      "[step: 29000] loss: 0.3227854371070862\n",
      "[step: 29050] loss: 0.32241395115852356\n",
      "[step: 29100] loss: 0.322040855884552\n",
      "[step: 29150] loss: 0.3216111660003662\n",
      "[step: 29200] loss: 0.3211469054222107\n",
      "[step: 29250] loss: 0.3207942247390747\n",
      "[step: 29300] loss: 0.3205001950263977\n",
      "[step: 29350] loss: 0.32031509280204773\n",
      "[step: 29400] loss: 0.3200357258319855\n",
      "[step: 29450] loss: 0.3194519579410553\n",
      "[step: 29500] loss: 0.31902971863746643\n",
      "[step: 29550] loss: 0.3187081813812256\n",
      "[step: 29600] loss: 0.318440318107605\n",
      "[step: 29650] loss: 0.31823405623435974\n",
      "[step: 29700] loss: 0.31797245144844055\n",
      "[step: 29750] loss: 0.3176126778125763\n",
      "[step: 29800] loss: 0.3171515166759491\n",
      "[step: 29850] loss: 0.3166825771331787\n",
      "[step: 29900] loss: 0.3162873089313507\n",
      "[step: 29950] loss: 0.3159714341163635\n",
      "[step: 30000] loss: 0.31573501229286194\n",
      "[step: 30050] loss: 0.3160584270954132\n",
      "[step: 30100] loss: 0.3148554265499115\n",
      "[step: 30150] loss: 0.3148122727870941\n",
      "[step: 30200] loss: 0.3145909011363983\n",
      "[step: 30250] loss: 0.3139171600341797\n",
      "[step: 30300] loss: 0.31405794620513916\n",
      "[step: 30350] loss: 0.3138607144355774\n",
      "[step: 30400] loss: 0.313144326210022\n",
      "[step: 30450] loss: 0.31266772747039795\n",
      "[step: 30500] loss: 0.31259334087371826\n",
      "[step: 30550] loss: 0.3125298321247101\n",
      "[step: 30600] loss: 0.312467098236084\n",
      "[step: 30650] loss: 0.31190529465675354\n",
      "[step: 30700] loss: 0.31131091713905334\n",
      "[step: 30750] loss: 0.3110109269618988\n",
      "[step: 30800] loss: 0.31084996461868286\n",
      "[step: 30850] loss: 0.3107386529445648\n",
      "[step: 30900] loss: 0.31071043014526367\n",
      "[step: 30950] loss: 0.310494065284729\n",
      "[step: 31000] loss: 0.3101217746734619\n",
      "[step: 31050] loss: 0.30984777212142944\n",
      "[step: 31100] loss: 0.3095974922180176\n",
      "[step: 31150] loss: 0.3091331124305725\n",
      "[step: 31200] loss: 0.3086111843585968\n",
      "[step: 31250] loss: 0.30825990438461304\n",
      "[step: 31300] loss: 0.30801400542259216\n",
      "[step: 31350] loss: 0.30810293555259705\n",
      "[step: 31400] loss: 0.3074924349784851\n",
      "[step: 31450] loss: 0.3072284162044525\n",
      "[step: 31500] loss: 0.3071431517601013\n",
      "[step: 31550] loss: 0.306684672832489\n",
      "[step: 31600] loss: 0.30661818385124207\n",
      "[step: 31650] loss: 0.30619505047798157\n",
      "[step: 31700] loss: 0.30610740184783936\n",
      "[step: 31750] loss: 0.305596262216568\n",
      "[step: 31800] loss: 0.3054414391517639\n",
      "[step: 31850] loss: 0.30513522028923035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 31900] loss: 0.30474475026130676\n",
      "[step: 31950] loss: 0.3045203685760498\n",
      "[step: 32000] loss: 0.30459854006767273\n",
      "[step: 32050] loss: 0.30445194244384766\n",
      "[step: 32100] loss: 0.30413946509361267\n",
      "[step: 32150] loss: 0.3038133382797241\n",
      "[step: 32200] loss: 0.3034723699092865\n",
      "[step: 32250] loss: 0.303093820810318\n",
      "[step: 32300] loss: 0.30270057916641235\n",
      "[step: 32350] loss: 0.3023560345172882\n",
      "[step: 32400] loss: 0.3020753264427185\n",
      "[step: 32450] loss: 0.30202245712280273\n",
      "[step: 32500] loss: 0.3018035888671875\n",
      "[step: 32550] loss: 0.3013472855091095\n",
      "[step: 32600] loss: 0.30137205123901367\n",
      "[step: 32650] loss: 0.300873339176178\n",
      "[step: 32700] loss: 0.3007507622241974\n",
      "[step: 32750] loss: 0.30049607157707214\n",
      "[step: 32800] loss: 0.3002035915851593\n",
      "[step: 32850] loss: 0.29996612668037415\n",
      "[step: 32900] loss: 0.29971978068351746\n",
      "[step: 32950] loss: 0.2994679808616638\n",
      "[step: 33000] loss: 0.2992223799228668\n",
      "[step: 33050] loss: 0.29896804690361023\n",
      "[step: 33100] loss: 0.2987234890460968\n",
      "[step: 33150] loss: 0.29847657680511475\n",
      "[step: 33200] loss: 0.2982322871685028\n",
      "[step: 33250] loss: 0.2979838252067566\n",
      "[step: 33300] loss: 0.29773807525634766\n",
      "[step: 33350] loss: 0.29749301075935364\n",
      "[step: 33400] loss: 0.2972486615180969\n",
      "[step: 33450] loss: 0.2970011234283447\n",
      "[step: 33500] loss: 0.296755313873291\n",
      "[step: 33550] loss: 0.29651203751564026\n",
      "[step: 33600] loss: 0.29626351594924927\n",
      "[step: 33650] loss: 0.29601702094078064\n",
      "[step: 33700] loss: 0.29577088356018066\n",
      "[step: 33750] loss: 0.2955172657966614\n",
      "[step: 33800] loss: 0.2953193783760071\n",
      "[step: 33850] loss: 0.29498741030693054\n",
      "[step: 33900] loss: 0.294760137796402\n",
      "[step: 33950] loss: 0.2945767343044281\n",
      "[step: 34000] loss: 0.2942223846912384\n",
      "[step: 34050] loss: 0.29415321350097656\n",
      "[step: 34100] loss: 0.2937277853488922\n",
      "[step: 34150] loss: 0.2935611605644226\n",
      "[step: 34200] loss: 0.29340097308158875\n",
      "[step: 34250] loss: 0.29299843311309814\n",
      "[step: 34300] loss: 0.29280686378479004\n",
      "[step: 34350] loss: 0.2926788628101349\n",
      "[step: 34400] loss: 0.2923375368118286\n",
      "[step: 34450] loss: 0.2920137643814087\n",
      "[step: 34500] loss: 0.2919026017189026\n",
      "[step: 34550] loss: 0.29165714979171753\n",
      "[step: 34600] loss: 0.2912771999835968\n",
      "[step: 34650] loss: 0.29119884967803955\n",
      "[step: 34700] loss: 0.29080888628959656\n",
      "[step: 34750] loss: 0.29069939255714417\n",
      "[step: 34800] loss: 0.29035472869873047\n",
      "[step: 34850] loss: 0.29009440541267395\n",
      "[step: 34900] loss: 0.2898925244808197\n",
      "[step: 34950] loss: 0.28964751958847046\n",
      "[step: 35000] loss: 0.2893962264060974\n",
      "[step: 35050] loss: 0.2891455888748169\n",
      "[step: 35100] loss: 0.28890442848205566\n",
      "[step: 35150] loss: 0.2886653542518616\n",
      "[step: 35200] loss: 0.2884259819984436\n",
      "[step: 35250] loss: 0.28818804025650024\n",
      "[step: 35300] loss: 0.2879491150379181\n",
      "[step: 35350] loss: 0.28771495819091797\n",
      "[step: 35400] loss: 0.2874801754951477\n",
      "[step: 35450] loss: 0.2872428297996521\n",
      "[step: 35500] loss: 0.2870095670223236\n",
      "[step: 35550] loss: 0.2867772877216339\n",
      "[step: 35600] loss: 0.28654298186302185\n",
      "[step: 35650] loss: 0.28630903363227844\n",
      "[step: 35700] loss: 0.2860774099826813\n",
      "[step: 35750] loss: 0.28608474135398865\n",
      "[step: 35800] loss: 0.28581762313842773\n",
      "[step: 35850] loss: 0.2855987250804901\n",
      "[step: 35900] loss: 0.28534355759620667\n",
      "[step: 35950] loss: 0.28492671251296997\n",
      "[step: 36000] loss: 0.2847740054130554\n",
      "[step: 36050] loss: 0.28487372398376465\n",
      "[step: 36100] loss: 0.28419312834739685\n",
      "[step: 36150] loss: 0.2844127416610718\n",
      "[step: 36200] loss: 0.2840912342071533\n",
      "[step: 36250] loss: 0.28351253271102905\n",
      "[step: 36300] loss: 0.2833489775657654\n",
      "[step: 36350] loss: 0.28323352336883545\n",
      "[step: 36400] loss: 0.28302934765815735\n",
      "[step: 36450] loss: 0.2828826904296875\n",
      "[step: 36500] loss: 0.2827260494232178\n",
      "[step: 36550] loss: 0.28250622749328613\n",
      "[step: 36600] loss: 0.2822849452495575\n",
      "[step: 36650] loss: 0.2820744216442108\n",
      "[step: 36700] loss: 0.28186163306236267\n",
      "[step: 36750] loss: 0.28164833784103394\n",
      "[step: 36800] loss: 0.28143635392189026\n",
      "[step: 36850] loss: 0.28122684359550476\n",
      "[step: 36900] loss: 0.28100717067718506\n",
      "[step: 36950] loss: 0.2808019816875458\n",
      "[step: 37000] loss: 0.2805837094783783\n",
      "[step: 37050] loss: 0.2803768515586853\n",
      "[step: 37100] loss: 0.28016382455825806\n",
      "[step: 37150] loss: 0.27995240688323975\n",
      "[step: 37200] loss: 0.2797406017780304\n",
      "[step: 37250] loss: 0.2795296609401703\n",
      "[step: 37300] loss: 0.27932414412498474\n",
      "[step: 37350] loss: 0.27910855412483215\n",
      "[step: 37400] loss: 0.27889564633369446\n",
      "[step: 37450] loss: 0.2786880135536194\n",
      "[step: 37500] loss: 0.2784792482852936\n",
      "[step: 37550] loss: 0.2782716453075409\n",
      "[step: 37600] loss: 0.27805933356285095\n",
      "[step: 37650] loss: 0.27785196900367737\n",
      "[step: 37700] loss: 0.277642160654068\n",
      "[step: 37750] loss: 0.27743232250213623\n",
      "[step: 37800] loss: 0.27722322940826416\n",
      "[step: 37850] loss: 0.277018666267395\n",
      "[step: 37900] loss: 0.2768045663833618\n",
      "[step: 37950] loss: 0.27660712599754333\n",
      "[step: 38000] loss: 0.27640289068222046\n",
      "[step: 38050] loss: 0.27619385719299316\n",
      "[step: 38100] loss: 0.27598774433135986\n",
      "[step: 38150] loss: 0.27578142285346985\n",
      "[step: 38200] loss: 0.27557462453842163\n",
      "[step: 38250] loss: 0.2753700613975525\n",
      "[step: 38300] loss: 0.27516254782676697\n",
      "[step: 38350] loss: 0.27495941519737244\n",
      "[step: 38400] loss: 0.2747528553009033\n",
      "[step: 38450] loss: 0.27454882860183716\n",
      "[step: 38500] loss: 0.2743431329727173\n",
      "[step: 38550] loss: 0.2741377353668213\n",
      "[step: 38600] loss: 0.27393433451652527\n",
      "[step: 38650] loss: 0.27372869849205017\n",
      "[step: 38700] loss: 0.27352550625801086\n",
      "[step: 38750] loss: 0.27344760298728943\n",
      "[step: 38800] loss: 0.2730429172515869\n",
      "[step: 38850] loss: 0.27295976877212524\n",
      "[step: 38900] loss: 0.27262231707572937\n",
      "[step: 38950] loss: 0.272398442029953\n",
      "[step: 39000] loss: 0.27230480313301086\n",
      "[step: 39050] loss: 0.2721526026725769\n",
      "[step: 39100] loss: 0.27172914147377014\n",
      "[step: 39150] loss: 0.2716733515262604\n",
      "[step: 39200] loss: 0.27158063650131226\n",
      "[step: 39250] loss: 0.27100443840026855\n",
      "[step: 39300] loss: 0.27132871747016907\n",
      "[step: 39350] loss: 0.2705121338367462\n",
      "[step: 39400] loss: 0.2710348069667816\n",
      "[step: 39450] loss: 0.26997724175453186\n",
      "[step: 39500] loss: 0.27069368958473206\n",
      "[step: 39550] loss: 0.26988136768341064\n",
      "[step: 39600] loss: 0.2695053219795227\n",
      "[step: 39650] loss: 0.2704434096813202\n",
      "[step: 39700] loss: 0.26906123757362366\n",
      "[step: 39750] loss: 0.2686300277709961\n",
      "[step: 39800] loss: 0.2700349986553192\n",
      "[step: 39850] loss: 0.26906052231788635\n",
      "[step: 39900] loss: 0.2674996256828308\n",
      "[step: 39950] loss: 0.26904886960983276\n",
      "[step: 40000] loss: 0.2691608965396881\n",
      "[step: 40050] loss: 0.2668629586696625\n",
      "[step: 40100] loss: 0.26807329058647156\n",
      "[step: 40150] loss: 0.2687767446041107\n",
      "[step: 40200] loss: 0.2662048637866974\n",
      "[step: 40250] loss: 0.26779818534851074\n",
      "[step: 40300] loss: 0.2677994668483734\n",
      "[step: 40350] loss: 0.2656192481517792\n",
      "[step: 40400] loss: 0.26801493763923645\n",
      "[step: 40450] loss: 0.26574912667274475\n",
      "[step: 40500] loss: 0.2664729058742523\n",
      "[step: 40550] loss: 0.2667311728000641\n",
      "[step: 40600] loss: 0.26503264904022217\n",
      "[step: 40650] loss: 0.2670100927352905\n",
      "[step: 40700] loss: 0.2642531394958496\n",
      "[step: 40750] loss: 0.2667219936847687\n",
      "[step: 40800] loss: 0.26390203833580017\n",
      "[step: 40850] loss: 0.26621705293655396\n",
      "[step: 40900] loss: 0.26363417506217957\n",
      "[step: 40950] loss: 0.26572394371032715\n",
      "[step: 41000] loss: 0.2633156180381775\n",
      "[step: 41050] loss: 0.2653267979621887\n",
      "[step: 41100] loss: 0.2629258334636688\n",
      "[step: 41150] loss: 0.2650122046470642\n",
      "[step: 41200] loss: 0.26248717308044434\n",
      "[step: 41250] loss: 0.26473739743232727\n",
      "[step: 41300] loss: 0.2620646059513092\n",
      "[step: 41350] loss: 0.26445838809013367\n",
      "[step: 41400] loss: 0.261666476726532\n",
      "[step: 41450] loss: 0.2641535699367523\n",
      "[step: 41500] loss: 0.2612910270690918\n",
      "[step: 41550] loss: 0.2638303339481354\n",
      "[step: 41600] loss: 0.2609030306339264\n",
      "[step: 41650] loss: 0.26347431540489197\n",
      "[step: 41700] loss: 0.26048439741134644\n",
      "[step: 41750] loss: 0.263052374124527\n",
      "[step: 41800] loss: 0.2601012587547302\n",
      "[step: 41850] loss: 0.26239949464797974\n",
      "[step: 41900] loss: 0.2600786089897156\n",
      "[step: 41950] loss: 0.26131343841552734\n",
      "[step: 42000] loss: 0.2609032988548279\n",
      "[step: 42050] loss: 0.2599160373210907\n",
      "[step: 42100] loss: 0.26191994547843933\n",
      "[step: 42150] loss: 0.25856664776802063\n",
      "[step: 42200] loss: 0.260944664478302\n",
      "[step: 42250] loss: 0.2600714862346649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 42300] loss: 0.2584649622440338\n",
      "[step: 42350] loss: 0.26094329357147217\n",
      "[step: 42400] loss: 0.2591647207736969\n",
      "[step: 42450] loss: 0.25773516297340393\n",
      "[step: 42500] loss: 0.2600615620613098\n",
      "[step: 42550] loss: 0.2598579227924347\n",
      "[step: 42600] loss: 0.2568698525428772\n",
      "[step: 42650] loss: 0.2578006088733673\n",
      "[step: 42700] loss: 0.25955313444137573\n",
      "[step: 42750] loss: 0.2594529390335083\n",
      "[step: 42800] loss: 0.25719717144966125\n",
      "[step: 42850] loss: 0.25576353073120117\n",
      "[step: 42900] loss: 0.25675395131111145\n",
      "[step: 42950] loss: 0.25809434056282043\n",
      "[step: 43000] loss: 0.25871163606643677\n",
      "[step: 43050] loss: 0.2585775554180145\n",
      "[step: 43100] loss: 0.25775346159935\n",
      "[step: 43150] loss: 0.2565838694572449\n",
      "[step: 43200] loss: 0.25545307993888855\n",
      "[step: 43250] loss: 0.25460395216941833\n",
      "[step: 43300] loss: 0.25414371490478516\n",
      "[step: 43350] loss: 0.2540593445301056\n",
      "[step: 43400] loss: 0.25428301095962524\n",
      "[step: 43450] loss: 0.25473418831825256\n",
      "[step: 43500] loss: 0.25529050827026367\n",
      "[step: 43550] loss: 0.25582417845726013\n",
      "[step: 43600] loss: 0.2562602758407593\n",
      "[step: 43650] loss: 0.2565895915031433\n",
      "[step: 43700] loss: 0.25663864612579346\n",
      "[step: 43750] loss: 0.25590285658836365\n",
      "[step: 43800] loss: 0.253976047039032\n",
      "[step: 43850] loss: 0.2518600821495056\n",
      "[step: 43900] loss: 0.25250157713890076\n",
      "[step: 43950] loss: 0.2546822428703308\n",
      "[step: 44000] loss: 0.2559775114059448\n",
      "[step: 44050] loss: 0.2526269853115082\n",
      "[step: 44100] loss: 0.25136566162109375\n",
      "[step: 44150] loss: 0.2546479403972626\n",
      "[step: 44200] loss: 0.2538469135761261\n",
      "[step: 44250] loss: 0.2505694627761841\n",
      "[step: 44300] loss: 0.2544485330581665\n",
      "[step: 44350] loss: 0.25187593698501587\n",
      "[step: 44400] loss: 0.2509763538837433\n",
      "[step: 44450] loss: 0.25467702746391296\n",
      "[step: 44500] loss: 0.24941712617874146\n",
      "[step: 44550] loss: 0.25340262055397034\n",
      "[step: 44600] loss: 0.2506820261478424\n",
      "[step: 44650] loss: 0.25056833028793335\n",
      "[step: 44700] loss: 0.25338366627693176\n",
      "[step: 44750] loss: 0.24848175048828125\n",
      "[step: 44800] loss: 0.2534297704696655\n",
      "[step: 44850] loss: 0.24897225201129913\n",
      "[step: 44900] loss: 0.25008025765419006\n",
      "[step: 44950] loss: 0.25281986594200134\n",
      "[step: 45000] loss: 0.24713470041751862\n",
      "[step: 45050] loss: 0.25150102376937866\n",
      "[step: 45100] loss: 0.2512860894203186\n",
      "[step: 45150] loss: 0.24643805623054504\n",
      "[step: 45200] loss: 0.2507762908935547\n",
      "[step: 45250] loss: 0.2520187199115753\n",
      "[step: 45300] loss: 0.24658118188381195\n",
      "[step: 45350] loss: 0.24673528969287872\n",
      "[step: 45400] loss: 0.2508893311023712\n",
      "[step: 45450] loss: 0.2518559396266937\n",
      "[step: 45500] loss: 0.24892039597034454\n",
      "[step: 45550] loss: 0.24570463597774506\n",
      "[step: 45600] loss: 0.24462386965751648\n",
      "[step: 45650] loss: 0.245151087641716\n",
      "[step: 45700] loss: 0.2460038810968399\n",
      "[step: 45750] loss: 0.24643434584140778\n",
      "[step: 45800] loss: 0.2461976855993271\n",
      "[step: 45850] loss: 0.24534648656845093\n",
      "[step: 45900] loss: 0.24426798522472382\n",
      "[step: 45950] loss: 0.24399243295192719\n",
      "[step: 46000] loss: 0.2456027716398239\n",
      "[step: 46050] loss: 0.24838145077228546\n",
      "[step: 46100] loss: 0.24914830923080444\n",
      "[step: 46150] loss: 0.24506331980228424\n",
      "[step: 46200] loss: 0.24415504932403564\n",
      "[step: 46250] loss: 0.248305082321167\n",
      "[step: 46300] loss: 0.24524877965450287\n",
      "[step: 46350] loss: 0.24464736878871918\n",
      "[step: 46400] loss: 0.24764315783977509\n",
      "[step: 46450] loss: 0.2432219386100769\n",
      "[step: 46500] loss: 0.24750828742980957\n",
      "[step: 46550] loss: 0.24320140480995178\n",
      "[step: 46600] loss: 0.24628190696239471\n",
      "[step: 46650] loss: 0.2447964996099472\n",
      "[step: 46700] loss: 0.24275419116020203\n",
      "[step: 46750] loss: 0.2464652806520462\n",
      "[step: 46800] loss: 0.24445730447769165\n",
      "[step: 46850] loss: 0.24211063981056213\n",
      "[step: 46900] loss: 0.2434212863445282\n",
      "[step: 46950] loss: 0.24551093578338623\n",
      "[step: 47000] loss: 0.24528340995311737\n",
      "[step: 47050] loss: 0.24406519532203674\n",
      "[step: 47100] loss: 0.24307340383529663\n",
      "[step: 47150] loss: 0.24241237342357635\n",
      "[step: 47200] loss: 0.24199676513671875\n",
      "[step: 47250] loss: 0.24188870191574097\n",
      "[step: 47300] loss: 0.2421310991048813\n",
      "[step: 47350] loss: 0.24293875694274902\n",
      "[step: 47400] loss: 0.24368952214717865\n",
      "[step: 47450] loss: 0.24381089210510254\n",
      "[step: 47500] loss: 0.24167867004871368\n",
      "[step: 47550] loss: 0.24030697345733643\n",
      "[step: 47600] loss: 0.24184875190258026\n",
      "[step: 47650] loss: 0.2431269735097885\n",
      "[step: 47700] loss: 0.23995983600616455\n",
      "[step: 47750] loss: 0.24143870174884796\n",
      "[step: 47800] loss: 0.241637721657753\n",
      "[step: 47850] loss: 0.24007809162139893\n",
      "[step: 47900] loss: 0.24194051325321198\n",
      "[step: 47950] loss: 0.23978614807128906\n",
      "[step: 48000] loss: 0.24093696475028992\n",
      "[step: 48050] loss: 0.24053449928760529\n",
      "[step: 48100] loss: 0.23885920643806458\n",
      "[step: 48150] loss: 0.24135838449001312\n",
      "[step: 48200] loss: 0.23979982733726501\n",
      "[step: 48250] loss: 0.2385922521352768\n",
      "[step: 48300] loss: 0.23937936127185822\n",
      "[step: 48350] loss: 0.24047991633415222\n",
      "[step: 48400] loss: 0.24010376632213593\n",
      "[step: 48450] loss: 0.23902468383312225\n",
      "[step: 48500] loss: 0.23795248568058014\n",
      "[step: 48550] loss: 0.2376541942358017\n",
      "[step: 48600] loss: 0.2371165007352829\n",
      "[step: 48650] loss: 0.23727117478847504\n",
      "[step: 48700] loss: 0.23734167218208313\n",
      "[step: 48750] loss: 0.23716096580028534\n",
      "[step: 48800] loss: 0.23677338659763336\n",
      "[step: 48850] loss: 0.23649153113365173\n",
      "[step: 48900] loss: 0.23679275810718536\n",
      "[step: 48950] loss: 0.2369174063205719\n",
      "[step: 49000] loss: 0.2377692312002182\n",
      "[step: 49050] loss: 0.2381170094013214\n",
      "[step: 49100] loss: 0.2377156764268875\n",
      "[step: 49150] loss: 0.23580504953861237\n",
      "[step: 49200] loss: 0.23616497218608856\n",
      "[step: 49250] loss: 0.23746038973331451\n",
      "[step: 49300] loss: 0.23669959604740143\n",
      "[step: 49350] loss: 0.235091894865036\n",
      "[step: 49400] loss: 0.23590609431266785\n",
      "[step: 49450] loss: 0.2369602620601654\n",
      "[step: 49500] loss: 0.23478345572948456\n",
      "[step: 49550] loss: 0.23660005629062653\n",
      "[step: 49600] loss: 0.23465722799301147\n",
      "[step: 49650] loss: 0.23563352227210999\n",
      "[step: 49700] loss: 0.235593780875206\n",
      "[step: 49750] loss: 0.2345304638147354\n",
      "[step: 49800] loss: 0.2337683141231537\n",
      "[step: 49850] loss: 0.23409229516983032\n",
      "[step: 49900] loss: 0.23403197526931763\n",
      "[step: 49950] loss: 0.233756884932518\n",
      "[step: 50000] loss: 0.23352116346359253\n"
     ]
    }
   ],
   "source": [
    "for i in range(iterations):\n",
    "    _  , cost = sess.run([train ,loss], feed_dict={X: trainX, Y: trainY})\n",
    "    if (i+1) % (iterations/1000) == 0:\n",
    "        print(\"[step: {}] loss: {}\".format(i+1, cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7302831276270303\n"
     ]
    }
   ],
   "source": [
    "a=0\n",
    "length=len(dataY)\n",
    "for k in range(0,length-1):\n",
    "    a+=dataY[k][0]\n",
    "real_avg=a/length\n",
    "\n",
    "test_predict = sess.run(Y_pred, feed_dict={X: testX}) #예측값\n",
    "length_=len(test_predict)\n",
    "\n",
    "b=0\n",
    "for j in range(0,length_-1):\n",
    "    b+=test_predict[j][0]\n",
    "pred_avg=b/length_\n",
    "    \n",
    "#(절대값(실제값의 평균 - 예측값의 평균)/실제값의 평균) *100 =평균 오차율 산정 방식\n",
    "accuracy=(abs(real_avg-pred_avg)/real_avg)*100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#예측값 불러오기\n",
    "train_predict = sess.run(Y_pred, feed_dict={X: trainX})\n",
    "test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "\n",
    "plt.plot(testY,'r')\n",
    "plt.plot(test_predict,'b')\n",
    "plt.xlabel(\"day\")\n",
    "plt.ylabel(\"elec\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2713458 ],\n",
       "       [0.27026406],\n",
       "       [0.27594167],\n",
       "       [0.2679586 ],\n",
       "       [0.26539028],\n",
       "       [0.2715481 ],\n",
       "       [0.26637578],\n",
       "       [0.26330405],\n",
       "       [0.2661841 ],\n",
       "       [0.26548666],\n",
       "       [0.26436377],\n",
       "       [0.2652558 ],\n",
       "       [0.263143  ],\n",
       "       [0.26581755],\n",
       "       [0.26405776],\n",
       "       [0.26716524],\n",
       "       [0.26776904],\n",
       "       [0.27095568],\n",
       "       [0.26768988],\n",
       "       [0.28909367],\n",
       "       [0.382987  ],\n",
       "       [0.38561165],\n",
       "       [0.41932115],\n",
       "       [0.47380987],\n",
       "       [0.49781114],\n",
       "       [0.57950103],\n",
       "       [0.7307552 ],\n",
       "       [0.7629676 ],\n",
       "       [0.7548609 ],\n",
       "       [0.7675667 ],\n",
       "       [0.75220597],\n",
       "       [0.7799895 ],\n",
       "       [0.8028579 ],\n",
       "       [0.81974864],\n",
       "       [0.84625053],\n",
       "       [0.8401332 ],\n",
       "       [0.8469238 ],\n",
       "       [0.8360543 ],\n",
       "       [0.8194654 ],\n",
       "       [0.82581484],\n",
       "       [0.812163  ],\n",
       "       [0.7813771 ],\n",
       "       [0.7631488 ],\n",
       "       [0.71227443],\n",
       "       [0.7230067 ],\n",
       "       [0.71327627],\n",
       "       [0.7284945 ],\n",
       "       [0.6907191 ],\n",
       "       [0.6580478 ],\n",
       "       [0.6373874 ],\n",
       "       [0.67144763],\n",
       "       [0.7251451 ],\n",
       "       [0.77354574],\n",
       "       [0.75278294],\n",
       "       [0.75499606],\n",
       "       [0.73685634],\n",
       "       [0.7367971 ],\n",
       "       [0.7600049 ],\n",
       "       [0.7872342 ],\n",
       "       [0.7336478 ],\n",
       "       [0.72718763],\n",
       "       [0.7338451 ],\n",
       "       [0.7150631 ],\n",
       "       [0.7159239 ],\n",
       "       [0.73673356],\n",
       "       [0.74150515],\n",
       "       [0.7487569 ],\n",
       "       [0.7849115 ],\n",
       "       [0.7743908 ],\n",
       "       [0.80038047],\n",
       "       [0.8070644 ],\n",
       "       [0.74404716],\n",
       "       [0.5619898 ],\n",
       "       [0.5201602 ],\n",
       "       [0.5040818 ],\n",
       "       [0.4652758 ],\n",
       "       [0.4275361 ],\n",
       "       [0.40951315],\n",
       "       [0.39795613],\n",
       "       [0.40689185],\n",
       "       [0.38166797],\n",
       "       [0.3794739 ],\n",
       "       [0.3756859 ],\n",
       "       [0.37311098],\n",
       "       [0.36393222],\n",
       "       [0.35126328],\n",
       "       [0.32047078],\n",
       "       [0.3074231 ],\n",
       "       [0.28639188],\n",
       "       [0.28236696],\n",
       "       [0.27770507],\n",
       "       [0.28381968],\n",
       "       [0.28065342],\n",
       "       [0.28028214],\n",
       "       [0.27524242],\n",
       "       [0.27468538],\n",
       "       [0.27085888],\n",
       "       [0.31032908],\n",
       "       [0.30603495],\n",
       "       [0.29225218],\n",
       "       [0.29848474],\n",
       "       [0.2908937 ],\n",
       "       [0.28983596],\n",
       "       [0.2907162 ],\n",
       "       [0.28875756],\n",
       "       [0.29095736],\n",
       "       [0.29794303],\n",
       "       [0.28955263],\n",
       "       [0.28669065],\n",
       "       [0.2929461 ],\n",
       "       [0.29049405],\n",
       "       [0.2928559 ],\n",
       "       [0.29417157],\n",
       "       [0.28789166],\n",
       "       [0.29005465],\n",
       "       [0.28914744],\n",
       "       [0.2953488 ],\n",
       "       [0.2934537 ],\n",
       "       [0.28614655],\n",
       "       [0.29702145],\n",
       "       [0.2960452 ],\n",
       "       [0.29329276],\n",
       "       [0.29777253],\n",
       "       [0.29115832],\n",
       "       [0.2988535 ],\n",
       "       [0.30123913],\n",
       "       [0.29648286],\n",
       "       [0.30508655],\n",
       "       [0.30096033],\n",
       "       [0.29710376],\n",
       "       [0.30013514],\n",
       "       [0.2826623 ],\n",
       "       [0.27169344],\n",
       "       [0.25849134],\n",
       "       [0.23363033],\n",
       "       [0.21933475],\n",
       "       [0.2078767 ],\n",
       "       [0.19398493],\n",
       "       [0.17517391],\n",
       "       [0.16506955],\n",
       "       [0.14866012],\n",
       "       [0.1465579 ],\n",
       "       [0.13015503],\n",
       "       [0.11109062],\n",
       "       [0.10592026],\n",
       "       [0.1026193 ],\n",
       "       [0.0971441 ],\n",
       "       [0.10258225],\n",
       "       [0.10300309],\n",
       "       [0.1011955 ],\n",
       "       [0.12023525],\n",
       "       [0.12028972],\n",
       "       [0.12905334],\n",
       "       [0.14156643],\n",
       "       [0.1568816 ],\n",
       "       [0.17348084],\n",
       "       [0.1884304 ],\n",
       "       [0.20867798],\n",
       "       [0.21864416],\n",
       "       [0.28415823],\n",
       "       [0.38417283],\n",
       "       [0.38248363],\n",
       "       [0.43011466],\n",
       "       [0.4426359 ],\n",
       "       [0.45244125],\n",
       "       [0.4638616 ],\n",
       "       [0.4712359 ],\n",
       "       [0.46472007],\n",
       "       [0.471048  ],\n",
       "       [0.4566396 ],\n",
       "       [0.46178222],\n",
       "       [0.4590868 ],\n",
       "       [0.46907663],\n",
       "       [0.45670918],\n",
       "       [0.44632894],\n",
       "       [0.43516865],\n",
       "       [0.44149435],\n",
       "       [0.4542203 ],\n",
       "       [0.455493  ],\n",
       "       [0.4421568 ],\n",
       "       [0.44328538],\n",
       "       [0.43928075],\n",
       "       [0.4492685 ],\n",
       "       [0.45267206],\n",
       "       [0.43589517],\n",
       "       [0.44524541],\n",
       "       [0.42930028],\n",
       "       [0.42155015],\n",
       "       [0.33576807],\n",
       "       [0.29027376],\n",
       "       [0.28378457],\n",
       "       [0.27385345],\n",
       "       [0.2774456 ],\n",
       "       [0.28012127],\n",
       "       [0.27456057],\n",
       "       [0.27336445],\n",
       "       [0.2753169 ],\n",
       "       [0.2797051 ],\n",
       "       [0.2788779 ],\n",
       "       [0.27054036],\n",
       "       [0.27485117],\n",
       "       [0.27945578],\n",
       "       [0.2771994 ],\n",
       "       [0.27412456],\n",
       "       [0.27730927],\n",
       "       [0.27834287],\n",
       "       [0.28013694],\n",
       "       [0.2788458 ],\n",
       "       [0.27280605],\n",
       "       [0.27279568],\n",
       "       [0.27397057],\n",
       "       [0.27660385],\n",
       "       [0.28566942],\n",
       "       [0.272499  ],\n",
       "       [0.27771133],\n",
       "       [0.27827805],\n",
       "       [0.28431252],\n",
       "       [0.2791531 ],\n",
       "       [0.276699  ],\n",
       "       [0.28285527],\n",
       "       [0.2857741 ],\n",
       "       [0.2837911 ],\n",
       "       [0.28825885],\n",
       "       [0.2932008 ],\n",
       "       [0.27964008],\n",
       "       [0.28630912],\n",
       "       [0.27688324],\n",
       "       [0.2665077 ],\n",
       "       [0.2447877 ],\n",
       "       [0.21681094],\n",
       "       [0.19516085],\n",
       "       [0.17155387],\n",
       "       [0.16222587],\n",
       "       [0.15510231],\n",
       "       [0.14465046],\n",
       "       [0.11672848],\n",
       "       [0.11645164],\n",
       "       [0.10806496],\n",
       "       [0.09316166],\n",
       "       [0.08538891],\n",
       "       [0.07834916],\n",
       "       [0.07508133],\n",
       "       [0.07556099],\n",
       "       [0.08111342],\n",
       "       [0.07479303],\n",
       "       [0.08657408],\n",
       "       [0.08881932],\n",
       "       [0.08786177],\n",
       "       [0.08431218],\n",
       "       [0.08365356],\n",
       "       [0.0909079 ],\n",
       "       [0.10577583],\n",
       "       [0.12716277],\n",
       "       [0.14892316],\n",
       "       [0.16543518],\n",
       "       [0.19045295],\n",
       "       [0.20781782],\n",
       "       [0.22343189],\n",
       "       [0.25540888],\n",
       "       [0.29048705],\n",
       "       [0.28740832],\n",
       "       [0.29838896],\n",
       "       [0.31389758],\n",
       "       [0.32584998],\n",
       "       [0.3442397 ],\n",
       "       [0.34546435],\n",
       "       [0.34097737],\n",
       "       [0.33873484],\n",
       "       [0.337564  ],\n",
       "       [0.33759496],\n",
       "       [0.32991946],\n",
       "       [0.31879112],\n",
       "       [0.31698048],\n",
       "       [0.3164792 ],\n",
       "       [0.31834418],\n",
       "       [0.30266878],\n",
       "       [0.30300677],\n",
       "       [0.3019198 ],\n",
       "       [0.30477956],\n",
       "       [0.30804628],\n",
       "       [0.2994266 ],\n",
       "       [0.29634202],\n",
       "       [0.30157384],\n",
       "       [0.30060032],\n",
       "       [0.32014814],\n",
       "       [0.34084615],\n",
       "       [0.33993724],\n",
       "       [0.33491322],\n",
       "       [0.33418435],\n",
       "       [0.34996095],\n",
       "       [0.32174242],\n",
       "       [0.33100942],\n",
       "       [0.32460892],\n",
       "       [0.32545254],\n",
       "       [0.31809157],\n",
       "       [0.33222482],\n",
       "       [0.32170728],\n",
       "       [0.3103171 ],\n",
       "       [0.32768974],\n",
       "       [0.324304  ],\n",
       "       [0.30738148],\n",
       "       [0.3165017 ],\n",
       "       [0.30445543],\n",
       "       [0.31132773],\n",
       "       [0.31191182],\n",
       "       [0.31680036],\n",
       "       [0.3052588 ],\n",
       "       [0.31147704],\n",
       "       [0.34920797],\n",
       "       [0.39383224],\n",
       "       [0.47446996],\n",
       "       [0.6444442 ],\n",
       "       [0.6828222 ],\n",
       "       [0.7155949 ],\n",
       "       [0.735085  ],\n",
       "       [0.75216913],\n",
       "       [0.7606343 ],\n",
       "       [0.77351   ],\n",
       "       [0.7771747 ],\n",
       "       [0.83696127],\n",
       "       [0.8462062 ],\n",
       "       [0.8550656 ],\n",
       "       [0.86931825],\n",
       "       [0.8923882 ],\n",
       "       [0.8991207 ],\n",
       "       [0.85208905],\n",
       "       [0.84277606],\n",
       "       [0.84377193],\n",
       "       [0.855417  ],\n",
       "       [0.86483455],\n",
       "       [0.8075998 ],\n",
       "       [0.7150816 ],\n",
       "       [0.7114607 ],\n",
       "       [0.6813283 ],\n",
       "       [0.6690111 ],\n",
       "       [0.63230145],\n",
       "       [0.6286794 ],\n",
       "       [0.67150295],\n",
       "       [0.65740705],\n",
       "       [0.6490483 ],\n",
       "       [0.7053443 ],\n",
       "       [0.74508405],\n",
       "       [0.66704285],\n",
       "       [0.66118944],\n",
       "       [0.66403973],\n",
       "       [0.68787766],\n",
       "       [0.6890956 ],\n",
       "       [0.6878201 ],\n",
       "       [0.71386695],\n",
       "       [0.7524756 ],\n",
       "       [0.74812853],\n",
       "       [0.74223244],\n",
       "       [0.75332916],\n",
       "       [0.7564713 ],\n",
       "       [0.78091455],\n",
       "       [0.77645314],\n",
       "       [0.7864796 ],\n",
       "       [0.7255846 ],\n",
       "       [0.6750871 ],\n",
       "       [0.6572548 ],\n",
       "       [0.6359998 ],\n",
       "       [0.5997137 ],\n",
       "       [0.5912409 ],\n",
       "       [0.56382024],\n",
       "       [0.5615796 ],\n",
       "       [0.5434499 ],\n",
       "       [0.49021918],\n",
       "       [0.47342134],\n",
       "       [0.43692634],\n",
       "       [0.4358932 ],\n",
       "       [0.41722044],\n",
       "       [0.40878162],\n",
       "       [0.3898412 ],\n",
       "       [0.39148363],\n",
       "       [0.418207  ],\n",
       "       [0.4018246 ],\n",
       "       [0.39704782],\n",
       "       [0.38020268],\n",
       "       [0.3864197 ],\n",
       "       [0.3675894 ],\n",
       "       [0.36571574],\n",
       "       [0.36181608],\n",
       "       [0.33536014],\n",
       "       [0.34637213],\n",
       "       [0.30787033],\n",
       "       [0.2994355 ],\n",
       "       [0.29348004],\n",
       "       [0.28751284],\n",
       "       [0.29205546],\n",
       "       [0.29574656],\n",
       "       [0.28629848],\n",
       "       [0.29193237],\n",
       "       [0.29829788],\n",
       "       [0.301862  ],\n",
       "       [0.29878265],\n",
       "       [0.29499713],\n",
       "       [0.30988258],\n",
       "       [0.29492757],\n",
       "       [0.29485595],\n",
       "       [0.29842067],\n",
       "       [0.29524714],\n",
       "       [0.3034894 ],\n",
       "       [0.29479492],\n",
       "       [0.30745405],\n",
       "       [0.3536844 ],\n",
       "       [0.40163985],\n",
       "       [0.5812098 ],\n",
       "       [0.70876646],\n",
       "       [0.7150887 ],\n",
       "       [0.7426262 ],\n",
       "       [0.75428426],\n",
       "       [0.7554544 ],\n",
       "       [0.7816386 ],\n",
       "       [0.7825564 ],\n",
       "       [0.79252994],\n",
       "       [0.7890378 ],\n",
       "       [0.81855154],\n",
       "       [0.82892585],\n",
       "       [0.84349525],\n",
       "       [0.8411075 ],\n",
       "       [0.8324654 ],\n",
       "       [0.80488455],\n",
       "       [0.7647742 ],\n",
       "       [0.7800484 ],\n",
       "       [0.72374225],\n",
       "       [0.71299875],\n",
       "       [0.7073468 ],\n",
       "       [0.7141657 ],\n",
       "       [0.6816374 ],\n",
       "       [0.6780791 ],\n",
       "       [0.6583767 ],\n",
       "       [0.6798792 ],\n",
       "       [0.58386266],\n",
       "       [0.58805144],\n",
       "       [0.6040292 ],\n",
       "       [0.6085539 ],\n",
       "       [0.6388134 ],\n",
       "       [0.64504635],\n",
       "       [0.6556711 ],\n",
       "       [0.66865623],\n",
       "       [0.6703267 ],\n",
       "       [0.68736756],\n",
       "       [0.6761465 ],\n",
       "       [0.69597864],\n",
       "       [0.6743915 ],\n",
       "       [0.69340205],\n",
       "       [0.7021078 ],\n",
       "       [0.7255546 ],\n",
       "       [0.7321249 ],\n",
       "       [0.76381755],\n",
       "       [0.7825724 ],\n",
       "       [0.8128065 ],\n",
       "       [0.8091167 ],\n",
       "       [0.8211757 ],\n",
       "       [0.8385712 ],\n",
       "       [0.8457072 ],\n",
       "       [0.82519543],\n",
       "       [0.6201211 ],\n",
       "       [0.57329154],\n",
       "       [0.53742325],\n",
       "       [0.51228106],\n",
       "       [0.47819486],\n",
       "       [0.4669919 ],\n",
       "       [0.4557604 ],\n",
       "       [0.42683467],\n",
       "       [0.40453294],\n",
       "       [0.40774196],\n",
       "       [0.39863047],\n",
       "       [0.39500389],\n",
       "       [0.36251354],\n",
       "       [0.36278278],\n",
       "       [0.35034904],\n",
       "       [0.34323788],\n",
       "       [0.34464905],\n",
       "       [0.34288958],\n",
       "       [0.34188282],\n",
       "       [0.3545294 ],\n",
       "       [0.3392144 ],\n",
       "       [0.33233282],\n",
       "       [0.3298555 ],\n",
       "       [0.32048136],\n",
       "       [0.30068693],\n",
       "       [0.2967692 ],\n",
       "       [0.2880838 ],\n",
       "       [0.290672  ],\n",
       "       [0.29348975],\n",
       "       [0.28747296],\n",
       "       [0.28156424],\n",
       "       [0.2841456 ],\n",
       "       [0.2918783 ],\n",
       "       [0.2967013 ],\n",
       "       [0.28091252],\n",
       "       [0.2861924 ],\n",
       "       [0.29244348],\n",
       "       [0.28372905],\n",
       "       [0.28275925],\n",
       "       [0.28962147],\n",
       "       [0.29308093],\n",
       "       [0.2925516 ],\n",
       "       [0.29037002],\n",
       "       [0.33849987],\n",
       "       [0.4779622 ],\n",
       "       [0.5714228 ],\n",
       "       [0.69399416],\n",
       "       [0.74368346],\n",
       "       [0.714919  ],\n",
       "       [0.72107613],\n",
       "       [0.7412456 ],\n",
       "       [0.7476177 ],\n",
       "       [0.75632894],\n",
       "       [0.7686322 ],\n",
       "       [0.7907039 ],\n",
       "       [0.80546737],\n",
       "       [0.841691  ],\n",
       "       [0.8617698 ],\n",
       "       [0.8374529 ],\n",
       "       [0.8189193 ],\n",
       "       [0.87461436],\n",
       "       [0.8911127 ],\n",
       "       [0.91325927],\n",
       "       [0.94022787],\n",
       "       [0.95222485],\n",
       "       [0.93263006],\n",
       "       [0.9283314 ],\n",
       "       [0.8978108 ],\n",
       "       [0.8772578 ],\n",
       "       [0.8685378 ],\n",
       "       [0.81720185],\n",
       "       [0.757591  ],\n",
       "       [0.7599429 ],\n",
       "       [0.7304665 ],\n",
       "       [0.7559233 ],\n",
       "       [0.7670938 ],\n",
       "       [0.7967367 ],\n",
       "       [0.82897615],\n",
       "       [0.85602903],\n",
       "       [0.84278333],\n",
       "       [0.86936724],\n",
       "       [0.8746067 ],\n",
       "       [0.87272286],\n",
       "       [0.88491666],\n",
       "       [0.8806354 ],\n",
       "       [0.8659055 ],\n",
       "       [0.8584076 ],\n",
       "       [0.83286154],\n",
       "       [0.8603997 ],\n",
       "       [0.8695655 ],\n",
       "       [0.8892133 ],\n",
       "       [0.90802467],\n",
       "       [0.92263687],\n",
       "       [0.8343805 ],\n",
       "       [0.6334535 ],\n",
       "       [0.57383   ],\n",
       "       [0.483978  ],\n",
       "       [0.46578208],\n",
       "       [0.44150046],\n",
       "       [0.4366803 ],\n",
       "       [0.42160186],\n",
       "       [0.40679532],\n",
       "       [0.4132141 ],\n",
       "       [0.3867311 ],\n",
       "       [0.37885487],\n",
       "       [0.37350205],\n",
       "       [0.38458908],\n",
       "       [0.3743029 ],\n",
       "       [0.35917416],\n",
       "       [0.35419148],\n",
       "       [0.3476926 ],\n",
       "       [0.3378264 ],\n",
       "       [0.4525369 ],\n",
       "       [0.4900741 ],\n",
       "       [0.46764392],\n",
       "       [0.3797558 ],\n",
       "       [0.3258597 ],\n",
       "       [0.3141646 ],\n",
       "       [0.303687  ]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step4(전력,온도,optim=RMSPropOptimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmlee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\jmlee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\jmlee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\jmlee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\jmlee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\jmlee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#http://mjgim.me/2017/08/02/LSTM.html\n",
    "\n",
    "#라이브러리 불러오기\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "tf.set_random_seed(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 불러오기\n",
    "jan = pd.read_csv(r\"C:/Users/jmlee/Desktop/4학년 2학기/딥러닝 전력 예측/전력 예측/생성data/1월.csv\", header=None)\n",
    "july = pd.read_csv(r\"C:/Users/jmlee/Desktop/4학년 2학기/딥러닝 전력 예측/전력 예측/생성data/7월.csv\", header=None)\n",
    "jan_=jan.loc[:,[1,2,4,6]][1:]\n",
    "# july_=july.loc[:,[1,2,4]][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All=pd.concat([jan_,july_])\n",
    "#str을 float로 바꾸기\n",
    "jan_.columns=['월','일','전력량','온도']\n",
    "jan_[['월','일','전력량','온도']]=jan_[['월','일','전력량','온도']].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinMaxScaler(data):\n",
    "    numerator=data-np.min(data,0)\n",
    "    denominator=np.max(data,0)-np.min(data,0)\n",
    "    return numerator/(denominator+1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#하이퍼 파라미터 설정\n",
    "timesteps = seq_length = 3  #한개의 시퀀스길이(시계열데이터 입력갯수)\n",
    "data_dim = 4  #variable개수\n",
    "hidden_dim = 3  #각셀의 출력크기\n",
    "output_dim = 1  #결과분류 총 수\n",
    "learing_rate = 0.001   #학습률\n",
    "iterations = 50_000   #에폭횟수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #데이터 조절\n",
    "# jan_[\"전력량\"] /= 1e5\n",
    "\n",
    "#Framework 제작\n",
    "jan_['전력량']=MinMaxScaler(jan_['전력량'])\n",
    "x = jan_[['월','일','전력량','온도']].values\n",
    "y = jan_[\"전력량\"].values  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(y) - seq_length):  #seq_length=timesteps=\n",
    "    _x = np.copy(x[i:i + seq_length + 1])\n",
    "#     _x[timesteps-2][data_dim-1] = 0\n",
    "#     _x[timesteps-1][data_dim-1] = 0\n",
    "#     _x[timesteps][data_dim-1] = 0\n",
    "    _y = [y[i + seq_length]] #다음 전력(정답)\n",
    "    dataX.append(_x)\n",
    "    dataY.append(_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습데이터와 테스트데이터 분류\n",
    "\n",
    "train_size = int(len(dataY) * 0.8)\n",
    "test_size = len(dataY) - train_size \n",
    "\n",
    "trainX = np.array(dataX[:train_size])  #2298\n",
    "testX = np.array(dataX[train_size : ])\n",
    "\n",
    "trainY = np.array(dataY[:train_size])  #575\n",
    "testY = np.array(dataY[train_size : ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-b7c77ad44290>:7: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-9-b7c77ad44290>:10: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    }
   ],
   "source": [
    "#LSTM모델 구축\n",
    "#placeholder 초기화 후 텐서에 매핑\n",
    "X = tf.placeholder(tf.float32, [None, seq_length+1, data_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "def lstm_cell(): #cell생성\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(hidden_dim, reuse=tf.AUTO_REUSE)  #출력의 크기를 hidden dim=4\n",
    "    return cell \n",
    "## 5 layers for hidden layer\n",
    "multi_cells = rnn.MultiRNNCell([lstm_cell() for _ in range(5)], state_is_tuple=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-832d8dcf8217>:2: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\jmlee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# dynamic rnn 구조(RNN 신경망)연결\n",
    "outputs, _states = tf.nn.dynamic_rnn(multi_cells, X, dtype=tf.float32) #결과값 output차원은 hidden_dim의 크기와 동일\n",
    "\n",
    "Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=None)\n",
    "\n",
    "# sum of the squares= 표준편차(예측값과 실제값의차이)\n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))  \n",
    "\n",
    "#RMSprop 알고리즘\n",
    "train = tf.train.RMSPropOptimizer(learing_rate).minimize(loss)\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 50] loss: 197.8739471435547\n",
      "[step: 100] loss: 122.07440948486328\n",
      "[step: 150] loss: 120.85841369628906\n",
      "[step: 200] loss: 116.11582946777344\n",
      "[step: 250] loss: 99.98678588867188\n",
      "[step: 300] loss: 71.45633697509766\n",
      "[step: 350] loss: 34.411285400390625\n",
      "[step: 400] loss: 15.867313385009766\n",
      "[step: 450] loss: 12.30759334564209\n",
      "[step: 500] loss: 11.361457824707031\n",
      "[step: 550] loss: 10.916762351989746\n",
      "[step: 600] loss: 10.656579971313477\n",
      "[step: 650] loss: 10.480658531188965\n",
      "[step: 700] loss: 10.348348617553711\n",
      "[step: 750] loss: 10.2410888671875\n",
      "[step: 800] loss: 10.149553298950195\n",
      "[step: 850] loss: 10.068568229675293\n",
      "[step: 900] loss: 9.995216369628906\n",
      "[step: 950] loss: 9.927572250366211\n",
      "[step: 1000] loss: 9.864360809326172\n",
      "[step: 1050] loss: 9.804794311523438\n",
      "[step: 1100] loss: 9.748198509216309\n",
      "[step: 1150] loss: 9.694138526916504\n",
      "[step: 1200] loss: 9.642239570617676\n",
      "[step: 1250] loss: 9.592236518859863\n",
      "[step: 1300] loss: 9.543938636779785\n",
      "[step: 1350] loss: 9.497119903564453\n",
      "[step: 1400] loss: 9.451650619506836\n",
      "[step: 1450] loss: 9.40739917755127\n",
      "[step: 1500] loss: 9.364274978637695\n",
      "[step: 1550] loss: 9.322131156921387\n",
      "[step: 1600] loss: 9.28090763092041\n",
      "[step: 1650] loss: 9.240530967712402\n",
      "[step: 1700] loss: 9.200960159301758\n",
      "[step: 1750] loss: 9.162023544311523\n",
      "[step: 1800] loss: 9.123794555664062\n",
      "[step: 1850] loss: 9.086124420166016\n",
      "[step: 1900] loss: 9.049017906188965\n",
      "[step: 1950] loss: 9.012384414672852\n",
      "[step: 2000] loss: 8.976191520690918\n",
      "[step: 2050] loss: 8.94046401977539\n",
      "[step: 2100] loss: 8.905052185058594\n",
      "[step: 2150] loss: 8.869989395141602\n",
      "[step: 2200] loss: 8.835249900817871\n",
      "[step: 2250] loss: 8.800798416137695\n",
      "[step: 2300] loss: 8.766597747802734\n",
      "[step: 2350] loss: 8.732593536376953\n",
      "[step: 2400] loss: 8.698813438415527\n",
      "[step: 2450] loss: 8.66524887084961\n",
      "[step: 2500] loss: 8.63180160522461\n",
      "[step: 2550] loss: 8.59852409362793\n",
      "[step: 2600] loss: 8.565369606018066\n",
      "[step: 2650] loss: 8.532316207885742\n",
      "[step: 2700] loss: 8.499382972717285\n",
      "[step: 2750] loss: 8.466523170471191\n",
      "[step: 2800] loss: 8.433757781982422\n",
      "[step: 2850] loss: 8.401037216186523\n",
      "[step: 2900] loss: 8.368390083312988\n",
      "[step: 2950] loss: 8.335775375366211\n",
      "[step: 3000] loss: 8.303174018859863\n",
      "[step: 3050] loss: 8.27061653137207\n",
      "[step: 3100] loss: 8.238088607788086\n",
      "[step: 3150] loss: 8.205554962158203\n",
      "[step: 3200] loss: 8.173030853271484\n",
      "[step: 3250] loss: 8.140498161315918\n",
      "[step: 3300] loss: 8.107958793640137\n",
      "[step: 3350] loss: 8.075377464294434\n",
      "[step: 3400] loss: 8.042794227600098\n",
      "[step: 3450] loss: 8.010171890258789\n",
      "[step: 3500] loss: 7.977481842041016\n",
      "[step: 3550] loss: 7.9447340965271\n",
      "[step: 3600] loss: 7.9119415283203125\n",
      "[step: 3650] loss: 7.879065990447998\n",
      "[step: 3700] loss: 7.846106052398682\n",
      "[step: 3750] loss: 7.813052654266357\n",
      "[step: 3800] loss: 7.779876708984375\n",
      "[step: 3850] loss: 7.746594429016113\n",
      "[step: 3900] loss: 7.713161945343018\n",
      "[step: 3950] loss: 7.679568767547607\n",
      "[step: 4000] loss: 7.645801544189453\n",
      "[step: 4050] loss: 7.611814022064209\n",
      "[step: 4100] loss: 7.577605724334717\n",
      "[step: 4150] loss: 7.544102668762207\n",
      "[step: 4200] loss: 7.514721393585205\n",
      "[step: 4250] loss: 7.478914737701416\n",
      "[step: 4300] loss: 7.444795608520508\n",
      "[step: 4350] loss: 7.410447597503662\n",
      "[step: 4400] loss: 7.375789642333984\n",
      "[step: 4450] loss: 7.34084415435791\n",
      "[step: 4500] loss: 7.305591106414795\n",
      "[step: 4550] loss: 7.270064830780029\n",
      "[step: 4600] loss: 7.234275817871094\n",
      "[step: 4650] loss: 7.198216915130615\n",
      "[step: 4700] loss: 7.16190242767334\n",
      "[step: 4750] loss: 7.125301361083984\n",
      "[step: 4800] loss: 7.088443756103516\n",
      "[step: 4850] loss: 7.051321029663086\n",
      "[step: 4900] loss: 7.013925552368164\n",
      "[step: 4950] loss: 6.976197242736816\n",
      "[step: 5000] loss: 6.938174724578857\n",
      "[step: 5050] loss: 6.899789333343506\n",
      "[step: 5100] loss: 6.861086845397949\n",
      "[step: 5150] loss: 6.8220930099487305\n",
      "[step: 5200] loss: 6.7830023765563965\n",
      "[step: 5250] loss: 6.743631839752197\n",
      "[step: 5300] loss: 6.704094886779785\n",
      "[step: 5350] loss: 6.664426803588867\n",
      "[step: 5400] loss: 6.624619960784912\n",
      "[step: 5450] loss: 6.584654331207275\n",
      "[step: 5500] loss: 6.544437885284424\n",
      "[step: 5550] loss: 6.503942012786865\n",
      "[step: 5600] loss: 6.463126182556152\n",
      "[step: 5650] loss: 6.421911239624023\n",
      "[step: 5700] loss: 6.380288600921631\n",
      "[step: 5750] loss: 6.338195323944092\n",
      "[step: 5800] loss: 6.295627593994141\n",
      "[step: 5850] loss: 6.252495288848877\n",
      "[step: 5900] loss: 6.208794593811035\n",
      "[step: 5950] loss: 6.164506435394287\n",
      "[step: 6000] loss: 6.119555950164795\n",
      "[step: 6050] loss: 6.073920249938965\n",
      "[step: 6100] loss: 6.027544975280762\n",
      "[step: 6150] loss: 5.980397701263428\n",
      "[step: 6200] loss: 5.932426929473877\n",
      "[step: 6250] loss: 5.883566856384277\n",
      "[step: 6300] loss: 5.833775997161865\n",
      "[step: 6350] loss: 5.782991409301758\n",
      "[step: 6400] loss: 5.731121063232422\n",
      "[step: 6450] loss: 5.678119659423828\n",
      "[step: 6500] loss: 5.623891353607178\n",
      "[step: 6550] loss: 5.568336486816406\n",
      "[step: 6600] loss: 5.511366844177246\n",
      "[step: 6650] loss: 5.452876091003418\n",
      "[step: 6700] loss: 5.392731666564941\n",
      "[step: 6750] loss: 5.3308491706848145\n",
      "[step: 6800] loss: 5.2671003341674805\n",
      "[step: 6850] loss: 5.201381206512451\n",
      "[step: 6900] loss: 5.133531093597412\n",
      "[step: 6950] loss: 5.063394546508789\n",
      "[step: 7000] loss: 4.990660667419434\n",
      "[step: 7050] loss: 4.9150919914245605\n",
      "[step: 7100] loss: 4.836548328399658\n",
      "[step: 7150] loss: 4.7551116943359375\n",
      "[step: 7200] loss: 4.671038627624512\n",
      "[step: 7250] loss: 4.5845947265625\n",
      "[step: 7300] loss: 4.495859146118164\n",
      "[step: 7350] loss: 4.404713153839111\n",
      "[step: 7400] loss: 4.310937881469727\n",
      "[step: 7450] loss: 4.213791370391846\n",
      "[step: 7500] loss: 4.11198616027832\n",
      "[step: 7550] loss: 4.006511211395264\n",
      "[step: 7600] loss: 3.8961684703826904\n",
      "[step: 7650] loss: 3.7832677364349365\n",
      "[step: 7700] loss: 3.674945831298828\n",
      "[step: 7750] loss: 3.569152593612671\n",
      "[step: 7800] loss: 3.4646077156066895\n",
      "[step: 7850] loss: 3.3681955337524414\n",
      "[step: 7900] loss: 3.2749505043029785\n",
      "[step: 7950] loss: 3.184720039367676\n",
      "[step: 8000] loss: 3.0973286628723145\n",
      "[step: 8050] loss: 3.016103982925415\n",
      "[step: 8100] loss: 2.937624454498291\n",
      "[step: 8150] loss: 2.8632497787475586\n",
      "[step: 8200] loss: 2.791536331176758\n",
      "[step: 8250] loss: 2.7216789722442627\n",
      "[step: 8300] loss: 2.653604507446289\n",
      "[step: 8350] loss: 2.5881540775299072\n",
      "[step: 8400] loss: 2.523939847946167\n",
      "[step: 8450] loss: 2.461156129837036\n",
      "[step: 8500] loss: 2.399609327316284\n",
      "[step: 8550] loss: 2.3395352363586426\n",
      "[step: 8600] loss: 2.2834630012512207\n",
      "[step: 8650] loss: 2.2304892539978027\n",
      "[step: 8700] loss: 2.1802785396575928\n",
      "[step: 8750] loss: 2.1325888633728027\n",
      "[step: 8800] loss: 2.0872256755828857\n",
      "[step: 8850] loss: 2.0439271926879883\n",
      "[step: 8900] loss: 2.0024936199188232\n",
      "[step: 8950] loss: 1.9625076055526733\n",
      "[step: 9000] loss: 1.9234247207641602\n",
      "[step: 9050] loss: 1.8858400583267212\n",
      "[step: 9100] loss: 1.8507847785949707\n",
      "[step: 9150] loss: 1.8178627490997314\n",
      "[step: 9200] loss: 1.786824107170105\n",
      "[step: 9250] loss: 1.7578577995300293\n",
      "[step: 9300] loss: 1.730358362197876\n",
      "[step: 9350] loss: 1.7040481567382812\n",
      "[step: 9400] loss: 1.6789132356643677\n",
      "[step: 9450] loss: 1.6553955078125\n",
      "[step: 9500] loss: 1.6328352689743042\n",
      "[step: 9550] loss: 1.6103801727294922\n",
      "[step: 9600] loss: 1.5877833366394043\n",
      "[step: 9650] loss: 1.5657919645309448\n",
      "[step: 9700] loss: 1.545428991317749\n",
      "[step: 9750] loss: 1.5268343687057495\n",
      "[step: 9800] loss: 1.5083168745040894\n",
      "[step: 9850] loss: 1.4900590181350708\n",
      "[step: 9900] loss: 1.4720971584320068\n",
      "[step: 9950] loss: 1.4536564350128174\n",
      "[step: 10000] loss: 1.436058759689331\n",
      "[step: 10050] loss: 1.41970956325531\n",
      "[step: 10100] loss: 1.4025084972381592\n",
      "[step: 10150] loss: 1.3842384815216064\n",
      "[step: 10200] loss: 1.3706148862838745\n",
      "[step: 10250] loss: 1.3555835485458374\n",
      "[step: 10300] loss: 1.3412344455718994\n",
      "[step: 10350] loss: 1.3270493745803833\n",
      "[step: 10400] loss: 1.31430184841156\n",
      "[step: 10450] loss: 1.3012994527816772\n",
      "[step: 10500] loss: 1.2884750366210938\n",
      "[step: 10550] loss: 1.2762601375579834\n",
      "[step: 10600] loss: 1.2643619775772095\n",
      "[step: 10650] loss: 1.2526780366897583\n",
      "[step: 10700] loss: 1.241219162940979\n",
      "[step: 10750] loss: 1.2298696041107178\n",
      "[step: 10800] loss: 1.2186920642852783\n",
      "[step: 10850] loss: 1.20773184299469\n",
      "[step: 10900] loss: 1.1970089673995972\n",
      "[step: 10950] loss: 1.18635892868042\n",
      "[step: 11000] loss: 1.175613284111023\n",
      "[step: 11050] loss: 1.1645231246948242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 11100] loss: 1.1543830633163452\n",
      "[step: 11150] loss: 1.1443060636520386\n",
      "[step: 11200] loss: 1.1345041990280151\n",
      "[step: 11250] loss: 1.1249492168426514\n",
      "[step: 11300] loss: 1.115607500076294\n",
      "[step: 11350] loss: 1.1066359281539917\n",
      "[step: 11400] loss: 1.0976556539535522\n",
      "[step: 11450] loss: 1.0888556241989136\n",
      "[step: 11500] loss: 1.079833984375\n",
      "[step: 11550] loss: 1.0720067024230957\n",
      "[step: 11600] loss: 1.0638456344604492\n",
      "[step: 11650] loss: 1.0557585954666138\n",
      "[step: 11700] loss: 1.0477505922317505\n",
      "[step: 11750] loss: 1.039791464805603\n",
      "[step: 11800] loss: 1.0319302082061768\n",
      "[step: 11850] loss: 1.024169921875\n",
      "[step: 11900] loss: 1.0165358781814575\n",
      "[step: 11950] loss: 1.0090006589889526\n",
      "[step: 12000] loss: 1.0016106367111206\n",
      "[step: 12050] loss: 0.9943075776100159\n",
      "[step: 12100] loss: 0.9870638847351074\n",
      "[step: 12150] loss: 0.9799042344093323\n",
      "[step: 12200] loss: 0.9726874828338623\n",
      "[step: 12250] loss: 0.9655656218528748\n",
      "[step: 12300] loss: 0.9587134122848511\n",
      "[step: 12350] loss: 0.9521154165267944\n",
      "[step: 12400] loss: 0.9457187056541443\n",
      "[step: 12450] loss: 0.939591646194458\n",
      "[step: 12500] loss: 0.9336884617805481\n",
      "[step: 12550] loss: 0.9279799461364746\n",
      "[step: 12600] loss: 0.9224148988723755\n",
      "[step: 12650] loss: 0.9169724583625793\n",
      "[step: 12700] loss: 0.9117584228515625\n",
      "[step: 12750] loss: 0.9066325426101685\n",
      "[step: 12800] loss: 0.9021192789077759\n",
      "[step: 12850] loss: 0.8971073627471924\n",
      "[step: 12900] loss: 0.8921987414360046\n",
      "[step: 12950] loss: 0.8872513771057129\n",
      "[step: 13000] loss: 0.8824639320373535\n",
      "[step: 13050] loss: 0.8776028752326965\n",
      "[step: 13100] loss: 0.873067319393158\n",
      "[step: 13150] loss: 0.8687437176704407\n",
      "[step: 13200] loss: 0.8645258545875549\n",
      "[step: 13250] loss: 0.8603588938713074\n",
      "[step: 13300] loss: 0.8562867641448975\n",
      "[step: 13350] loss: 0.852332353591919\n",
      "[step: 13400] loss: 0.8485544323921204\n",
      "[step: 13450] loss: 0.8449618816375732\n",
      "[step: 13500] loss: 0.8413615822792053\n",
      "[step: 13550] loss: 0.8378297686576843\n",
      "[step: 13600] loss: 0.8343244194984436\n",
      "[step: 13650] loss: 0.8308349251747131\n",
      "[step: 13700] loss: 0.8273205161094666\n",
      "[step: 13750] loss: 0.8237076997756958\n",
      "[step: 13800] loss: 0.8196423649787903\n",
      "[step: 13850] loss: 0.8160950541496277\n",
      "[step: 13900] loss: 0.8128678202629089\n",
      "[step: 13950] loss: 0.8096722960472107\n",
      "[step: 14000] loss: 0.8065537810325623\n",
      "[step: 14050] loss: 0.803493082523346\n",
      "[step: 14100] loss: 0.8004661202430725\n",
      "[step: 14150] loss: 0.7975026369094849\n",
      "[step: 14200] loss: 0.7945696711540222\n",
      "[step: 14250] loss: 0.7916699647903442\n",
      "[step: 14300] loss: 0.788799524307251\n",
      "[step: 14350] loss: 0.7859429717063904\n",
      "[step: 14400] loss: 0.7831143140792847\n",
      "[step: 14450] loss: 0.7802972793579102\n",
      "[step: 14500] loss: 0.7774739861488342\n",
      "[step: 14550] loss: 0.774689793586731\n",
      "[step: 14600] loss: 0.7718866467475891\n",
      "[step: 14650] loss: 0.7690963745117188\n",
      "[step: 14700] loss: 0.7663536071777344\n",
      "[step: 14750] loss: 0.763623058795929\n",
      "[step: 14800] loss: 0.7609253525733948\n",
      "[step: 14850] loss: 0.7582206130027771\n",
      "[step: 14900] loss: 0.7555400729179382\n",
      "[step: 14950] loss: 0.7528738379478455\n",
      "[step: 15000] loss: 0.7501992583274841\n",
      "[step: 15050] loss: 0.7475890517234802\n",
      "[step: 15100] loss: 0.7449510097503662\n",
      "[step: 15150] loss: 0.7423308491706848\n",
      "[step: 15200] loss: 0.7396753430366516\n",
      "[step: 15250] loss: 0.7370043992996216\n",
      "[step: 15300] loss: 0.7342771291732788\n",
      "[step: 15350] loss: 0.7314682006835938\n",
      "[step: 15400] loss: 0.7284401655197144\n",
      "[step: 15450] loss: 0.7246167063713074\n",
      "[step: 15500] loss: 0.7224031686782837\n",
      "[step: 15550] loss: 0.7200801968574524\n",
      "[step: 15600] loss: 0.7177518010139465\n",
      "[step: 15650] loss: 0.7154757380485535\n",
      "[step: 15700] loss: 0.7132439017295837\n",
      "[step: 15750] loss: 0.7110235095024109\n",
      "[step: 15800] loss: 0.7088372111320496\n",
      "[step: 15850] loss: 0.7066793441772461\n",
      "[step: 15900] loss: 0.7045433521270752\n",
      "[step: 15950] loss: 0.7024330496788025\n",
      "[step: 16000] loss: 0.7003268003463745\n",
      "[step: 16050] loss: 0.6982493996620178\n",
      "[step: 16100] loss: 0.6961831450462341\n",
      "[step: 16150] loss: 0.6941360831260681\n",
      "[step: 16200] loss: 0.6920773983001709\n",
      "[step: 16250] loss: 0.6900385618209839\n",
      "[step: 16300] loss: 0.6880042552947998\n",
      "[step: 16350] loss: 0.6859675049781799\n",
      "[step: 16400] loss: 0.6839401125907898\n",
      "[step: 16450] loss: 0.6819003224372864\n",
      "[step: 16500] loss: 0.6798515915870667\n",
      "[step: 16550] loss: 0.6777877807617188\n",
      "[step: 16600] loss: 0.6757073402404785\n",
      "[step: 16650] loss: 0.6735963821411133\n",
      "[step: 16700] loss: 0.6714287400245667\n",
      "[step: 16750] loss: 0.6692802906036377\n",
      "[step: 16800] loss: 0.6671931147575378\n",
      "[step: 16850] loss: 0.6651848554611206\n",
      "[step: 16900] loss: 0.6632553339004517\n",
      "[step: 16950] loss: 0.6613773703575134\n",
      "[step: 17000] loss: 0.6595401763916016\n",
      "[step: 17050] loss: 0.6577315330505371\n",
      "[step: 17100] loss: 0.6559455990791321\n",
      "[step: 17150] loss: 0.6541872620582581\n",
      "[step: 17200] loss: 0.6524364352226257\n",
      "[step: 17250] loss: 0.650703489780426\n",
      "[step: 17300] loss: 0.6489596366882324\n",
      "[step: 17350] loss: 0.6472744345664978\n",
      "[step: 17400] loss: 0.6455826163291931\n",
      "[step: 17450] loss: 0.64391028881073\n",
      "[step: 17500] loss: 0.6422560811042786\n",
      "[step: 17550] loss: 0.6405975818634033\n",
      "[step: 17600] loss: 0.6388907432556152\n",
      "[step: 17650] loss: 0.6372639536857605\n",
      "[step: 17700] loss: 0.6356468796730042\n",
      "[step: 17750] loss: 0.6339502930641174\n",
      "[step: 17800] loss: 0.6322967410087585\n",
      "[step: 17850] loss: 0.6306064128875732\n",
      "[step: 17900] loss: 0.6289554834365845\n",
      "[step: 17950] loss: 0.6281734108924866\n",
      "[step: 18000] loss: 0.6253252625465393\n",
      "[step: 18050] loss: 0.6250736713409424\n",
      "[step: 18100] loss: 0.6223852634429932\n",
      "[step: 18150] loss: 0.6219750642776489\n",
      "[step: 18200] loss: 0.6197918653488159\n",
      "[step: 18250] loss: 0.6192746162414551\n",
      "[step: 18300] loss: 0.617076575756073\n",
      "[step: 18350] loss: 0.6163328289985657\n",
      "[step: 18400] loss: 0.6145632266998291\n",
      "[step: 18450] loss: 0.6134551763534546\n",
      "[step: 18500] loss: 0.6121032238006592\n",
      "[step: 18550] loss: 0.6106025576591492\n",
      "[step: 18600] loss: 0.6095030307769775\n",
      "[step: 18650] loss: 0.6080660223960876\n",
      "[step: 18700] loss: 0.6077399253845215\n",
      "[step: 18750] loss: 0.605621337890625\n",
      "[step: 18800] loss: 0.6035568118095398\n",
      "[step: 18850] loss: 0.602618932723999\n",
      "[step: 18900] loss: 0.6026982665061951\n",
      "[step: 18950] loss: 0.601373553276062\n",
      "[step: 19000] loss: 0.6000734567642212\n",
      "[step: 19050] loss: 0.5991964340209961\n",
      "[step: 19100] loss: 0.5971246361732483\n",
      "[step: 19150] loss: 0.5961215496063232\n",
      "[step: 19200] loss: 0.5949506163597107\n",
      "[step: 19250] loss: 0.5939935445785522\n",
      "[step: 19300] loss: 0.5931375622749329\n",
      "[step: 19350] loss: 0.5927570462226868\n",
      "[step: 19400] loss: 0.5911262631416321\n",
      "[step: 19450] loss: 0.5896339416503906\n",
      "[step: 19500] loss: 0.5887892842292786\n",
      "[step: 19550] loss: 0.5877944827079773\n",
      "[step: 19600] loss: 0.5872926115989685\n",
      "[step: 19650] loss: 0.5864841341972351\n",
      "[step: 19700] loss: 0.5851763486862183\n",
      "[step: 19750] loss: 0.5835842490196228\n",
      "[step: 19800] loss: 0.5824150443077087\n",
      "[step: 19850] loss: 0.5815631747245789\n",
      "[step: 19900] loss: 0.5808175206184387\n",
      "[step: 19950] loss: 0.5798466205596924\n",
      "[step: 20000] loss: 0.5788252949714661\n",
      "[step: 20050] loss: 0.5778903365135193\n",
      "[step: 20100] loss: 0.5770041346549988\n",
      "[step: 20150] loss: 0.576133668422699\n",
      "[step: 20200] loss: 0.5753549337387085\n",
      "[step: 20250] loss: 0.5743568539619446\n",
      "[step: 20300] loss: 0.5734071135520935\n",
      "[step: 20350] loss: 0.5720146298408508\n",
      "[step: 20400] loss: 0.5707994103431702\n",
      "[step: 20450] loss: 0.5699265599250793\n",
      "[step: 20500] loss: 0.5693173408508301\n",
      "[step: 20550] loss: 0.5682855248451233\n",
      "[step: 20600] loss: 0.5675248503684998\n",
      "[step: 20650] loss: 0.5661317110061646\n",
      "[step: 20700] loss: 0.5655354261398315\n",
      "[step: 20750] loss: 0.5646212100982666\n",
      "[step: 20800] loss: 0.5633652210235596\n",
      "[step: 20850] loss: 0.5627280473709106\n",
      "[step: 20900] loss: 0.5617603063583374\n",
      "[step: 20950] loss: 0.5605925917625427\n",
      "[step: 21000] loss: 0.5599547624588013\n",
      "[step: 21050] loss: 0.5589412450790405\n",
      "[step: 21100] loss: 0.5578171014785767\n",
      "[step: 21150] loss: 0.5572044253349304\n",
      "[step: 21200] loss: 0.5561741590499878\n",
      "[step: 21250] loss: 0.5550481081008911\n",
      "[step: 21300] loss: 0.5544906854629517\n",
      "[step: 21350] loss: 0.5533532500267029\n",
      "[step: 21400] loss: 0.5524759888648987\n",
      "[step: 21450] loss: 0.5514784455299377\n",
      "[step: 21500] loss: 0.5507432818412781\n",
      "[step: 21550] loss: 0.5498353242874146\n",
      "[step: 21600] loss: 0.5487843155860901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 21650] loss: 0.5478572249412537\n",
      "[step: 21700] loss: 0.5475229620933533\n",
      "[step: 21750] loss: 0.5462732911109924\n",
      "[step: 21800] loss: 0.5451908111572266\n",
      "[step: 21850] loss: 0.543921709060669\n",
      "[step: 21900] loss: 0.5435492396354675\n",
      "[step: 21950] loss: 0.5427396297454834\n",
      "[step: 22000] loss: 0.5422476530075073\n",
      "[step: 22050] loss: 0.5409959554672241\n",
      "[step: 22100] loss: 0.5401768088340759\n",
      "[step: 22150] loss: 0.5386452674865723\n",
      "[step: 22200] loss: 0.5381909012794495\n",
      "[step: 22250] loss: 0.5366259217262268\n",
      "[step: 22300] loss: 0.5364478230476379\n",
      "[step: 22350] loss: 0.5350139737129211\n",
      "[step: 22400] loss: 0.5346460938453674\n",
      "[step: 22450] loss: 0.5331635475158691\n",
      "[step: 22500] loss: 0.532777726650238\n",
      "[step: 22550] loss: 0.531428337097168\n",
      "[step: 22600] loss: 0.5310402512550354\n",
      "[step: 22650] loss: 0.5297170281410217\n",
      "[step: 22700] loss: 0.5297018885612488\n",
      "[step: 22750] loss: 0.5288935899734497\n",
      "[step: 22800] loss: 0.5275198817253113\n",
      "[step: 22850] loss: 0.5269194841384888\n",
      "[step: 22900] loss: 0.5260829925537109\n",
      "[step: 22950] loss: 0.5249829888343811\n",
      "[step: 23000] loss: 0.5243242979049683\n",
      "[step: 23050] loss: 0.5232806205749512\n",
      "[step: 23100] loss: 0.5224616527557373\n",
      "[step: 23150] loss: 0.5215865969657898\n",
      "[step: 23200] loss: 0.5212867856025696\n",
      "[step: 23250] loss: 0.5201305747032166\n",
      "[step: 23300] loss: 0.5194190740585327\n",
      "[step: 23350] loss: 0.5187521576881409\n",
      "[step: 23400] loss: 0.5174576640129089\n",
      "[step: 23450] loss: 0.5171947479248047\n",
      "[step: 23500] loss: 0.5162150263786316\n",
      "[step: 23550] loss: 0.5149485468864441\n",
      "[step: 23600] loss: 0.5148957967758179\n",
      "[step: 23650] loss: 0.5136479735374451\n",
      "[step: 23700] loss: 0.513187825679779\n",
      "[step: 23750] loss: 0.5121057033538818\n",
      "[step: 23800] loss: 0.5108940005302429\n",
      "[step: 23850] loss: 0.5108131170272827\n",
      "[step: 23900] loss: 0.509783923625946\n",
      "[step: 23950] loss: 0.5085863471031189\n",
      "[step: 24000] loss: 0.5080747008323669\n",
      "[step: 24050] loss: 0.5074638724327087\n",
      "[step: 24100] loss: 0.5063146948814392\n",
      "[step: 24150] loss: 0.5054758787155151\n",
      "[step: 24200] loss: 0.5052227973937988\n",
      "[step: 24250] loss: 0.5041835308074951\n",
      "[step: 24300] loss: 0.5030848383903503\n",
      "[step: 24350] loss: 0.5025061964988708\n",
      "[step: 24400] loss: 0.5019229650497437\n",
      "[step: 24450] loss: 0.5007197260856628\n",
      "[step: 24500] loss: 0.5003346800804138\n",
      "[step: 24550] loss: 0.499554306268692\n",
      "[step: 24600] loss: 0.4983784854412079\n",
      "[step: 24650] loss: 0.4981659948825836\n",
      "[step: 24700] loss: 0.4971846640110016\n",
      "[step: 24750] loss: 0.4960702061653137\n",
      "[step: 24800] loss: 0.49602949619293213\n",
      "[step: 24850] loss: 0.4946853518486023\n",
      "[step: 24900] loss: 0.49446871876716614\n",
      "[step: 24950] loss: 0.4933812916278839\n",
      "[step: 25000] loss: 0.49280741810798645\n",
      "[step: 25050] loss: 0.49210304021835327\n",
      "[step: 25100] loss: 0.49157509207725525\n",
      "[step: 25150] loss: 0.4910720884799957\n",
      "[step: 25200] loss: 0.49002397060394287\n",
      "[step: 25250] loss: 0.48892632126808167\n",
      "[step: 25300] loss: 0.48821014165878296\n",
      "[step: 25350] loss: 0.4877012372016907\n",
      "[step: 25400] loss: 0.4866759777069092\n",
      "[step: 25450] loss: 0.4862011671066284\n",
      "[step: 25500] loss: 0.4849514067173004\n",
      "[step: 25550] loss: 0.48446616530418396\n",
      "[step: 25600] loss: 0.4839373528957367\n",
      "[step: 25650] loss: 0.48339441418647766\n",
      "[step: 25700] loss: 0.4823884963989258\n",
      "[step: 25750] loss: 0.48148638010025024\n",
      "[step: 25800] loss: 0.48092812299728394\n",
      "[step: 25850] loss: 0.48085111379623413\n",
      "[step: 25900] loss: 0.4790450632572174\n",
      "[step: 25950] loss: 0.4783778786659241\n",
      "[step: 26000] loss: 0.4782026410102844\n",
      "[step: 26050] loss: 0.47676005959510803\n",
      "[step: 26100] loss: 0.4763525128364563\n",
      "[step: 26150] loss: 0.47647154331207275\n",
      "[step: 26200] loss: 0.47485247254371643\n",
      "[step: 26250] loss: 0.47492098808288574\n",
      "[step: 26300] loss: 0.47336772084236145\n",
      "[step: 26350] loss: 0.473503053188324\n",
      "[step: 26400] loss: 0.4718891680240631\n",
      "[step: 26450] loss: 0.47210389375686646\n",
      "[step: 26500] loss: 0.47043806314468384\n",
      "[step: 26550] loss: 0.4707135260105133\n",
      "[step: 26600] loss: 0.4689815640449524\n",
      "[step: 26650] loss: 0.46931713819503784\n",
      "[step: 26700] loss: 0.4675339162349701\n",
      "[step: 26750] loss: 0.46791547536849976\n",
      "[step: 26800] loss: 0.46611645817756653\n",
      "[step: 26850] loss: 0.466511070728302\n",
      "[step: 26900] loss: 0.46470797061920166\n",
      "[step: 26950] loss: 0.46511998772621155\n",
      "[step: 27000] loss: 0.46331295371055603\n",
      "[step: 27050] loss: 0.4637211859226227\n",
      "[step: 27100] loss: 0.4619206190109253\n",
      "[step: 27150] loss: 0.4623410999774933\n",
      "[step: 27200] loss: 0.46055594086647034\n",
      "[step: 27250] loss: 0.46096405386924744\n",
      "[step: 27300] loss: 0.4591938853263855\n",
      "[step: 27350] loss: 0.45959770679473877\n",
      "[step: 27400] loss: 0.45784255862236023\n",
      "[step: 27450] loss: 0.45821908116340637\n",
      "[step: 27500] loss: 0.45651549100875854\n",
      "[step: 27550] loss: 0.4568675458431244\n",
      "[step: 27600] loss: 0.4551953971385956\n",
      "[step: 27650] loss: 0.4555111229419708\n",
      "[step: 27700] loss: 0.4538697898387909\n",
      "[step: 27750] loss: 0.45416462421417236\n",
      "[step: 27800] loss: 0.4525587558746338\n",
      "[step: 27850] loss: 0.4528217017650604\n",
      "[step: 27900] loss: 0.4512363374233246\n",
      "[step: 27950] loss: 0.45150241255760193\n",
      "[step: 28000] loss: 0.44991469383239746\n",
      "[step: 28050] loss: 0.45018529891967773\n",
      "[step: 28100] loss: 0.44858166575431824\n",
      "[step: 28150] loss: 0.4488746225833893\n",
      "[step: 28200] loss: 0.4472522437572479\n",
      "[step: 28250] loss: 0.44756942987442017\n",
      "[step: 28300] loss: 0.44592729210853577\n",
      "[step: 28350] loss: 0.44627895951271057\n",
      "[step: 28400] loss: 0.44460514187812805\n",
      "[step: 28450] loss: 0.44499194622039795\n",
      "[step: 28500] loss: 0.44328081607818604\n",
      "[step: 28550] loss: 0.4437163770198822\n",
      "[step: 28600] loss: 0.4419558346271515\n",
      "[step: 28650] loss: 0.4424442946910858\n",
      "[step: 28700] loss: 0.4406397342681885\n",
      "[step: 28750] loss: 0.4411563277244568\n",
      "[step: 28800] loss: 0.43932345509529114\n",
      "[step: 28850] loss: 0.43983718752861023\n",
      "[step: 28900] loss: 0.4380376636981964\n",
      "[step: 28950] loss: 0.4385524392127991\n",
      "[step: 29000] loss: 0.43676120042800903\n",
      "[step: 29050] loss: 0.4372638463973999\n",
      "[step: 29100] loss: 0.43550339341163635\n",
      "[step: 29150] loss: 0.4359992444515228\n",
      "[step: 29200] loss: 0.4342502951622009\n",
      "[step: 29250] loss: 0.4347202181816101\n",
      "[step: 29300] loss: 0.4329889416694641\n",
      "[step: 29350] loss: 0.43344834446907043\n",
      "[step: 29400] loss: 0.4317362308502197\n",
      "[step: 29450] loss: 0.4322005808353424\n",
      "[step: 29500] loss: 0.4305040240287781\n",
      "[step: 29550] loss: 0.43094122409820557\n",
      "[step: 29600] loss: 0.4292600750923157\n",
      "[step: 29650] loss: 0.4297036826610565\n",
      "[step: 29700] loss: 0.42803293466567993\n",
      "[step: 29750] loss: 0.4284617304801941\n",
      "[step: 29800] loss: 0.4268026351928711\n",
      "[step: 29850] loss: 0.42723459005355835\n",
      "[step: 29900] loss: 0.42557674646377563\n",
      "[step: 29950] loss: 0.42599207162857056\n",
      "[step: 30000] loss: 0.42434263229370117\n",
      "[step: 30050] loss: 0.4247548580169678\n",
      "[step: 30100] loss: 0.42311567068099976\n",
      "[step: 30150] loss: 0.4235187768936157\n",
      "[step: 30200] loss: 0.42186906933784485\n",
      "[step: 30250] loss: 0.422274112701416\n",
      "[step: 30300] loss: 0.4206162989139557\n",
      "[step: 30350] loss: 0.42101675271987915\n",
      "[step: 30400] loss: 0.4193238317966461\n",
      "[step: 30450] loss: 0.41975507140159607\n",
      "[step: 30500] loss: 0.4180033504962921\n",
      "[step: 30550] loss: 0.4184842109680176\n",
      "[step: 30600] loss: 0.41664671897888184\n",
      "[step: 30650] loss: 0.41720685362815857\n",
      "[step: 30700] loss: 0.4153216779232025\n",
      "[step: 30750] loss: 0.415953665971756\n",
      "[step: 30800] loss: 0.41406843066215515\n",
      "[step: 30850] loss: 0.4147286117076874\n",
      "[step: 30900] loss: 0.41285085678100586\n",
      "[step: 30950] loss: 0.41351962089538574\n",
      "[step: 31000] loss: 0.411680668592453\n",
      "[step: 31050] loss: 0.41233202815055847\n",
      "[step: 31100] loss: 0.41052278876304626\n",
      "[step: 31150] loss: 0.41114768385887146\n",
      "[step: 31200] loss: 0.40937894582748413\n",
      "[step: 31250] loss: 0.4099823534488678\n",
      "[step: 31300] loss: 0.4082464277744293\n",
      "[step: 31350] loss: 0.408816397190094\n",
      "[step: 31400] loss: 0.40711838006973267\n",
      "[step: 31450] loss: 0.4076721966266632\n",
      "[step: 31500] loss: 0.4060099720954895\n",
      "[step: 31550] loss: 0.4065279960632324\n",
      "[step: 31600] loss: 0.40490806102752686\n",
      "[step: 31650] loss: 0.4053973853588104\n",
      "[step: 31700] loss: 0.4038054049015045\n",
      "[step: 31750] loss: 0.4042815566062927\n",
      "[step: 31800] loss: 0.4027048349380493\n",
      "[step: 31850] loss: 0.40317344665527344\n",
      "[step: 31900] loss: 0.40161073207855225\n",
      "[step: 31950] loss: 0.40206676721572876\n",
      "[step: 32000] loss: 0.4005281329154968\n",
      "[step: 32050] loss: 0.40095680952072144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 32100] loss: 0.399453729391098\n",
      "[step: 32150] loss: 0.3998451232910156\n",
      "[step: 32200] loss: 0.39838171005249023\n",
      "[step: 32250] loss: 0.3987351655960083\n",
      "[step: 32300] loss: 0.397304505109787\n",
      "[step: 32350] loss: 0.39764341711997986\n",
      "[step: 32400] loss: 0.396229088306427\n",
      "[step: 32450] loss: 0.3965466618537903\n",
      "[step: 32500] loss: 0.39516177773475647\n",
      "[step: 32550] loss: 0.39546456933021545\n",
      "[step: 32600] loss: 0.3941028118133545\n",
      "[step: 32650] loss: 0.3943737745285034\n",
      "[step: 32700] loss: 0.39302879571914673\n",
      "[step: 32750] loss: 0.3932865858078003\n",
      "[step: 32800] loss: 0.3919646143913269\n",
      "[step: 32850] loss: 0.39220160245895386\n",
      "[step: 32900] loss: 0.3908972144126892\n",
      "[step: 32950] loss: 0.3911162316799164\n",
      "[step: 33000] loss: 0.38983169198036194\n",
      "[step: 33050] loss: 0.39002543687820435\n",
      "[step: 33100] loss: 0.38876426219940186\n",
      "[step: 33150] loss: 0.38894444704055786\n",
      "[step: 33200] loss: 0.3876877427101135\n",
      "[step: 33250] loss: 0.38785359263420105\n",
      "[step: 33300] loss: 0.3866199851036072\n",
      "[step: 33350] loss: 0.38678455352783203\n",
      "[step: 33400] loss: 0.38556790351867676\n",
      "[step: 33450] loss: 0.38567692041397095\n",
      "[step: 33500] loss: 0.384530633687973\n",
      "[step: 33550] loss: 0.38441869616508484\n",
      "[step: 33600] loss: 0.38361531496047974\n",
      "[step: 33650] loss: 0.38294827938079834\n",
      "[step: 33700] loss: 0.38279134035110474\n",
      "[step: 33750] loss: 0.38232484459877014\n",
      "[step: 33800] loss: 0.38180968165397644\n",
      "[step: 33850] loss: 0.38126346468925476\n",
      "[step: 33900] loss: 0.38085344433784485\n",
      "[step: 33950] loss: 0.3801828920841217\n",
      "[step: 34000] loss: 0.379818856716156\n",
      "[step: 34050] loss: 0.37938886880874634\n",
      "[step: 34100] loss: 0.3786564767360687\n",
      "[step: 34150] loss: 0.3782811462879181\n",
      "[step: 34200] loss: 0.3776184618473053\n",
      "[step: 34250] loss: 0.37726661562919617\n",
      "[step: 34300] loss: 0.37626969814300537\n",
      "[step: 34350] loss: 0.3754284977912903\n",
      "[step: 34400] loss: 0.37513723969459534\n",
      "[step: 34450] loss: 0.37519359588623047\n",
      "[step: 34500] loss: 0.37439030408859253\n",
      "[step: 34550] loss: 0.3735399842262268\n",
      "[step: 34600] loss: 0.37307485938072205\n",
      "[step: 34650] loss: 0.37307536602020264\n",
      "[step: 34700] loss: 0.37230002880096436\n",
      "[step: 34750] loss: 0.3716503977775574\n",
      "[step: 34800] loss: 0.3710227906703949\n",
      "[step: 34850] loss: 0.37160080671310425\n",
      "[step: 34900] loss: 0.3699268102645874\n",
      "[step: 34950] loss: 0.36938196420669556\n",
      "[step: 35000] loss: 0.3698320984840393\n",
      "[step: 35050] loss: 0.36862748861312866\n",
      "[step: 35100] loss: 0.367583304643631\n",
      "[step: 35150] loss: 0.36807781457901\n",
      "[step: 35200] loss: 0.36813902854919434\n",
      "[step: 35250] loss: 0.36744073033332825\n",
      "[step: 35300] loss: 0.36616867780685425\n",
      "[step: 35350] loss: 0.36496731638908386\n",
      "[step: 35400] loss: 0.36585643887519836\n",
      "[step: 35450] loss: 0.36351829767227173\n",
      "[step: 35500] loss: 0.3633086085319519\n",
      "[step: 35550] loss: 0.36269184947013855\n",
      "[step: 35600] loss: 0.36305510997772217\n",
      "[step: 35650] loss: 0.36213797330856323\n",
      "[step: 35700] loss: 0.36141765117645264\n",
      "[step: 35750] loss: 0.3612527549266815\n",
      "[step: 35800] loss: 0.3609267771244049\n",
      "[step: 35850] loss: 0.3599335253238678\n",
      "[step: 35900] loss: 0.3589578866958618\n",
      "[step: 35950] loss: 0.358643114566803\n",
      "[step: 36000] loss: 0.35866302251815796\n",
      "[step: 36050] loss: 0.3584532141685486\n",
      "[step: 36100] loss: 0.3569493293762207\n",
      "[step: 36150] loss: 0.3578052818775177\n",
      "[step: 36200] loss: 0.3566432595252991\n",
      "[step: 36250] loss: 0.3560265898704529\n",
      "[step: 36300] loss: 0.3549378216266632\n",
      "[step: 36350] loss: 0.35616201162338257\n",
      "[step: 36400] loss: 0.3543062210083008\n",
      "[step: 36450] loss: 0.35427069664001465\n",
      "[step: 36500] loss: 0.35429584980010986\n",
      "[step: 36550] loss: 0.35094231367111206\n",
      "[step: 36600] loss: 0.3521190583705902\n",
      "[step: 36650] loss: 0.35345155000686646\n",
      "[step: 36700] loss: 0.35292819142341614\n",
      "[step: 36750] loss: 0.35275715589523315\n",
      "[step: 36800] loss: 0.3520199656486511\n",
      "[step: 36850] loss: 0.3517312705516815\n",
      "[step: 36900] loss: 0.35053473711013794\n",
      "[step: 36950] loss: 0.35098761320114136\n",
      "[step: 37000] loss: 0.3496168255805969\n",
      "[step: 37050] loss: 0.3484862148761749\n",
      "[step: 37100] loss: 0.3461958169937134\n",
      "[step: 37150] loss: 0.3458836078643799\n",
      "[step: 37200] loss: 0.34572336077690125\n",
      "[step: 37250] loss: 0.3466380536556244\n",
      "[step: 37300] loss: 0.3467285931110382\n",
      "[step: 37350] loss: 0.347136914730072\n",
      "[step: 37400] loss: 0.34665733575820923\n",
      "[step: 37450] loss: 0.34578266739845276\n",
      "[step: 37500] loss: 0.34481966495513916\n",
      "[step: 37550] loss: 0.3424452245235443\n",
      "[step: 37600] loss: 0.341879278421402\n",
      "[step: 37650] loss: 0.34196850657463074\n",
      "[step: 37700] loss: 0.3415759205818176\n",
      "[step: 37750] loss: 0.3416656255722046\n",
      "[step: 37800] loss: 0.34193503856658936\n",
      "[step: 37850] loss: 0.3426680266857147\n",
      "[step: 37900] loss: 0.34223175048828125\n",
      "[step: 37950] loss: 0.34168940782546997\n",
      "[step: 38000] loss: 0.34121331572532654\n",
      "[step: 38050] loss: 0.34070345759391785\n",
      "[step: 38100] loss: 0.3402195870876312\n",
      "[step: 38150] loss: 0.3397276997566223\n",
      "[step: 38200] loss: 0.3392491042613983\n",
      "[step: 38250] loss: 0.338763564825058\n",
      "[step: 38300] loss: 0.33829253911972046\n",
      "[step: 38350] loss: 0.3378742039203644\n",
      "[step: 38400] loss: 0.33654072880744934\n",
      "[step: 38450] loss: 0.3352426290512085\n",
      "[step: 38500] loss: 0.33360087871551514\n",
      "[step: 38550] loss: 0.3330549895763397\n",
      "[step: 38600] loss: 0.33393651247024536\n",
      "[step: 38650] loss: 0.3352448642253876\n",
      "[step: 38700] loss: 0.33456483483314514\n",
      "[step: 38750] loss: 0.33192580938339233\n",
      "[step: 38800] loss: 0.33008456230163574\n",
      "[step: 38850] loss: 0.33200645446777344\n",
      "[step: 38900] loss: 0.3322618305683136\n",
      "[step: 38950] loss: 0.3312014639377594\n",
      "[step: 39000] loss: 0.32807329297065735\n",
      "[step: 39050] loss: 0.33024662733078003\n",
      "[step: 39100] loss: 0.3309617340564728\n",
      "[step: 39150] loss: 0.3274611830711365\n",
      "[step: 39200] loss: 0.3289494812488556\n",
      "[step: 39250] loss: 0.32887041568756104\n",
      "[step: 39300] loss: 0.3258596658706665\n",
      "[step: 39350] loss: 0.32829371094703674\n",
      "[step: 39400] loss: 0.326659619808197\n",
      "[step: 39450] loss: 0.3247596025466919\n",
      "[step: 39500] loss: 0.32703548669815063\n",
      "[step: 39550] loss: 0.3235042989253998\n",
      "[step: 39600] loss: 0.3263327479362488\n",
      "[step: 39650] loss: 0.32325032353401184\n",
      "[step: 39700] loss: 0.32487842440605164\n",
      "[step: 39750] loss: 0.323028028011322\n",
      "[step: 39800] loss: 0.3235757648944855\n",
      "[step: 39850] loss: 0.32221710681915283\n",
      "[step: 39900] loss: 0.3227597177028656\n",
      "[step: 39950] loss: 0.32109981775283813\n",
      "[step: 40000] loss: 0.32225459814071655\n",
      "[step: 40050] loss: 0.3192456066608429\n",
      "[step: 40100] loss: 0.32189086079597473\n",
      "[step: 40150] loss: 0.3177885115146637\n",
      "[step: 40200] loss: 0.3206172287464142\n",
      "[step: 40250] loss: 0.3176976442337036\n",
      "[step: 40300] loss: 0.3180428743362427\n",
      "[step: 40350] loss: 0.3190256655216217\n",
      "[step: 40400] loss: 0.31550344824790955\n",
      "[step: 40450] loss: 0.318340539932251\n",
      "[step: 40500] loss: 0.3166249394416809\n",
      "[step: 40550] loss: 0.31443220376968384\n",
      "[step: 40600] loss: 0.3170985281467438\n",
      "[step: 40650] loss: 0.3150351047515869\n",
      "[step: 40700] loss: 0.31297263503074646\n",
      "[step: 40750] loss: 0.31558024883270264\n",
      "[step: 40800] loss: 0.31438907980918884\n",
      "[step: 40850] loss: 0.31096091866493225\n",
      "[step: 40900] loss: 0.3131973147392273\n",
      "[step: 40950] loss: 0.31386616826057434\n",
      "[step: 41000] loss: 0.3106313943862915\n",
      "[step: 41050] loss: 0.30962738394737244\n",
      "[step: 41100] loss: 0.31210240721702576\n",
      "[step: 41150] loss: 0.3118407726287842\n",
      "[step: 41200] loss: 0.3080385625362396\n",
      "[step: 41250] loss: 0.30816662311553955\n",
      "[step: 41300] loss: 0.3103787899017334\n",
      "[step: 41350] loss: 0.309781014919281\n",
      "[step: 41400] loss: 0.30603447556495667\n",
      "[step: 41450] loss: 0.3065144121646881\n",
      "[step: 41500] loss: 0.3084571361541748\n",
      "[step: 41550] loss: 0.3077232241630554\n",
      "[step: 41600] loss: 0.30410531163215637\n",
      "[step: 41650] loss: 0.304909348487854\n",
      "[step: 41700] loss: 0.3064750134944916\n",
      "[step: 41750] loss: 0.30545419454574585\n",
      "[step: 41800] loss: 0.30211693048477173\n",
      "[step: 41850] loss: 0.3036527931690216\n",
      "[step: 41900] loss: 0.30442044138908386\n",
      "[step: 41950] loss: 0.30277594923973083\n",
      "[step: 42000] loss: 0.3006175458431244\n",
      "[step: 42050] loss: 0.30231139063835144\n",
      "[step: 42100] loss: 0.30209875106811523\n",
      "[step: 42150] loss: 0.2992497682571411\n",
      "[step: 42200] loss: 0.3002263605594635\n",
      "[step: 42250] loss: 0.30064657330513\n",
      "[step: 42300] loss: 0.29781562089920044\n",
      "[step: 42350] loss: 0.2990661561489105\n",
      "[step: 42400] loss: 0.29925936460494995\n",
      "[step: 42450] loss: 0.2962278127670288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 42500] loss: 0.29851096868515015\n",
      "[step: 42550] loss: 0.2968641221523285\n",
      "[step: 42600] loss: 0.29543831944465637\n",
      "[step: 42650] loss: 0.2977413833141327\n",
      "[step: 42700] loss: 0.29608842730522156\n",
      "[step: 42750] loss: 0.293064147233963\n",
      "[step: 42800] loss: 0.295206755399704\n",
      "[step: 42850] loss: 0.29626724123954773\n",
      "[step: 42900] loss: 0.29486843943595886\n",
      "[step: 42950] loss: 0.2911500632762909\n",
      "[step: 43000] loss: 0.2919345498085022\n",
      "[step: 43050] loss: 0.2936093807220459\n",
      "[step: 43100] loss: 0.29430827498435974\n",
      "[step: 43150] loss: 0.2936849892139435\n",
      "[step: 43200] loss: 0.290850430727005\n",
      "[step: 43250] loss: 0.28826332092285156\n",
      "[step: 43300] loss: 0.28873589634895325\n",
      "[step: 43350] loss: 0.2904253602027893\n",
      "[step: 43400] loss: 0.29171040654182434\n",
      "[step: 43450] loss: 0.2917352616786957\n",
      "[step: 43500] loss: 0.2894737124443054\n",
      "[step: 43550] loss: 0.2861824333667755\n",
      "[step: 43600] loss: 0.28551602363586426\n",
      "[step: 43650] loss: 0.28631237149238586\n",
      "[step: 43700] loss: 0.28696203231811523\n",
      "[step: 43750] loss: 0.28829705715179443\n",
      "[step: 43800] loss: 0.2887563705444336\n",
      "[step: 43850] loss: 0.28898850083351135\n",
      "[step: 43900] loss: 0.2888697683811188\n",
      "[step: 43950] loss: 0.2886095345020294\n",
      "[step: 44000] loss: 0.2880461812019348\n",
      "[step: 44050] loss: 0.28752461075782776\n",
      "[step: 44100] loss: 0.2871406674385071\n",
      "[step: 44150] loss: 0.28690460324287415\n",
      "[step: 44200] loss: 0.28679990768432617\n",
      "[step: 44250] loss: 0.28670912981033325\n",
      "[step: 44300] loss: 0.28628474473953247\n",
      "[step: 44350] loss: 0.2856121063232422\n",
      "[step: 44400] loss: 0.28382986783981323\n",
      "[step: 44450] loss: 0.28068211674690247\n",
      "[step: 44500] loss: 0.2775236964225769\n",
      "[step: 44550] loss: 0.27753299474716187\n",
      "[step: 44600] loss: 0.28259485960006714\n",
      "[step: 44650] loss: 0.28424134850502014\n",
      "[step: 44700] loss: 0.27755603194236755\n",
      "[step: 44750] loss: 0.27922365069389343\n",
      "[step: 44800] loss: 0.2819555401802063\n",
      "[step: 44850] loss: 0.27639099955558777\n",
      "[step: 44900] loss: 0.2823184132575989\n",
      "[step: 44950] loss: 0.2739918828010559\n",
      "[step: 45000] loss: 0.281722754240036\n",
      "[step: 45050] loss: 0.27310290932655334\n",
      "[step: 45100] loss: 0.2803109884262085\n",
      "[step: 45150] loss: 0.27405622601509094\n",
      "[step: 45200] loss: 0.27792662382125854\n",
      "[step: 45250] loss: 0.2757473886013031\n",
      "[step: 45300] loss: 0.2763635218143463\n",
      "[step: 45350] loss: 0.27615031599998474\n",
      "[step: 45400] loss: 0.2750602066516876\n",
      "[step: 45450] loss: 0.2756878137588501\n",
      "[step: 45500] loss: 0.27447187900543213\n",
      "[step: 45550] loss: 0.2744196355342865\n",
      "[step: 45600] loss: 0.27444130182266235\n",
      "[step: 45650] loss: 0.2720281779766083\n",
      "[step: 45700] loss: 0.2748686373233795\n",
      "[step: 45750] loss: 0.269374281167984\n",
      "[step: 45800] loss: 0.2755888104438782\n",
      "[step: 45850] loss: 0.26840338110923767\n",
      "[step: 45900] loss: 0.2749572992324829\n",
      "[step: 45950] loss: 0.2706790268421173\n",
      "[step: 46000] loss: 0.2709208130836487\n",
      "[step: 46050] loss: 0.27265655994415283\n",
      "[step: 46100] loss: 0.2664491832256317\n",
      "[step: 46150] loss: 0.2737816274166107\n",
      "[step: 46200] loss: 0.2686585485935211\n",
      "[step: 46250] loss: 0.26903095841407776\n",
      "[step: 46300] loss: 0.271691232919693\n",
      "[step: 46350] loss: 0.26490435004234314\n",
      "[step: 46400] loss: 0.27129065990448\n",
      "[step: 46450] loss: 0.26883214712142944\n",
      "[step: 46500] loss: 0.2640046179294586\n",
      "[step: 46550] loss: 0.2713664174079895\n",
      "[step: 46600] loss: 0.2669253647327423\n",
      "[step: 46650] loss: 0.263672798871994\n",
      "[step: 46700] loss: 0.270469605922699\n",
      "[step: 46750] loss: 0.2658733129501343\n",
      "[step: 46800] loss: 0.2623463273048401\n",
      "[step: 46850] loss: 0.26941370964050293\n",
      "[step: 46900] loss: 0.266033411026001\n",
      "[step: 46950] loss: 0.2610861659049988\n",
      "[step: 47000] loss: 0.2657396197319031\n",
      "[step: 47050] loss: 0.26800084114074707\n",
      "[step: 47100] loss: 0.26331669092178345\n",
      "[step: 47150] loss: 0.2598108947277069\n",
      "[step: 47200] loss: 0.2642417550086975\n",
      "[step: 47250] loss: 0.2674006521701813\n",
      "[step: 47300] loss: 0.2639962136745453\n",
      "[step: 47350] loss: 0.26105353236198425\n",
      "[step: 47400] loss: 0.2584567964076996\n",
      "[step: 47450] loss: 0.2604381740093231\n",
      "[step: 47500] loss: 0.26468625664711\n",
      "[step: 47550] loss: 0.26562008261680603\n",
      "[step: 47600] loss: 0.2636077105998993\n",
      "[step: 47650] loss: 0.26094335317611694\n",
      "[step: 47700] loss: 0.25923532247543335\n",
      "[step: 47750] loss: 0.2584707736968994\n",
      "[step: 47800] loss: 0.2570849359035492\n",
      "[step: 47850] loss: 0.256009042263031\n",
      "[step: 47900] loss: 0.25575318932533264\n",
      "[step: 47950] loss: 0.25588375329971313\n",
      "[step: 48000] loss: 0.25612515211105347\n",
      "[step: 48050] loss: 0.2561803162097931\n",
      "[step: 48100] loss: 0.25593483448028564\n",
      "[step: 48150] loss: 0.255426287651062\n",
      "[step: 48200] loss: 0.25475063920021057\n",
      "[step: 48250] loss: 0.25407925248146057\n",
      "[step: 48300] loss: 0.2535480260848999\n",
      "[step: 48350] loss: 0.253210186958313\n",
      "[step: 48400] loss: 0.25307464599609375\n",
      "[step: 48450] loss: 0.25318700075149536\n",
      "[step: 48500] loss: 0.2534407377243042\n",
      "[step: 48550] loss: 0.2536342442035675\n",
      "[step: 48600] loss: 0.25369563698768616\n",
      "[step: 48650] loss: 0.25361719727516174\n",
      "[step: 48700] loss: 0.25343459844589233\n",
      "[step: 48750] loss: 0.25320759415626526\n",
      "[step: 48800] loss: 0.25302207469940186\n",
      "[step: 48850] loss: 0.25288456678390503\n",
      "[step: 48900] loss: 0.252821683883667\n",
      "[step: 48950] loss: 0.25284361839294434\n",
      "[step: 49000] loss: 0.2529934346675873\n",
      "[step: 49050] loss: 0.25325408577919006\n",
      "[step: 49100] loss: 0.25359758734703064\n",
      "[step: 49150] loss: 0.25400009751319885\n",
      "[step: 49200] loss: 0.25443339347839355\n",
      "[step: 49250] loss: 0.2548712193965912\n",
      "[step: 49300] loss: 0.25527170300483704\n",
      "[step: 49350] loss: 0.25558143854141235\n",
      "[step: 49400] loss: 0.25573253631591797\n",
      "[step: 49450] loss: 0.25566068291664124\n",
      "[step: 49500] loss: 0.2553803026676178\n",
      "[step: 49550] loss: 0.2548855245113373\n",
      "[step: 49600] loss: 0.25418010354042053\n",
      "[step: 49650] loss: 0.25331297516822815\n",
      "[step: 49700] loss: 0.2523161768913269\n",
      "[step: 49750] loss: 0.2512613534927368\n",
      "[step: 49800] loss: 0.2501804828643799\n",
      "[step: 49850] loss: 0.24908216297626495\n",
      "[step: 49900] loss: 0.24802137911319733\n",
      "[step: 49950] loss: 0.24702486395835876\n",
      "[step: 50000] loss: 0.24610188603401184\n"
     ]
    }
   ],
   "source": [
    "for i in range(iterations):\n",
    "    _  , cost = sess.run([train ,loss], feed_dict={X: trainX, Y: trainY})\n",
    "    if (i+1) % (iterations/1000) == 0:\n",
    "        print(\"[step: {}] loss: {}\".format(i+1, cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.336667390952787\n"
     ]
    }
   ],
   "source": [
    "a=0\n",
    "length=len(dataY)\n",
    "for k in range(0,length-1):\n",
    "    a+=dataY[k][0]\n",
    "real_avg=a/length\n",
    "\n",
    "test_predict = sess.run(Y_pred, feed_dict={X: testX}) #예측값\n",
    "length_=len(test_predict)\n",
    "\n",
    "b=0\n",
    "for j in range(0,length_-1):\n",
    "    b+=test_predict[j][0]\n",
    "pred_avg=b/length_\n",
    "    \n",
    "#(절대값(실제값의 평균 - 예측값의 평균)/실제값의 평균) *100 =평균 오차율 산정 방식\n",
    "accuracy=(abs(real_avg-pred_avg)/real_avg)*100\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEJCAYAAABhbdtlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOydd3ykVb3wv2dmMpOeTPr2zTbYZSnb6B1REEUpIiAXRF70YrlWVETEq4jKVVCvKC8iirxIR0REivSFZWGXpe0uy5bsbrLpyUwymZlMPe8fZ2rqTDLPzCQ5388nn2lPOZN5nvM7vy6klGg0Go1GMxKmXA9Ao9FoNPmLFhIajUajGRUtJDQajUYzKlpIaDQajWZUtJDQaDQazahoIaHRaDSaUTFMSAgh7hRCdAoh3hvlcyGE+I0QYpcQ4h0hxGqjxqLRaDSaiWGkJvFn4IwxPj8TWBr5+zzwewPHotFoNJoJYDHqwFLKl4QQC8fY5BPAX6TK5ntNCFEphJglpWwb67g1NTVy4cKxDqvRaDSaoWzevLlbSlmb7n6GCYkUmAM0J7xuibw3ppBYuHAhmzZtMnJcGo1GM+0QQuybyH65dFyLEd4bsUaIEOLzQohNQohNXV1dBg9Lo9FoNFFyKSRagHkJr+cCrSNtKKW8XUq5Vkq5trY2bW1Jo9FoNBMkl0LiMeDSSJTT0UDfeP4IjUaj0WQXw3wSQoh7gZOBGiFEC3A9UAAgpbwNeAL4KLAL8ACXGzUWjUaj0UwMI6ObLhrncwl8yajzazQajWby6IxrjUaj0YyKFhIajUajGRUtJDQajSZdtmyBF17I9SiyQi6T6TQajWbqISWsXh1/Ps3RmoRGo9Gkw3sj1iydtmghodFoNOnQ0ZHrEWQVLSQ0Go0mHZzO+PNQKHfjyBJaSGg0Gk069PXFnw8M5G4cWUILCY1Go0mHRE2ivz9348gSWkhoNBpNOjid+CnAhzVZq5imaCGh0Wg06dDXx1o2UU8HdHamv/+GDbBzZ+bHZRA6T0Kj0WjSQDqcvMth6sVpp6WfK3HssZEDTY0cC61JaDQaTRrsa7PGnqc9zU9BH4YWEhqNRpMGTR3FsedOKtPTCHbsMGBExqKFhEaj0aRBT3dcKHRSB2536jtHhcTChZkdlIFoIaHRaDRp0O2Mu3K7qE3PhNTcrB5nzcrwqIxDCwmNRqNJFZ+P7sGS2MsdHJReGOyBA+qxoCDDAzMOLSQ0mnzD7YarroLnn8/1SDRD6eykh2psliAr5ru4iW+np0m0tvJ1bub/HTjFuDFmGB0Cq9HkG7fdpv48Hjhl6kwmM4LOTrqpYVaVjzOO6ef2/bOhb1/Ku4dbWvkVX4fdcImBw8wkWpPQaPKNvXvVo82W02FoRuDtt+mhmupqQVWtmQHK8PekrknsbxYGDs4YtJDQaPKNri71mE7UjCY7PPggbdaFNCwqoqpe+RV6W32jb79hA/z5z+p5KMT2zmoAykxTpzCgFhIaTb4RLfXg8eR2HJrhHDhAq5jN7NmCqgVlAPTuHUOTOPZYuPxy9Vt2drIzvAiABZYD2RhtRtBCQqPJNzo72cQauh3mXI9EMwS/N0Snr5I5c6CqQWVe9+7qHX/Hl1+G1lb2sQCACqZO5rUWEhpNnrG3zcY6NvHF976Y66FohtDuVtrD7NlQVaXe690/gukoHIYvfSn+essWOHCA/cwHwC+nTszQ1BmpRjMTCAa5x3EmAO7A1Imlnykc8CrJMGdOXEj0tHiVUDAlrLk3boTf/S7+es8eKCtjP2sB8IenztSrNQmNJp9obqZZzgWgUjpVP+UvfQkGB3M8sBmO3w8vv8x73sUALF0K9fXqo/b+InjyyeTtu7vV42mnwYIFsHs3PPEE+0yNAPiklamCFhIaTT6xaxdtqJINrkChcnr+7nfKpq3JHV//Opx4Iht9h1NV6GHJEiguhpoaqfwMmzcnb//uuwD8ct19nOJ9ArlzF4NPv0RHuA4Av5w6WuLU0Xk0mpnArl20sQaAgWBhXDhYp87Kc1ry4osAvMNhrJ7djhAqSmn+fMH+noXg3hvf9rHH4NprAfjWz2qAGtYznwbUb2gxhfCFp87vqTUJjSaf2LqVNjEbgIFQIQxEnKI6HDa3uFwADFCKvcQfe3vBAtgnFibntES0ikf5ROytFzkp5rReWtWLH6vyY0wBtJBIlyeegPnz4eGHcz0SzXQjFCL8t7/TTgMAAzJeSE4LCSAUgmAwN+eOCAkPxRQVxUuFL14Mu8MLcTkSxlVSQjv1nMOjsbd2cFAs/HVprQMfNggEsjP2SaKFRLp84xuq3O/550/JBiKaPOa669jfaiYYCY90UcZ+5nEB9zPgmBoTShJ9fXDeebBrV2aO97GPqbCiHCAH3DipwEsRxQlC4pxzwEchf9+5Ir6x18tWDgHgE5+ANWvgA5bxGkdTVhxkcU2/0iT8/qGnyUu0kEiXZcviz99+O3fj0Ew/WlrYbloJwKrFfQxQyme4hwe5gA3vleV4cBPgxRfhkUfg9NMnfyyXS0UQdXbmpDf0r0JfwY6TTuopKoy/f/TR6nG3wx5/s7tbNSMCfvpTlXS9jRU8wrmccZyLkqIwPmxI/9QQ/FpIpIslwde/ZUvuxqGZfni9bKs5EYB1p5YzYCpng+k4AMwBby5HNjGiGsTeveAbo75RKmzdGn/e0zO5Y6WL389d4XjN1uIEK6DJBEWmQQa88Xlhy85SLuZeAOrq4KyzYIAyeqjhiMPBZpNITAQ9WpOYlgy0D/DG6i/Qf/CRsH17roejmU54vaz3rWPePFiwUBAMmwmF1S3q7p8aTs4kEs2xr78+uWNFK+MCtLZO7ljp0teHh3hf6+Li5EqupZZB3L64kPjHruWx53a7SpWIUjPXhtWq9vd7cuRfSRMtJNLkm9uu4Mg3b6Pi/Y10/H0D/OMfuR6SZprgcFl4ynUMZ58N1dXJn03JgrBvv83g/Ih5dtu2yR1r3z62soInOBPa2iY/tnTo68NLUexlUWlyTa0Si58BfzzvoWywK/bcZEo2PtTMKcQWMVf5BrS5CSHEGUKIHUKIXUKI747w+XwhxPNCiC1CiHeEEB81cjyZoNlTFXv+KsfC2WfncDSa6cTNez6JL1zAlVfCiScmf+YeyL4dflI4nex6vZei/Tv4Nf8VL38+UfbuZSVbOYsn4i1As0V/f7ImUZo8bZZafbj98byHvoHRCzPW1Jm0JhFFCGEGbgXOBFYAFwkhVgzZ7PvAA1LKVcCFwO/IZ3w+KgLdsZevEfFa5cCRppl+7HPXML+om8MPh4MPhkMOUU5PALdnijWr2byZjXIdAF/j18jOyQmJ5nccsecexyT9G+nS14ebuCOi2JRcIqXEFmQgGG8Q1eVRAuWKK4YfqrYWbIXqt/R5QgYMNvMYqUkcCeySUu6RUvqB+yAhu0QhgfLI8wogy8bGNGlqopM6jl3ayeGHBHnPskq9P9lVkkYDeIIFFBcoZ6YQ8NZb8Mwz6rMpJyS6uthGfE24fdckylDceCPvvxp3Vrf0FI2xceYZ7HLhIx7SVDQ32RZYagsyEIyMyeejK1DJspoe7rhj+LFqasBmVYtK/+DU8DMZKSTmAM0Jr1si7yXyQ+ASIUQL8ATwlZEOJIT4vBBikxBiU1cuJ+Tdu+mgnvrZFpYut/BE8MM8xykTjwP3eOCppzI7Rs2UxRssoLggbqe2WKCoCARh3INTzH3Y08N7rMRiURPi63vrJn6sa69lD4tiL5t7S8bYOPM425Ijy4oXz0p6XVIUwi2LVLJfTw9d1FJbkRy5dMYZ6tFuB2tE6fANTg0LhJFX3khLn6H/lYuAP0sp5wIfBe4WQgwbk5TydinlWinl2traWgOGmiK7dtFBPXULi5irCnVyGs9NXJP4yEfU1RPtRKaZ0XiCNooKkk0QQkCJyYvbO7UaEIW6enmZEzjvHHXLt/dOolZRQQG7WRx7mW0h4ehInvCLhigypcVhBihVi77ubiUkqpK1hIceUsFeFguYLWqKCwW0JtECzEt4PZfh5qQrgAcApJQbgEKgxsAxTQrf7hZ6qKZhQSGzZ8ffl13do+80FuvXq8f+qdOlSmMc3rCVYttwZ2aJeRCPf2rV4nxnhw0HVXz8EyZKCwbpcBWPv9NolJWxs/Ej1NQogePyZvd/4dydnJcRGuJKKC2RymcxMAA9PXRTQ21d8hq5pCSeh2suUJ9pIQFvAEuFEI1CCCvKMf3YkG32A6cBCCGWo4RE3hr49+xWSTBLlgq++lU452x1Q+/fNYGkmMS6LboujwbwhAsptg13ZpaYfVOuAdHeVqU5HHII1BW76fBVTOxAoRDP9q7i0abDWbNaRt/KKs6dako691z12m5P/rykRChNwu0m3BURErNG1/y0JhFBShkEvgw8BWxHRTFtFUL8SAgRjRv9JnClEOJt4F7gs1Lmb6jQB/uV82rZMlW5+ctfVSuaPU0TcCp2ddHKLH7BN5FuLSRmPKEQHllEkW34xFFS4MMdmDqlpXnzTbpfUhnSNTVQX+6lI1Qzsaxrp5MP8W8AVqyIrMCzLCQc+5Smf8MN8N57qhZTIqVlAjclqr5Ts4sQFmrnFo5wJIXZEvkewbyd6pIw1BsmpXxCSrlMSrlYSvmTyHs/kFI+Fnm+TUp5nJTycCnlEVLKp40cz2TZ2alWQ0uXqtdR90h35zhX7de+Bt/6VvJ73/42q3mTq/kFB/ZPjVA4jYH4fKp4XOHwiaPY4sedEGKZElLCz36Wm6oA999PF+rmqK6GerufDupVwb906enBjNLYL7woB0LC78fZq05YVaU0o6GUlJmQmPD2eulqVuGxNfNHN6+ZCyKahH9q3PdTLGQit+ztq6TS5ompm1Eh0dU1jibxzDPwyivx134/3HMPHZGS0FmP+9bkH15vpAz18I9slhC+UJp2+P/+b7jmGrj00syML1X27YObbqK7YDYlxWGKiqC2OqSExquvpn+83l7sOPjPj+7nsMPUW6FQFlfgbW000YjNEhyWBR+ltEKZlga6B+lqUwKtdvbo5sGYkNCaxDTD56PFV8u8SlfsrehF09UzjpBob483jwFwOvEmxF27nVMjPV9jIF6v0iRGWIAWWkL4Qmn4JNralJCA4QZ0o/nnPwHoXnkytXVqerFXmeijQtXVTpNQcyu9VFEzqwBzxMwfymai8oEDvMdKls/3JJXXSKSkQn3gdvjp6lDmwrGCMGPmppnuk5h2dHTQzDzm1sad1AUFUGnz0NVjHp51/fe/w86d4PPR2mujzZmwRHQ6lfodwdM/NdLzNcYRevARfBRSXD+8JLitIIwvnIYmkVgAb+HCyQ8uHTo6eI2juXvLylhNvsoaMz4KGcSWdjc257ZWwpipWVIRFxLZtNIcOMBWDuGQg0c/aaldCfABR4CuHjWljikktCYxDZESrr+eFuYyb07yRV5b7qM7UD68fPEnP6k83B0dXMpfuLLjhvjheh00sjf22t0/NWyTGuMYfFitwItWNA77zGYN4wunoUm0txPEzPG8zFO7l2RqiKnR2ckDhcrEtU5V5cBep5zuDuzgTa/kefcOdV/VzCvGFJmtQllcgA/u66CZ+Sw7dHSfUGm1+mzAEaDLqX6nlISE1iSmEQ89hO/Pf6WTeuYsSF7R1dWElVawe3f8zcTVUns7u1lMezBu0HS3u0jE49KaxIxGSjzvquunuGS46dJWIPHJ1KObggc6OIgdvMLxfHnjJePvkEHe/qCIWwavYtkyeDoShlJZqybO33NVstk1Bbqb1L1SE8meMhPMqiZx4B0lpOYtG70USEmVEhLu/hDdTgulFi+Fowc3aU1i2rFtG1xwQazT1OwlyRfLgvnQRGNy1nTCakm2ttHGLNWvOJIb4TiQHPLqdk2Ni0VjEM3NdPUrITCi49qanpDY976XPZEM5VWlOzMyxFR5oUn1cf7BD6CyUr1XuUTN8D/mB2nXPN9/QNmYol1LzYQIhbJXx6pli8qRmDd/9HOW1iiJMHDb3XQNFFJbPnbelBYS041ICGG0iXndomSb8eJFkmbm4XckXPwDA9TTznf4GY6b/4SPQlyUxW4QR3tyNJPHPTUuFo1BrF/PL/kmRbYQp5wy/GObVeLDBsHUNM62tzpizyvkBMJOJ4rPR0drEIsIctFF8bftVfEJNtSXniax26k08EWR0k1KSEx6pKkRDtOyUy34omV4RqKkUmlKbkroopaahrFLqGghMd3YuZPHOYsTUCU06uYk24aXHGQijDmpcRYuF53UcxPfoe1ltZIboDSmajs71UrjoXuUsNC5dDOcf/+bDabjOf3DJhqHuySw2VBCYrxktOuug2OOofX592NvDfiymKl9++10+Cqpq/DF/AcQ1ygA3D2Dw/cbg92eBmaX9sWivswiTCicJU1iyxaafcq5MJaQKC1VjwOUqrpNc8fOaTFblRDRPolpwtuvefk4j8de1w0pZtl4kLog9jbHVw+JWkUbqmLkAKXIfmVfjWZwzmlUJgRdlWNm43nrA3aEl3DEqpEnv5SFxA03wGuv0YoqLDa/uJuBQJpJeJNh40babQupb0wuwJcYhTvQnYaQ8HjYE1rAopp4bbOsahJPP827HMqcWaGYIBiJksjX3cAx7GIJCxePHYmmNYlpRsuB5Bu3vj7588pZyojc74yvClydcZ/Eq6iuMWHMeHs8EArh2PiBOlaDwEwQt39q1eXRZJY391QSxsyqVSN/bisU+LEhvUMm2Pp6uOmm+Ov58wFoti7GapXML3dkV0hs2UJH4XwaGpLfrquDb1yifHYDPWkkjvb20k4Ds2viNv6saRJSwt1382bhcaxZN7b5KCok7uKzuCjn5FPGHp8WEtOMloR2pRC/IKKUVah/YX9f/Ad3dcdvhOv5Ufz9Ha3w17/icKrXdnukDLR/CtXl0WSWgQGe7jsSkwhz0kkjbxLtiex3JUywoZAKlvjOd2Jvrfes5v6Sz3Gn+fMcfrigtDCIO5QlIREOs22Hmc19S5k1a/jHJxyj/ClpJY729NBDdVKms5mw8SGw/+f/QEMDju1t7PAtGFaraShDk+yi3QRHIyYkpkjk+9SqP5wDWrxKSDz7bHKOUpSyiB/blRDVOppKPXDl16gv66Kz9CeYPJLyckGxGMQzxSp8ajLI3r2s53hWNzqx26tG3MRWqCYV30CA2JTvdA7b7oTuv8We/+xn8Psvh9gXLlarYmHw6ruriz+HVLjt1VcP/7i0Si2EBhypC4lQZw+9HEpNXdzZbRZZiG764x8BeJbzkFJw2mnj7/LkvyRnnKnGNVSTGor2SUwzmj01zLV2cuqpcMkIIecxIeEIwmmnwVtv4epVN8LSxuQbwkUZuFw8KT/CUUcJTCYoNPkYTLcuj2b60NTEHhZx0NLRJ4xYT2RXQmilw8GDnM+uaDOeQAArStMwm+Goo6C0WKqAiTQT2CZEayubWMvaxQ4OPnj4x6W1yiw70Jf68tm5pQmJiep58Vol2XRcP8AF2CtCHHXU+NvWN8THZB6nP1RUSKQYrJZztJAYhx5/GbXW0cMIrVaw4sO14wA89xx885tKYACrDku+8R3YeYYP8ZZ7GZ/+tHovq9EamrwjsHs/zcxj0SGjJ2vZiuKaRBTZ6+ACHmQtm9QbfX3Mog2AD31ImUVLS2RSVJ2htLayhVWsOWzkHIFoLoG7I/Wx9Dz0PADVB8fTl7NxvzzLqfyOq3iY87ji8vCoNZsSGRrQMhba3DTNCEoTBaaxf80yXEpLAJg1C1efEg6f+rSJ/R1w6KHwhz/AqTzPXJqpKXLzhS8o54ZZhLOaHKTJL/a/4ySMmcZDRi8tbStWK09ff9wnMdCqIn76iMSXOp04qOW/PrydXz+5HFChmW5KwD2CnTTD9O/uwomdxcuHm8EASsojlVKbUu8p1rNDdXysrklYpWdBSHyIZ2PPjz85tbaxNWn004yZm7TjenoQDJuwjCckLF56qOYz/D+2BZbi6ldCYs1RFjZsgNtui2/bwjyWzXHH0va1JjGzafpAaQeNi0a/BmyLVLqxb8Obsfd6mpPjpkM9TvqpwF4Vv6VLykz4seF3Gh9jfWCLil6ac9DIsaKxXIL3muA3vxn/gD4f7f1Ku0pyXGf5fkm1PqI1EnsyVj5FFC0kphnBsAmzaewfs690Dg/waf7KZ/jGS59goF9tX1auLmaTCe64I779wrXxZYfFFCYY1j/DTGVPswpaiGYUj4StXmkLvpdfj73X267MOjZUkERfs9Is7HXxIIhonwN3l8FCQkpaHn8LgLkLRzZORIVEP+Xw1a+q8vlj0drKw5xHRZGPlSvjbyshkYlBp8aCBalv++678Oab429nsUWExBQxN+nZaRyCYTOWcYSEwxn/N1bRi8uthENiAs4VVyizE8CCxvj2WpOY2TQ5KigwBWO1iUbCFglp8nXFk8p6O5SQKDIpE5SjSZl57HPjMdqxZjjp5CZMhMFBDnSrpfRo36OoCGbPCrONFeqNrnHMTgcO8CIn8bGje5J6bKj7xcBpa4g3OTFbfDxWrhy7+muUmCahhcT0ICQFFlPqS5eKYC8utxkLgdjNHSWqui5JqN5sFpKQ1EJiRiIle9z1LKhwjhkRE42ge7E73juzN1KZvlioyCXHfhWDXTm/PLZNrM+B0UIiEGAvCwGYPXv0zdYdaeLFqnN4npOhu3vMQ4aaW2ljFguXJGsmhi+qEpz8qTisJ8JUc1xrITEOwbAZi3lsIfHqq3Dhheq5w2PD5TVTZvYMC02/5Ra4//7kUFqLSWsSMxank33heSysHbsy6jHHwGENndwXODdWw6W3T0mVooi5qfeAEhZVc+JRUlEhYXjnw0CAf/Mh1szrGLGKbZRjjoGW3hJO5Xl6m8YoPPj003Tc/TQhLMw5JHkpb/iiKiHh6a67jDlFrDOd9klMD4LSPK5P4phj4N574aiGvfT6SnB5CyizDE+oW7wYLrgg7uQCMJvCBMOpRVBophkdHXRQT0Pt2IsQsxnWLHaqPtERM02vSwmAQpRwaG9Vx0hM5EpshmMkQW+ADRzDh5YfGHO7yy6LP+9sGkUwBgLwkY9w4F9vA/H6ZlHMJmPNTdI1gCDMdedu5eKLjTlHTjrsTQItJMYhJE3j+iSiVJcF6AlVMOA1U2ZNrZCZNjfNYDo66KKWulnj34a19Sa6qEV2qCgil1vtEw4DN99M+8a9QLKQKKmO9DlII4FtIvjcQcKYqS4fOzusoQFu+YUaS9eBUXou3HwzAAdQzo2h5iujzU2eHi8SE2UVxp1DC4lpRlCasJhTFBL2ED1U4/KYKbWmtnozemWkyV/czb24KaVuzvi1u2rnFODHRv8eZct3D6qZxisL4Ze/pN06n9LCYFKwRKwZTr+x4UABrxIOBdbxJ9aTT1Pj7t43gibR2wvf/S6vcgyP8zHMpjCLFydvYjYZu6hyOdXMXVoyzoaTINaGdYoICZ1MNw6pmJuiVFcLJSRCrZQVjt2dKorFFMYntZCYiXTtV6aiunnjF+GrPUglC3RtaaHiwgQhQRG0ttK24kQa/Mm3c8xxbbSQ8KgFUSpCIpp01v3c26qe1N698TjTDz4giJnjeBWAD69zYrcP9UmECRl4v7ii4etl42w4CYQAUzZLnk8SPTuNQ1CaU9YkqmrNDFBGD9WUFaVWmMVsQmsSM5TOVnWN1C4cPds6St0ipSJ0vd0KBw7g9imB4KWI7/ET7tt22LAy9lGtwt1jbO0mvyd1TSKaGNdFJFb0nXfiH+7cyYN8Kvby6OOHF740m6Sh98tApJXwWP0jMkFW+2JMEj07jUMoDSFRPUuZDfaxgLKS1FZvZpOxKyNN/tLZpmaJ2jnjaxLR2kBtW9ph7lzcvcrn5aWIO/kcAF/7WvI+0bL2aTX6mQCBQfU9CmzjX8dFRVBSFKKNWbgoTUpE6H/sBS7m3tjrxpXDbT5G+/B8XnXfFhYbe0+aCU2ZqEY9O41DEPO4VR2jVM9VNmAfhZSVp/avtZgkQS0kZiS9PWrxkVibaDQOOggEYd7rVCtwN2oCDWClgwb++z/bOP/85H2sVrCaArh6jS03mo5PAuDEEyS/5SuU42Lbzoi20NTEzofeStpupCx05ZMw7n6JCbxCYyMOVc02Q0+RMfTsNA5BaUldk5gf11FL7am5e8xmYy96Tf7iTGg+NR6lpbDE3sPbHM69XEgz85I+n7dq5Apzc8pdNLsqDfWSRidWa2Fq1/FVX47fG/taIpPxxo3sYknSdvOSvyJgvOYd8ClNIhWtaDJMJXOTdlyPQwgTFkuKPom5cduyvTa1RkJGr4w0+YujT/3uFRWpbX/o7F6ec5zKI5w37LN5i0a+3pbMcrPTuVjVShqr9sckCPhSNzcBNDbGn5eZI3Wl3niDV80nQkglsQ0MjFxcz3BNIiIkLIXGTo1TqRyPnp3GIYgFS6rmpoRqlTUphDWCMjeFpE6mm4k4Bgoos3hSLv9QVx3CSVztOKNsfez50FDRKEsWhdUKff/+yQx1TAKDkdV3iiaaxMnf51ECpuOVXdwa+gKnnQaXXgpf/OLIzfSMFhJBf7Y0iewWKpwMWpMYi3CYIJaUfRKJNeWr56UWaG02S4JaSMxInP4i7FY3MH50E0BVdfKsueDEhdx5nnIGJ67OE1m83Ibjn1X0bmun6phJDngU/N70NInEyCGfJwShEG+8VUAIC9dfP/a+ZhPZMTcZ7pPIQhvWDKE1ibEIhZQmkaK5qSRBLtQsTC2GzmxSJi3NzMMRLMduTb2Mt71uSLG7BXO5/PJ43bCRmL9SFfxr2TpGraRJEjM3pTGxXnyOCsv1ecPwxhts9h2CEJJVq8bez3Bzk1/d6wVF2TA3GXqKjKFnp7EIhQhhnlA1yOo5hSltZzYbuzLS5C+OUBmVttSFRFVDcqjs1q3j7zPvIKWltOw0LlciXXMTwPevVpVpfd4w3Hcfm03rOHhZeNz8hGwJCe2TiKNnp7EIBtMyNyWSajtDi1mZtDQzD2e4nEpb6pN31eJ4TsFHPgI33jj+PtFOaS3txl1jsdV3GkLCVqZ8dr7BMGzbxmbzkaxZN/7+KhrQOFNQzCeRFU1CCwmEEBVucNoAACAASURBVGcIIXYIIXYJIb47yjYXCCG2CSG2CiH+auR40iXsDyIxYbGk/mOWR8r5pxLWCMbbWDX5iy9spagg9RyGqgXxXhEPPgjHHjv+Pg0NqgRES29qfo+JMBE7flxISFpaoDVQx5o14+9ntHk2q+YmY9NXMoZh/wkhhBm4FTgdaAHeEEI8JqXclrDNUuAa4DgppUMIUWfUeCZCyK9sremYmzZtUv0lUt3HbIYQ2nE9E0mnLhhAVVX8eaplIywWaLA5ONBvXJ2JqJCwFqUhJErUDeIbhD+1nA7AWWeNv5/h5qZIXU7DzU2mqdNPwsj/xJHALinlHgAhxH3AJ4BtCdtcCdwqpXQASCk7DRxP2gQHlahPR0gsXar+UsVikdrcNEMJYUo5vBqSI5hGCg8djaoiLw5Paj6yieCPrr6LU8sNgoSWrIOSDQMrOaKulaVLx2hrF8FcYMqKkEjnu0wEs1lqIQHMAZoTXrcARw3ZZhmAEOIVwAz8UEr5pIFjSotgJGrDnIa5KV2U+mxWjQFM2uw0k0hXkygpgXvugbfeGn/bRKqKffS6jKt9HfClb6KJCYm+QbrkQhqqUmuxaraa1f3i9yd378oQwUDku6RYYmSiaE1CMdJ/eeh/xQIsBU4G5gIvCyFWSimdSQcS4vPA5wHmz5+f+ZGOQszcZOCiwmyJCIlQSAuJGUYojbpgUS6+mLQ7plWVB9jdWq6usYlEYYzDROz4BZF7yufw0EUty2tSLIhpiwgJt9sQIRHTJIxVJJSZOTA1hISRs1ILJBWYmQu0jrDN36WUASllE7ADJTSSkFLeLqVcK6VcW1tba9iAhxLVJNJxXKeLxZwgJDQzilAaDa0mg70ijAN7vFhUhpmIkBACbPjw7W+ni1pqG1Nr4GC2WtT9MjAwobGOR8wnYbAFuMAiCQSnxqLQyFG+ASwVQjQKIazAhcBjQ7Z5FDgFQAhRgzI/7TFwTGkRDYcz1NxkUaU/tJCYeUw0vDpdqmot9FIFTU2GHH+idnybyY8DOx5KqF2R2uLPbMuOkDBakygrDNAfMi7iLJMYJiSklEHgy8BTwHbgASnlViHEj4QQZ0c2ewroEUJsA54HrpZS9hg1pnSJaRIFRva7FUhMyMAUiYfTZAyVqJkFTWJ5PR5K8L200ZDjB6J2/DTrHdlEINbLOlUDQWm5iQFKCbtGaH+aAYJBMBNMKzBgIlSW+OkLl4HMf5OToUqVlPIJ4Ikh7/0g4bkEvhH5yzuy4pOINkX3h3SM00wiWhcsCxaHqkaVhNf7ynZmGXCn+f1qRk3XRWBjkCdQca+puhrt1YIwZlydXlIsnpsWgaDAQhCjy9pVlIToowK8XijOb41iahjFckTU3GQpMO7fFLV9RrUWzQxhEiVf0iXavOfdncaEwU7URGOrVn6I448Y4LTTUtvHXq1WVY6O1HrIp0sgKCgQxmv1FWURIeFJvSxLrtBCYgyyEgIbOXZUa9HMECZR8iVdjjsOLCLI8weWGXL8QKSaabrfxWtRQuLMT5WmHNhXFSly6OgMpHeyFMmekJC4KCM8oIXElCYUiGoSRvokIufSQmJmEQoRwpIVTaK0FFZUdbC1f4RWbxkgEBAU4E/bjt8aiXU8+ujU97FHihz29hhTQjUYJDtCogIkJlxdxvYfzwTaDD4GQV8k49pqoLkpIoC0uWlmEQ5EtNQsVWRpsPvo6KlSNvCiooweOxAyUUAQSM8pcdNN4HLBKaekvo99ljKZOXqMcfgGQiYswvga3pWR2m7OTr8hvpVMooXEGAQ9SqU1F2U+aSeK1iRmJhMp+TIZ6mrDfLCrDtra4k6KDBEMCSwTWH1ffXX656qaowSco9coISEoMGVBk7CrG7+vyxjfSibR5qYxCHqVkLAYWBFS+yRmJtHf20h/VyL1S8rpoB75hzsyfuxQSFU1zQb2OuUd7zVKkwiaKDAZfy+WV6jfvd+Z/52HtJAYg6iQMBcap0lYbGpFEfKkVrtGMz2IBUVkydxUt7IOL8UM/OvljB87GI6GjRpPcTEU4MfhNEa4BsMiK0IiWgU32vo1n9FCYgx8bnXhF5YaqEmUKEdcsD//oxw0mSOeg5MlTaJePbbtHMh4AlcoJLKmSQgBVZZ+HP3GSNdA0ITFZPx3iZVKd+d/Em1KQkII8YwQojLhtV0I8ZRxw8oPfB51IxeWGZdNZy5WjrhQvzEZpJr8JF7yJTvnW7cOzKYwv/JcCR0dGT12KJw9IQFgt3nodRuj3QdCJgrMxn8Xa/H00yRqEiuzRvo/5FWDICMY9KiLpbDcQMd1sdIkQgPG9SDW5B/xhlbZ0SRWrICPrOnmVY6F3bszeuxQGCwie5OdvXAQhzezEVpRAiETBdnQJErVwtPnnT4+ibAQIpY4L4RYwPCy39OOqJCItlo0AluF0iQGnfkfL63JHNl2XAMsbgyzh0VIZ19GjxsMmbKrSZT6cfiNKWURCGenMq+1ZOoIiVSV3WuB9UKIFyOvTyTS32E6M+g1XpOomhVJDurKf7VTkzniJV+yJyQWNYKLcnpafdRk8LjZNjdVVYTYvq9cZb5lOIbYFyqg0JIFx3Vk4ekfnCZCQkr5pBBiNXA0qpnQ16WU3YaOLA/wedWKwkjHdfVcpTb3dE97xUyTQC40iUVLlbN3z17TlBYS9mqTKn3e0QFz5mT02P6whYqC7AkJ32D+3/epOq4FcAawWkr5D6BYCHGkoSPLAwYjFqBC49oDU92g1M4ehw40m0nkQpOonRstaZHh6KawyKpPoqrBRh+VBPe2ZPzYPlmArSALjuvSiJDwTRMhAfwOOAa4KPLaBdxqyIjyiKiQiPbjNYLqavXY48xSwLwmL4jWBcumJlFaq7TWgb7MTujBsAmzyN5kV7dQfY/u7V0ZP7YvnB0hEcuTmALpUakKiaOklF8CBiEW3WScoT5PGPSpG9hIIVFcDIVikJ5+g1thafKKWAhsFjWJUru6xgb6MzsJhsICcxYigqLULSkHoHNnZh3wAD5pxVaQBce1Tf3uvmkkJAJCCDORiCYhRC2Q/x6XSTLoU9Utjc6Krbb00dOnNYmZRLzCcPbMjGWRNtIDrgybm6TIqiZRHxESHXszHDYeDuPDhs1q/Hcxm1UHvOkkJH4D/A2oE0L8BFgP3GjYqPIEn19QaDK+AFddhY/2DpOK1tDMCHJibipVjwPuzJ4zFDZhyUIpiyh19Wr8He0Znsz9/qwJCQAbfvz5X98v5eime4QQm4HTUNFNn5RSbjd0ZHnAoN+EzWRMc5NEliyGLRsbYetWOPxww8+nyT0xx7WBZeiHYrOp1WumhUQwbMKchdyCKNESI509Gda+fT4lJAw0LydiFQF8/uwtEibKmFeoEKIq+gd0AvcCfwU6Iu9Nawb9pqxoEgcfDE004m/pNPxcmvwgpklk0dwkBJSavQx4Mzu5hmR2fRIVFWAVfjqcGZ7No5pEloSEzeTHH8h/ITGeJrEZ5YcQJGdYR19ntjB9njE4CIVZiJk+aLmJEBZ27wiw/CzDT6fJA4IBdTtlU5MAKLUMZl5IhE3YTNnTJISAKps74/WbQh4fISzYDAx5T8RqCuILjPP7e72qIGOxMRnmqTCmkJBSNgIIIUzAZ4BGKeWPIiU6ZmVhfDnF5w1TWGz8CmnOEhXS19ESZLnhZ9PkA7nwSQCUFQZwuTJ7zKA0UZJFTQKgushDrzuz9Zt8A8q0bLNl5zexmQLjC4klS6C7O6dhUKkuY25FZVsn5kn81pAR5QuhEIM+ga3Q+Aumcq7yKDo7p4AXS5MRcqZJlEgGBs0ZnXRC0oQ5i5oEQFWpn15/aUbLnvtc6v7Lxj0PYDWF8AfH+f1bW8m1d1vnSYxGTw+D2CgsyoKQqFdGUGePrt80U8iFTwKgtNyEizLYvz9jx8yJkKgI04sdujKXUBft7WArys5vYrOkYG6KksHvmS46T2I02tsZoJSSMuMvmMpIpw6nI/9T9Edlyxa49lr405/U6ww3tpluhIK50STqZ5lpYxbs3ZuxY+ZESMwtVvWbNm3K2DFj5qYsaRK2QoEv1X4S23MXTKrzJEajowMHduzVxie5lZeDIIyjbwrXbzrpJLjxRvjc5+CWW8Bkgp6eXI8qb4mam8zW7CZRLlpqZi8LCbZlbmUalGYs2RYSS6uVkNi4MWPHjGkSxdn5TazFBcrq5x0lKTBxoeV0jrxNFkhpVpJS3gN8G/gp0IbKk3jQyIHlnL4+eqmiqsb4idtkgnKzG6drimZdOxwkeUO/8Q31+NpruRnPFCBX5qZFB1sJUkBLU+byf3KiSTRY8VDC4KtvZuyYMSGRJU2itk7QSR3s2zfyBgMDXMLdnMXjU8JxjZTyfSnlrVLK386ERDrZ14+TSuw12Zm4K60enJ4p6uZ55RX1eNddye9v2ADhMLz/fvbHlOcEfWpCshRn9zdffKgKpdzdlDnhFJLZTaYDWLhQPX6w0ZEx02a0AZC1KDv3fONSC000Et61Z+QNnE7u4RKe4CykL3fO6yls3zAWV7eKmbbXZafwXmXhIM7BLGXxZJpXXmG9+SRO/eNn+MzBm/H/9Je8Oftj8OqrcNVVsHw5PPccvPwy/P73uR5tXhCKCIlo+9ps0TBHTYCdHZmb1EPSjDnLM8natepxk2tZxpzwUSGRLcf1osNK8FFI+9uj9BxPMDHta8vdAlILiVHo7VQ3cbRznNFUFgdwGtSS0XA2bODE0HM8/5KZv76/mos3fYM1rf+g6bUOuPtutc3TT8OJJ8IXv6hrVAHBwUiP65LsCgm7XT06ejMnJIKYs9LyM5ElS6C8OMAm1qow0QwQ7RKXLSHReJgqVLjnnYERP09sM7t9X+7mBi0kRsHRoy4Ye61xXekSqSwN4gyVQcD4WlEZJRSi6429yIRL6eGH1eMG7+Fxp9zPfx7fpzSz8e1TkaAvIiSybG6KRtI5nJmzu+fCJ2EywdpDBpWQaG/PyDH9kQZA1sLsTIvzF6jfoGXPyKakvo543/tBb+7uFy0kRiG60qrKUoWqygqJk8qcRjFMiK1b2eA5DIBHH03+6FWOJYCFl0/4XlJNF3y+jK3+pipR2VlUkt1bsLAQCk0+HP2Zs7uHMBteTn8k1q4TvM3hGat5FvCrq7SgMDtfZvZs9di62wOh4aGwnV1xQZ7LNqdaSIxCr1P9a6LqudFU2lFCwuHIzgkzxcaNbOQoLBbJhz8Mv/kNPPssHHec5FWO5RP8nRNf/glP/fA17vvMY3yPn6j9tm7N7bhzTHRlaGRr3NGwF3pxtA/C5s0ZOV5ImrJubgI49Khi/Nhoej8zkT8Bn7IeFBRlx3pQUQFF1iCtjiJljh1CV29cWPlz2OZUC4lRiK60siYkqsz0U0Goe+oJidcsJ3DYYVBUBF/5Cpx6Kpx6qmALq/kXHwXg4ZajuOiej/NTvsfznAzvvJPbceeYaGvcosyWH0oJ+9wStSB57rmMHC+IJSeaxOy5avpq35+ZyJ9saxJCqO/QyuwRkwL7++PPp60mIYQ4QwixQwixSwjx3TG2O18IIYUQa40cTzo4BlRUU9bMTZHSHP37ppaQCO1t5g25hqOOSrZxH3dc8nZ33BF/firP4//3S1kYXf7i9QkEYQpy0LXWXluAo6AOdu3KyPGUuSn7k1hDg3psa83MubOtSQDMnmOi1bYItm0bdTwAfv80FBKRMh63AmcCK4CLhBArRtiuDPgvIHOpkxmg122jQASyVqG3cqHyKDp3jBIOl6ds7ajBFSrhqKOS3098/T//M3y/Pc/tndFRToM+E4XCh8hBOwG7HTaFV+Pe0TL5g0kZERLZ/yJRIdHelZmVfzRmJFuaBMCKFbApeDjOd5uHfeZPEBK5bHNqpCZxJLBLSrlHSukH7gM+McJ2PwZuIlI8MF9weG3Yre6s3cSV81U4nHNXd3ZOmCEe71gHwOmnJ79fWQnz5sGqVfCtb8EHH8Dzz8c/fz+wKG5zmYEMZqk17kh86lPgCpVw71vLYfdu+L//d+IHC4UI5SAEFpSws5oCtDky49iJVeYtzJ4mccUV4A4V8dj7y4YtmgIJfgifL3fNiYwUEnOARPHYEnkvhhBiFTBPSvm4geOYEA5vEVVF2ZvEKu3qIujd2zfOlvnFK65DWWlviUVqJPL++yrpGmDpUjj5ZOiLfL13OTS3y6Mc4/WbKTLl5vt/5jNQYA6xp68KVq6E//zPiYeRhkI580kIAQ2lA7S7y1Rm/ySJ+SSyaG5atQqslhBbQwfBuecmfZborM5ltXAjhcRIoi/2rSONjG4BvjnugYT4vBBikxBiU1c2SuZKSa+/FHtJ9m7iuXPVY/Mr++Gee7J23snSFyimrsQz4mfFxQxrBVleDusWdPIon5zRQmIwYKbQnJucGLMZ5s8J0URjXJt7662JHSwUIpQjIQEwyz5Im6yH+++f9LECEU2ioDh7jiKLBQ5a6GMbK+Af/0gKhQ0k+CF8OWxzaqSQaAHmJbyeCyQGx5cBK4EXhBB7UU2NHhvJeS2lvF1KuVZKuba2ttbAIUdwuXBQib0se/0d5s8Hkwizh0VwySVZO++kkBJXqJiy4vT+Tx89opU3WcOga4olDmYQJSRy55NpXFpAk0joPjzBktthf6S8SJY77EVpqJO00wAXXzzpYwUiq/Vs+iQAVhxhU0ICVBe62HgSNYnpKSTeAJYKIRqFEFbgQuCx6IdSyj4pZY2UcqGUciHwGnC2lDJzBeInSm8vDuxU2bPXMsNqhXk1XrW6g6mRee3x4KKM0pL07NHlper/Gu0ENhPxBgootOTuN16yVPC+WM5NXI1AErzzLxMy2cQyxy25ib5pOLyedut89eK99yZ1rEBEZhdYszshrzjUzB4W46QC2tpi7yeamFJuTmQAhp1ZShkEvgw8BWwHHpBSbhVC/EgIcbZR580IDge9VGGvyu7FsmhFIZvFWg4wGzqmQJRTfz8DlFJWmt4EES3F7B+YuUJiMGShqCB3msTpp0NfuJzvcBMAu5pME2pEFPIrIZGL6CaAWXPNdPkrCZRXww9/OKljBSImnWyHJa+IKBF2nElCIqpJlJg8+IPTU5NASvmElHKZlHKxlPInkfd+IKV8bIRtT84LLQIIdfXSRyX2muw5sABWHmZmu1zOXA7AgQNZPfeEcLlwUUZZeXoXcLQUs98zg0NgQwUUWnLXrva005Jfb+UQ2Lkz7eOEXMofZS7MQcIHELU+n2l7FnbsmNSxosp7tv0ry5fHn/v2xxeH/sh4Ss1efMHc9ZrRGdcj0NesUh2rGrJbfO2II+LPZXMGYtgNJuhwMUgRZRXpXUbRKpvRdpEzEW/ISqE1d0KiogKq7fHzv8bRcMYZ0Nub1nFCTtVsKtslz6OsWaMen+06nGDX5BJRAwEowJ/13JXly2HeXKU1dG7YHevPEvWRlFoG8QenoblpKtP7lqpPb1+cpXTrCCefHH/ed+OtGQnrMxLXfnVTltWkJ0yjVTZnvCaRpQ5oo9G4OL46/Q3/RT9l8O67aR0j6FRlrrNdzTbKUUfBvfeq5493Hz2p6sKBoMBC9q9Jkwl+93t1LbTf9SQcfzzs348/IDATpNAcwBfMrlUjaXw5O3Me43hbtRO0N2R3dbRoUTz6tXVLO/zyl1k9f7q43lcmsbL56QlTa6SHsM+Tu5V0TvH5GJQ2CotyKyRmzVKPS5aAHxs7WQpNTWkdw+9wA2Aty13DrE9+EpbV9vLL0FdhYOTeDKkQDAkKRG4WLrHscSJP/v3viGYTwGYOanNTvuHYpVTubNVtSiSaL9G67pNwzTV5XTrctUvZT0vnVKS1n61YrYpmrCbR14eHYoqKcyskoprrt7+tHs/mMQI796Z1jGjPg4ra3HVOKyyE1Y1OOqiHnp4JHycQzCMhsX49/qAJqwhgtYTxh7SQyCv2upU3rKYm++eeE8lJv9X2DZVYk1jLIs9wNamY7gk7rr0zVJPoV/3Tow2AcsXXvqYusYsuUq9bmcPb97wHnpGTI0eir0sZzsvrctt6t7pG0E1NUp5BuuRSSNTXQ0kJ/Gz2bzileCO+93YSCEKBCGKzhPFpIZFf3O/5GEsqOlm2LPvnXrQIPvc5eHR9Df8uOFP1hc5T9rSpOtfz5o2z4RBsJUqTmKnmpsEu5fCvrMrt7Wcyqb/S0vh7+/eF4de/TvkY/b1qUq1oyG3r3ep6C04qCf18hGqSKRIICgpMubkmCwrgnHNgT2sRL3iOZOPWUvwBE1ZTEGuB1JpEvrEjuIQT5u3NSYVOIdQ9On8+fCp8H4ObJpcgZCTv9i/AIoIcdFB6+1lLVLikf3BmCglnq1qp22tyd+MPZfdu9dg0/2T4179S3q+vV/2GFfU56J6UQPXKWUhMOJ56Y8LO60DIlDNNApILLbzkWUPAMaA0CavEF8pNiDFoITEcKfFhpciWu8ii0lLV4c0ZKmfDm7a87Qf9nqeRg8sOYE3THB3TJHLYtzeXONqUHd9el7sbfyiNjaqu1j+8pyHbU0/k7HOq37DCntuppLpeXVM9rgKVFPj88/DZz6Z17wRCppxpEqByV448Uj3fzBoCfW6spiA2m2RQWnM2D2ghMZRgkEEKhxWmyzanngpmU5jn3EfmbUvTzoCd2cXpV62NaxL5HeKbbs4AL76oloPj3MyODmXHz3b03FgIoRzZL3YdwoN71sSbcI9DX79St8vLDRxcClRXq8e/cQ68/rq6ge66K62kVKVJ5E5IWCywcSOcfz68V3QkfqwUmEIUF0q8FOWstL4WEkPx+fBhozDH929ZGayY08dm1kwqYsNIfOGCCSWEWUuV6pHXQuIPf1Azz//+b+r7nHyyimEeR7g4upRJo3JWDnqXjsEjj0Cj3cGfQ5fE42PHod+lhERZmZEjG5/GSMmza/gZXHhh/IM0eqkHQiYKzLk3ga5cCbsHZ+OkEuugi6KiiJBII6Agk2ghMYSw10cAa841CYA1B7nZzBpkd/4KCVtB+hO9rVRpEr4cNncfl2i/1W99C269FVyu0bd95pl4fQgYt+6Ws1tlmtvn53hmHYLZDIfN7+MAc+KNP8ahz22hzOzOWanwKAcdBFddpZ73YmcTa7iWG5DvpJ4cGAibsJhyv3A5/niQUvBvTqdA+igqEngo1kIiX/D1KZUu14lOAIcsD9FJPX3787MRkRIS6U/0sRDYHDZ3H5emJtrO/JzKNPvyl+EXvxh92z/8Abq7Wc9xXM6d49r0HT1qIrLX5i6LdjQayj1sZzk9pJYk1Oe1UmHJzeQ1lLMjZUPfeaaTE20buZFr6XpsQ8r7B8O59UlEOeUUFeUIYLVIikuE1iTyiWiPA1uOSyYA1M5VESM9+905HsnIDEobNmv6E31US8vbnkMuFz/s+iKz//VHbrn8HaX/P/TQ6Nvv3Ytr/iGcwHr+zOU4TjsPHn1UGfq3bx+2ucOhri273agvMHEaTlhKACs1pKa99g9aKbflRxvadeuUNvTxcyx4fWohsm19b8q9MpS5KfcLF5MJrrxSPd9ft4aiEhNBCgj2ayGRF/j61cxVWJz7f031/BIAeg7kx02YhJT4mJiQiEZD5a1PYs8eHkG1kvzO98z8aNbv6d7WAaN0RfzT+8dQsf+d2OsO6lXQO6gKdJdcktRxzNFvpsTkyXpJ6lSom5MwqOD44aB9/kIqCvND2ldXw2WXJVfm2FayFm6+OaX9A0FBQe4Sx5M47zz12NJqpqhEzUVeZ27+z7mfCfOMwQF1Y0QrleaS6nkqQamnPQ+rpfr9SkjYJqFJePNTSMj3trKbxVx2toNVq+D6Z47nf7hahZ4MVX88Hv7mOg2ZcCv9izMJI3iFY1WU0D33wBNPxD53DBRQWZCf2mGSRaO/f9zt+wIlVBTlz/X5+9+rCNgHH1TX2VOVn4YXXkgpfDQQNFFQkHsLAigrZ5TiMqUVeZy56b+S+5kwz4iWry4syX2iU3Wt+nl6mvPD5pvE4GBESKS/q9kMgnDeahJtr+zBQwnrTivntdfgxONCPCzOh49/XNmIEifPfftoopF1i+LmmW9yM9/mJo7nFe7nAtVx7KKLVEOZK6/E2eXHbkstxDTbJHXOHc95HQ7TFy6lojR/anBZrbBggQoj/eY34fHW1bzZ1gD794+9o5QMhAopLc6Pa1IIeOMNFZxVVKZ8V1qTyBNiPol80CQisd89e0dY0blzuxINe30EKZhQqLAQYBN+/L78uCGH8sEbanJcerAZIeDcT5nZLRfTwhylGWzbFttWNu2liUaOXeNLbCrGL/kWABdyP3acDLiB2bPhjjtwYMdenB8mmqE0NMB/X6i+X7B7nOKSbjf9lFNRlp+/47e+BRWlIW7h6+NXt/X56Kec8tLcO66jrF2rutYVVaqbzNuam3yp3M+EeUa0nlA+aBKVlWASYXrag8k9rx0OlZZ99dUTO7CUKuFoEhmc0f7UE3Xw28xBBgfzQ7VPQkre2q5uysMPV28dc4x6fPCaN3mEc1QHtGuvhV/9iu6tHbgppfGQYurqRj/s43wMgC5q6KIWe3UefvcIFVVq5drXOsJC5CMfiZeN7e+nj4qcJ9KNht0Opxzj41WOHV+T6O9XQiK/opIBKG6sB8Dz92dycn4tJIbgc0d8EqW59yqazVBfMci+8FzYty/+QWurevzFL5Icoinh88H3v6+6tdxww9jx/2MdJiokJqhxlVh8uH35FwJKeztvupcxu2KAenVvcsQRaoX9jZ/WcR6P0PzZ78ONN8KNN9J010sANB5ejskEv/0tPP308MNexH08yPnU0cU2DsG+PLVktVxQMVsFTPRtS8hWDgRgyxb15f7nf8DjIfCpi/FSnNMy4eNxzMk29rCYrm0jBx1ECTldDFBGeUX+Ce+Y43rL+6MGTxiJFhJDGHTnjyYBcMQyD29xRFJ5AV9nH+fwCJtYA4ceqjSClhb4+teHp+47Jv3B4wAAGC5JREFUndDeDnfeqYRCYaGa4AB+8IP4MjlNJqtJlBT4GfDnXhAP4+232cwaVi2Pm4OsVnj1VTjpJPV6O8vpti+lpytE01a12m5crG6lL30JTj9dKRuf/WzyoS/gwdhz+9wSQ7/GZKhYPhuAvocSVq7/+Z+wejXvcQjNzIXPfpa+DSqbuWLJGCpUjllxuLrGdm4ZuxnRQLv6vLwy/6bEokhivpdCePLJrJ8///4jOSZqbrKV5cfqaPUq1aR+cE9r7L29H/h5lHNYxyYVh//mmyrd9Fe/gu9+Fy69FI4+WmUCz56tSixccQVcdx0ArcziQ1VvsovFyjOWrjbC5KPASq0B3P4M/I87OlR2dIaKn7lf38r7HMyaE5Mn8cZGuO8+9fwXpz5BreMDTi9aT1PRitjniSxbBrfcAl/4QvwmT+SUUzIyXEOoqFSCv+/NXfDjH6vB3nknAIfyHo00wYMP0oLqkFW5KAfduVIk+rs0PbML3n9fxce2tSlzbULEmetVlZldXpV/2m30+uk7+1KVs5NltJAYwmAkLLOwND8uluVrSwhhYd/W+ErI2R7XFjZyJKxdi3z8cT5gqaozfvfdKlzzwx8GrxcJSJQ9/E4uZw6tPNu7iqXs4i/8h9JC0iQaBWabYD5JSWGQgWAGykuvW6cyj156afLHAt5+ppMwZtYcN3xsUfPTM8+pa2OLdznXeK+nuDi5J0OUykq47TbYs0cVbwMVDbtsmTLt5yvRLmnbWaG0zRde4GWO57QjVEOfEBbe/Oj3WcVbACxdln8mmigLF6rHvXI+LF+uSq3eeqsK/DjrLPjYx+CGG+i/RmnX5dX5p91Gm1N97vn/ILByVdbPr4XEEHw9ynxgq0uvJadRzF2qlhHNz7wPl18O7e04O+Px0kezEdfXruOm5X/iID5gM6tp/ez3uPvjD+Aqn8PnGp/nUzyICUkdXVzBnUnHv4y/qFksTWK+m6KJmeVKC0O4w4UQTjEyprkZjjsOdu6Ez39eCcElS5DNzXRRo4rr7dgxobHEePZZNq9X4cZr1gz/WAh49lkVXnnNNfH3v/zlsQ/b0KDcSFu3wsUXq2HmQ22w0Vi+XPlh7ij7Gj2ihjdYy0XiPp57qzq2zZonfhx7noPFbcoUFUF9vWQTa9Ubr79O+Cc3cjNf5wCz4Z//hOuuo79BNUUpP3h2Dkc7MosWwQ9/qNyHf/lLDgYgpZxSf2vWrJFGcsfJd0uQcv9+Q0+TMrt2SQlS/onL1BOQ9x3x0+hTCVKec45Mej3e37e/LeU770j56Y8NSJCy/+Y/pD2uN37zqgQpH/v5tgl9r/MOfV+u4D0pt2+X8he/kNLhkPLHP5by9delfPppKVtbpfznP6X8zGek3L1byi99acQv8+XSOyVI2U2VlLffPqGxxPjNb+Rl/EnWV/tlODz+5n//u5QPPji5U+YrP/+5+hcvnz8Q+3fX1o58PeU73/2uGuelJ+6Rd/A5eR8XSJDyfB6Q6zlWbmZV7LusX5/r0Y5MOCzldddJ+cEHEz8GsElOYM7N+aSf7t+khMT69VJedpmUg4OjbnLT8j9KkLKvb+KnySRer/qVruXH8k9cJn0UyN/zBQlS7tsn5SGHSGm1SinE2IJhyRIpd+6U8pJLpBwYUMd+8IGwBCm3nHVt2uNa/9OXJEj51G8ndtVeesp+uYCm+ABLS0cf/OrV0rX6RPk9bpD9lMo/c6nspVJ+deW/kzZzfvrzExpLlF1f+ZW04ZUfOys0qeNMB7ZtG/4z+P1SdnZK+dRT6rrcs0fKHTtyPdLx6egYcjnVNye9rirzxZ7v2ZPr0RqHFhLj8dvfxq+Kykopr7lm+Da33Sa/wq9ludk1sXMYRF1d8kV+OUqQeTzq80BAypYWKbu7pbztNil9Pinb2tSivLNTyuuvl3LTpuHH3bxZHe+h+i+mPaZnr31WgpQv3LV3Qt/pi1eFZbXFISVIBxUyDPIFTpTdVMk+ymSowi73X/wdeevpf5NuiuR/c50EKU/h2VFlyd9qr5zQWKL8/qR7JUj5yiuTOsy04TvfkfKEE6ScPVvKW27J9Wgmx8KFYy+i/vKX/LEeGIUWEuPx9tuyv26xfIRPynD0yli1SsovflHKk06KXS2f5BG5omzfxM5hEH/+88gXdiomkbFwu6U0m0Lye9wQVy9S5ImvPilByg1/a5vQua++WspCW0j+ct1fpRBh+ekznBKknF3QIYsLgxLi2tFlx34gzz6ua9j3P/nk5FXi9/mRlM3NExqPlFLecOh9EsZUNDVTFIdDyv5+Kc8/Xy26jjxSXTPXXCPlpz41M37ziQqJ/AjhyQJPth7GmZ27ADivfj1n993NpVtuVwlCEf595Pd49PVzOH55bgppjcZll8EHH6hIjXfeUQlboBypk6G4GFY1Onll93HKm7p6dcr7egdU2Gy0ZEC6lJbCoM/EN9+4CID7n1SBAq2BOogklx92mGqLedfLS5P2veoqKClRQU11ddDdDSescfPyvhOUY3vu3AmNqafPQonJg81WPKH9NflLNELor39VSwq/XzV8XLAgt+OaCswYIZFY3fLhjuN5mON5+oL/ZdU6C22vt/DHp+bgfF1F6hSU5EeORCI/+Un8+d69mWt7fcxRkjt3ryW89e+Y0hASngEVlVRcNbEw1sQaeY88oiISDzsMjj1WRfGedJKa67u74frrlYBsaFC/41e/mnys6mr4xIcH+cUfjqN/998on2AOQveAjRqbC9BCYroSLc9utY4ctqwZzowREueeCw8/rMIQb79dpQbc84CVex4AmA/AqlUqyvTMM3M61HF57LHMHWvFMeW4/1pAy9s9zP+P1PfzuCUAJVUT0yQ+/nF44AG46aZ46wVQwi+xz8LcufDHP45/vBM/XMjP/lDAu2+FOG5CI4IebzHVRXlYcVejySEzRkiAEhTnnqsSj/ftU2qn2axi3+fMgRNPzO/49SiTNTMlsvxQNSNv/8AcEZWpERUSxaUTS7U56aSRa65NtBHPnKVq9d+2f+K9DXp8ZVTX5pepUaPJNTNKSCSSaIscWmNnJnHwwepxx/4i0kkCdnuUpCrOE8vMrNlqPG2tcmIH8PnoCVeyqDI/S3hrNLlCZ1zPcOrqoEAEaOtJzw/j8QrMBPOmBWd1NVhEkLauCa57enropobqqgkKGY1mmqKFxAxHCKgv7Ke9b4QqdGPgGTRRbBrMqOlrMphM0FDopM01seqqwY4enNiprtG3hEaTiL4jNDSUuWn3lKVVSdUzaKLElF8tOGeVumhzT6zmlmOfCreqbsgT1UijyRMMFRJCiDOEEDuEELuEEN8d4fNvCCG2CSHeEUI8K4TQUcs5oKEuRHuoNqlnxXh4/BaKzfnl5K0t99Hlr5hQ2fCe/aqwY82cKRC5oNFkEcOEhBDCDNwKnAmsAC4SQqwYstkWYK2U8jDgIeAmo8ajGZ2GhUV0UA9vvZXyPh6/hWJLngmJqhDdVKtGS2nS3ay0oup5eeKJ12jyBCM1iSOBXVLKPVLK/9/evQdXdV13HP8uCQThIWOQ0BuEAPGSQbKxwfWjrl/Fbuu4Gc+UlMR2SuKZjjOTzrSTxuNOpsmkf3QybpJmWjdpYjsZZ0IedlrX48ShxnaaOGAwCAQC9AIsgYzEU2BhELD7x96ShXSvpIsl3XOk32dGc87ZZ19YC87Vuvc89r4AbAA+3ruDc+5151z3jembgat7VFY+kuLrZnCUPDq37hnya97vmsiUidEqEjk50E6ufwIvRcebTgMwa8G1wx2WSKyNZJEoApp7bbeEtmTWA78cwXgkieU3TuYymez+7RA/gV+4QOf7jqnTInLVOsjJn8A5ptDZfDzl1x4/eKbnzxCRD41kkUj0GyThyWIz+xSwEvh6kv2Pmdk2M9vWnoaJwMe6ykq/rK4Z4gRCW7fSeXkyU/KyRy6oq5Bb4ocIaa9LfcySwy3+0OyefU5EvJEsEi1ASa/tYuBI305mdjfwJPCAcy7hk0zOue8651Y651bm5uaOSLDjWWkpZE/6gOr2Ij/91WB27aKdXGbMiVaRyCnzdzYda+oYpGcfly/TdDybommnmDwMM6qKjCUjWSS2AgvNbJ6ZZQFrgStGHTKzKuA7+ALRNoKxyADMoHLBWapZATU1g/Y/V99CMyWUr4jWb9TZC3zROnowxVtzW1poujSXefnRuqVXJApGrEg45y4CnwdeBfYCP3XO7TGzr5rZA6Hb14FpwM/MrNrMhnHoOklF5U2T2MkKLu3YNWjfxtrzODIoXxStx2wKi308R1qGOG92t7o6DjCPsrIRCEok5kb0Kp1z7hXglT5tX+61fvdI/v0ydJW3TqPzWaPxraOUPz5w37pGf+2ivHwUAktBQYFfHj46xGsrwfnaRlq4k7KKsyMQlUi8ReujoKRNZZW/z2DH3sFPIdUd8UNfLFw4SMdRlpUFuVmnOXIitSFGDu05iyODsuWaYECkLxUJAWDZMj/Q386WWQN3bGujrrOYguyzTJ8+OrGlonD6GQ6fTW1ojqYmv5w3X28Hkb70rhDAfwovzT5J08lBHiarraWehZSXRutBum5FOR9w5GLulVMRDuLAYT8Crq5JiPSnIiE95s7u5ODFIjhxInmn2lrqKGfhsmiOcVSUf5nDFEFr65Bf03x8ChPsIvn5IxiYSEypSEiPueWTOMRc2LQpaZ9T25toI4/yymiOcVRYkkkbeXS9O/Qi0doxhfwpHWTo3SDSj94W0qP0ptm8RwHnfvVm0j71O/wdQOWLojUkR7eiZTMAaH1p69Be0NXFex/MIP8aPSMhkoiKhPRYWuFvHd1enfywqGvw+6J2+2u3wuv8hffDP3pjaEOGv/su75FPfu6lkQ1MJKZUJKTHHXeAcZnXmuYl7tDRQW1HERMyLjF//qiGNmSlpX5Z054H+/YN/oKmJlopIL9Ykw2JJKIiIT1mzoTFs9qpPjkHtmzp3+HAAXZTQXnhWbJSmxJ71CxdCksWXOA5HoWNGwft31V/kHZyyZ8fzWssIummIiFXKMvrpIkyuPfe/jsbG9lNBRVLUhz2YhSZwV8+ksVmVtP28tuD9j+44ySXyWT+igg+9CESASoScoV5q2dzgHm4nP6j7Z6v8WMcLa5K7Ynm0XbffeDI4NXNgz9UV7fXX4soX6y3gkgiemfIFcoqptLBNXy19XP99h3a8HscGcxfFq3RX/uqqoK8qWf45dlbB714XXfQnzeL6oV4kXRTkZAr3HOPX/7PubvgfK/pPU6dommf3476k8kZGXDPombedLfD2YEH7WtszyY76xyzBhmNRGS8UpGQK1RUwLrVDRxnFhzpNUfUU0/RwAKAyN7Z1Nucgi6OksflYwM8PX7yJM0XZjN31lksmo99iKSdioT0M6c0k8MUcanpkG9ob4evfY2f8BeUlVyIxfAVuXkZXGICp+uOJu/U1EQzJZQU6hkJkWRUJKSfkmXZdJHF0XdafMOhQ5wmm99yG4/+VWYsPnXn5PupUtrXfCr5dYnuIjFPz0iIJKMiIf2UVvmRYBu3nfQNzc3UshSAqpWpTeiTLjnF/uL6MXLgaOJvE+f2v8sxcilZonkkRJJRkZB+Kpb7w2LXjovQ2krXK7/mD/g94B9Wi4PcOf423Vt4C1ffkLBP084zAJQtieaItiJRMKLTl0o8FRfDzCnn2NkwFQoLaWBxz77uYS+iLmdxTs96+44WZt/Wv8/+On/ebNGi0YpKJH70TUL6MYMbb8nizcl/jAP2hSKxcSOxGU67cM6Hn3/qf9eWsM/+Zj8UR9SmYRWJkpi85WW0PfiJTOo+mMvT/DV/X/g8AKtWpTmoFEycCHV1fr3+rV5FYtMmeOYZqKmh7lQuhdM6IjkNq0hUqEhIQmvXQmEhPM6/U39kKhUVxO6XaWkpTM26wBstC+D4cd94112wfj0sX85+V0558ftpjVEk6lQkJKEZM+Cpp/z67bcPOFldZE2cCJ9e084G1tL5m22wfTsADcynliXst8UsunlmmqMUiTYVCUlq7VrYvNkXiNz+4/3Fwv3rZnKeyWzfUAc33MBn+U8W0sAyajnhZlJeoTubRAaiIiEDWrUKMuPxaERCq+7wt8K+9dNmWsnn+3z2iv0PPpiOqETiQ0VCxrTZs2FV5Qf8E/9AIa0AZGT4J7B/8IPoD1Yokm4qEjLmfes/JtNBds/2iy8a58/Dww+nMSiRmNDDdDLmrVoFzz8Ply5BdbWfdC+q06+KRI2KhIwL69b5pb49iKRGp5tERCQpFQkREUlKRUJERJJSkRARkaRUJEREJCkVCRERSUpFQkREklKREBGRpMw5l+4YUmJm7cChq3x5DnBsGMOJAuUUD8opHsZyTnOdcymP5xy7IvFRmNk259zKdMcxnJRTPCineFBO/el0k4iIJKUiISIiSY23IvHddAcwApRTPCineFBOfYyraxIiIpKa8fZNQkREUjBuioSZrTGz/WbWYGZfSnc8Q2Vmz5hZm5nt7tU208w2mll9WF4b2s3M/jXkuMvMrk9f5ImZWYmZvW5me81sj5l9IbTHOafJZva2me0MOX0ltM8zsy0hp5+YWVZonxS2G8L+0nTGPxAzyzSzHWb2ctiOdU5mdtDMasys2sy2hbbYHnsAZjbDzH5uZvvC++rm4cxpXBQJM8sE/g24D1gKfNLMlqY3qiF7DljTp+1LwGvOuYXAa2EbfH4Lw89jwNOjFGMqLgJ/65xbAqwGHg//F3HO6Txwp3NuBVAJrDGz1cA/A98IOZ0E1of+64GTzrkFwDdCv6j6ArC31/ZYyOmPnHOVvW4LjfOxB/At4FfOucXACvz/1/Dl5Jwb8z/AzcCrvbafAJ5Id1wpxF8K7O61vR8oCOsFwP6w/h3gk4n6RfUH+G/gnrGSEzAF2A6swj/ANCG09xyDwKvAzWF9Quhn6Y49QS7F4RfMncDLgI2BnA4COX3aYnvsAdnAgb7/1sOZ07j4JgEUAc29tltCW1zlOedaAcJydmiPVZ7hlEQVsIWY5xROy1QDbcBGoBE45Zy7GLr0jrsnp7D/NDBrdCMekm8CXwQuh+1ZxD8nB/zazN4xs8dCW5yPvTKgHXg2nBb8nplNZRhzGi9FwhK0jcXbumKTp5lNA14A/sY51zFQ1wRtkcvJOXfJOVeJ//R9E7AkUbewjHxOZvanQJtz7p3ezQm6xian4Bbn3PX40y6Pm9ntA/SNQ04TgOuBp51zVcD7fHhqKZGUcxovRaIFKOm1XQwcSVMsw+GomRUAhGVbaI9FnmY2EV8gfuScezE0xzqnbs65U8Ab+OstM8xsQtjVO+6enML+a4AToxvpoG4BHjCzg8AG/CmnbxLvnHDOHQnLNuAX+IIe52OvBWhxzm0J2z/HF41hy2m8FImtwMJwZ0YWsBZ4Kc0xfRQvAY+E9Ufw5/W72x8OdzCsBk53f+WMCjMz4PvAXufcv/TaFeeccs1sRlj/GHA3/uLh68BDoVvfnLpzfQjY5MIJ4qhwzj3hnCt2zpXi3y+bnHPriHFOZjbVzKZ3rwP3AruJ8bHnnHsPaDazRaHpLqCW4cwp3RdeRvECz/1AHf5c8ZPpjieFuH8MtAJd+E8B6/Hnel8D6sNyZuhr+Lu4GoEaYGW640+Qz634r7e7gOrwc3/Mc1oO7Ag57Qa+HNrLgLeBBuBnwKTQPjlsN4T9ZenOYZD87gBejntOIfad4WdP9++BOB97Ic5KYFs4/v4LuHY4c9IT1yIiktR4Od0kIiJXQUVCRESSUpEQEZGkVCRERCQpFQkREUlKRULkKpnZP5rZ36U7DpGRpCIhIiJJqUiIpMDMnjQ/L8n/AotC2+fMbKv5+SReMLMpZjbdzA6EIUgws+wwl8HEtCYgkiIVCZEhMrMb8ENUVAGfAG4Mu150zt3o/HwSe4H1zrkz+DGc/iT0WQu84JzrGt2oRT4aFQmRobsN+IVzrtP5kWu7x/+qMLP/M7MaYB2wLLR/D/hMWP8M8OyoRisyDFQkRFKTaByb54DPO+euA76CH8cI59zvgFIz+0Mg0zm3O8FrRSJNRUJk6H4D/LmZfSyMJvpnoX060BquN6zr85of4gdp1LcIiSUN8CeSAjN7EngYOIQflbcWP9HLF0NbDTDdOfdo6J+Pn16ywPm5JkRiRUVCZASZ2UPAx51zn053LCJXY8LgXUTkapjZt/HTZN6f7lhErpa+SYiISFK6cC0iIkmpSIiISFIqEiIikpSKhIiIJKUiISIiSalIiIhIUv8PZMaK3A+3iUwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#예측값 불러오기\n",
    "train_predict = sess.run(Y_pred, feed_dict={X: trainX})\n",
    "test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "\n",
    "plt.plot(testY,'r')\n",
    "plt.plot(test_predict,'b')\n",
    "plt.xlabel(\"day\")\n",
    "plt.ylabel(\"elec\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
